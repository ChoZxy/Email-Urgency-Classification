{"cells":[{"cell_type":"markdown","source":["# Classifier Model"],"metadata":{"id":"_OwBCVqJNcNK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQTT9x-6d2JI"},"outputs":[],"source":["from scipy import sparse\n","from sklearn import linear_model\n","from collections import Counter\n","import numpy as np\n","import pandas as pd\n","import operator\n","import nltk\n","import math\n","from scipy.stats import norm\n","from sklearn.metrics import plot_confusion_matrix\n","from pandas import option_context"]},{"cell_type":"code","source":["from google.colab import drive\n","#drive.flush_and_unmount()\n","drive.mount('/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f06gWDQyUi-X","executionInfo":{"status":"ok","timestamp":1650611018518,"user_tz":420,"elapsed":1246,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"4679ce7a-c021-4b15-85fe-355f7f82e6de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4KuVSCSqlUX","outputId":"2f56d57e-ac80-4481-ebbd-062fe6d47680","executionInfo":{"status":"ok","timestamp":1650611019576,"user_tz":420,"elapsed":1063,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n","  warn(RuntimeWarning(msg))\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["!python -m nltk.downloader punkt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWRzmH3kTWIE"},"outputs":[],"source":["def load_ordinal_data(filename, ordering):\n","    X = []\n","    Y = []\n","    orig_Y=[]\n","    for ordinal in ordering:\n","        Y.append([])\n","        \n","    with open(filename, encoding=\"utf-8\") as file:\n","        for line in file:\n","            cols = line.split(\"\\t\")\n","            idd = cols[0]\n","            label = cols[1].lstrip().rstrip()\n","            text = cols[2]\n","\n","            X.append(text)\n","            \n","            index=ordering.index(label)\n","            for i in range(len(ordering)):\n","                if index > i:\n","                    Y[i].append(1)\n","                else:\n","                    Y[i].append(0)\n","            orig_Y.append(label)\n","                    \n","    return X, Y, orig_Y\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4w-icqRTWIF"},"outputs":[],"source":["class OrdinalClassifier:\n","\n","    def __init__(self, ordinal_values, feature_method, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY):\n","        self.ordinal_values=ordinal_values\n","        self.feature_vocab = {}\n","        self.feature_method = feature_method\n","        self.min_feature_count=2\n","        self.log_regs = [None]* (len(self.ordinal_values)-1)\n","        self.log_reg = None\n","\n","        self.trainY=trainY\n","        self.devY=devY\n","        self.testY=testY\n","        \n","        self.orig_trainY=orig_trainY\n","        self.orig_devY=orig_devY\n","        self.orig_testY=orig_testY\n","        \n","        self.trainX = self.process(trainX, training=True)\n","        self.devX = self.process(devX, training=False)\n","        self.testX = self.process(testX, training=False)\n","\n","    # Featurize entire dataset\n","    def featurize(self, data):\n","        featurized_data = []\n","        for text in data:\n","            feats = self.feature_method(text)\n","            featurized_data.append(feats)\n","        return featurized_data\n","\n","    # Read dataset and returned featurized representation as sparse matrix + label array\n","    def process(self, X_data, training = False):\n","        \n","        data = self.featurize(X_data)\n","\n","        if training:\n","            fid = 0\n","            feature_doc_count = Counter()\n","            for feats in data:\n","                for feat in feats:\n","                    feature_doc_count[feat]+= 1\n","\n","            for feat in feature_doc_count:\n","                if feature_doc_count[feat] >= self.min_feature_count:\n","                    self.feature_vocab[feat] = fid\n","                    fid += 1\n","\n","        F = len(self.feature_vocab)\n","        D = len(data)\n","        X = sparse.dok_matrix((D, F))\n","        for idx, feats in enumerate(data):\n","            for feat in feats:\n","                if feat in self.feature_vocab:\n","                    X[idx, self.feature_vocab[feat]] = feats[feat]\n","\n","        return X\n","\n","\n","    def train(self):\n","        (D,F) = self.trainX.shape\n","\n","        \n","        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n","            best_dev_accuracy=0\n","            best_model=None\n","            for C in [0.1, 1, 10, 100]:\n","\n","                log_reg = linear_model.LogisticRegression(C = C, max_iter=1000)\n","                log_reg.fit(self.trainX, self.trainY[idx])\n","                development_accuracy = log_reg.score(self.devX, self.devY[idx])\n","                if development_accuracy > best_dev_accuracy:\n","                    best_dev_accuracy=development_accuracy\n","                    best_model=log_reg\n","\n","\n","            self.log_regs[idx]=best_model\n","            self.log_reg = best_model\n","        \n","    def test(self):\n","        cor=tot=0\n","        counts=Counter()\n","        preds=[None]*(len(self.ordinal_values)-1)\n","        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n","            preds[idx]=self.log_regs[idx].predict_proba(self.testX)[:,1]\n","        \n","        preds=np.array(preds)\n","\n","            \n","        for data_point in range(len(preds[0])):\n","            \n","    \n","            ordinal_preds=np.zeros(len(self.ordinal_values))\n","            for ordinal in range(len(self.ordinal_values)-1):\n","                if ordinal == 0:\n","                    ordinal_preds[ordinal]=1-preds[ordinal][data_point]\n","                else:\n","                    ordinal_preds[ordinal]=preds[ordinal-1][data_point]-preds[ordinal][data_point]\n","\n","            ordinal_preds[len(self.ordinal_values)-1]=preds[len(preds)-1][data_point]\n","\n","            prediction=np.argmax(ordinal_preds)\n","            counts[prediction]+=1\n","            if prediction == self.ordinal_values.index(self.orig_testY[data_point]):\n","                cor+=1\n","            tot+=1\n","\n","\n","        return cor/tot\n","    def prediction(self):\n","        counts=Counter()\n","        preds=[None]*(len(self.ordinal_values)-1)\n","        for idx, ordinal_value in enumerate(self.ordinal_values[:-1]):\n","            preds[idx]=self.log_regs[idx].predict_proba(self.testX)[:,1]\n","        \n","        preds=np.array(preds)\n","        predictions = []\n","\n","            \n","        for data_point in range(len(preds[0])):\n","            \n","    \n","            ordinal_preds=np.zeros(len(self.ordinal_values))\n","            for ordinal in range(len(self.ordinal_values)-1):\n","                if ordinal == 0:\n","                    ordinal_preds[ordinal]=1-preds[ordinal][data_point]\n","                else:\n","                    ordinal_preds[ordinal]=preds[ordinal-1][data_point]-preds[ordinal][data_point]\n","\n","            ordinal_preds[len(self.ordinal_values)-1]=preds[len(preds)-1][data_point]\n","\n","            prediction=np.argmax(ordinal_preds)\n","            predictions.append(prediction)\n","            counts[prediction]+=1\n","        return counts, predictions\n","    def printWeights(self, n=10):\n","\n","        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n","        for k in self.feature_vocab:\n","            reverse_vocab[self.feature_vocab[k]]=k\n","\n","        # binary\n","        if len(self.log_reg.classes_) == 2:\n","              weights=self.log_reg.coef_[0]\n","\n","              cat=self.log_reg.classes_[1]\n","              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n","                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n","              print()\n","\n","              cat=self.log_reg.classes_[0]\n","              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n","                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n","              print()\n","\n","        # multiclass\n","        else:\n","          for i, cat in enumerate(self.log_reg.classes_):\n","\n","              weights=self.log_reg.coef_[i]\n","\n","              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n","                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n","              print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31OXa6VhTWIH"},"outputs":[],"source":["def binary_bow_featurize(text):\n","    feats = {}\n","    words = nltk.word_tokenize(text)\n","\n","    for word in words:\n","        word=word.lower()\n","        feats[word]=1\n","            \n","    return feats"]},{"cell_type":"code","source":["def feature1(text):\n","  feats = {}\n","  words = nltk.word_tokenize(text)\n","  for i in range(len(words)-1):\n","      word1 = words[i].lower()\n","      word2 = words[i + 1].lower()\n","      feats[word1 + \" \" + word2] = 1\n","  return feats"],"metadata":{"id":"bBRCWpSTXq_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature implements the idea of bigrams. A single word may not be able to express the meaning precisely. Each key is a combination of two consecutive words."],"metadata":{"id":"u0XNLFuN-X8U"}},{"cell_type":"code","source":["high = [\"asap\", \"password\", \"suggestion\", \"think\", \"userid\", \"can\", \"comments\", \"login\"]\n","medium = [\"looking\", \"send\", \"respond\", \"forward\", \"let\", \"problems\"]\n","low = [\"call\", \"no\", \"haha\", \"fyi\", \"note\", \"pls\", \"thanks\", \"link\", \"www\"]\n","def feature2(text):\n","  feats = {}\n","  words = nltk.word_tokenize(text)\n","  for word in words:\n","    word = word.lower()\n","    if word in high:\n","      if \"high\" in feats:\n","        feats[\"high_f\"] = feats[\"high\"] + 1\n","      else:\n","        feats[\"high_f\"] = 1\n","    if word in medium:\n","      if \"medium\" in feats:\n","        feats[\"medium_f\"] = feats[\"medium\"] + 1\n","      else:\n","        feats[\"meidum_f\"] = 1\n","    if word in low:\n","      if \"low\" in feats:\n","        feats[\"low_f\"] = feats[\"low\"] + 1\n","      else:\n","        feats[\"low_f\"] = 1\n","  return feats"],"metadata":{"id":"VycYc9cfYPnE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["feature2 is looking at the words that appear in the dataset, if the word appear, it will add 1 to the specific category(e.g. high, medium, low). "],"metadata":{"id":"s1jaJ5Z7lC4-"}},{"cell_type":"code","source":["def generate_N_grams(text,ngram=1):\n","  words=[word for word in text.split(\" \")]  \n","  temp=zip(*[words[i:] for i in range(0,ngram)])\n","  ans=[' '.join(ngram) for ngram in temp]\n","  return ans"],"metadata":{"id":"9Pj-xE7xYdLx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature corresponds to the \"time sensitive\" point we have among our high urgency points. Words/phrases are all high frequency time related words that we discovered during annotation process."],"metadata":{"id":"ZEwXKPQS0FzG"}},{"cell_type":"code","source":["def time_featurize(text):\n","    feats = {}\n","    feats['bias_term']=1\n","    words = nltk.word_tokenize(text.lower())\n","    timesensitive_uni = [\"asap\",\"now\",\"deadline\",\"now\",\"urgent\"]\n","    timesensitive_bi = [\"done by\",\"have by\",\"send by\",\"right now\"]\n","    for word in words:\n","      if word in timesensitive_uni:\n","        feats['timesensitive_'] = 1\n","    for bi in generate_N_grams(text.lower(),2):\n","      if bi in timesensitive_bi:\n","        feats['timesensitive_'] = 1      \n","    return feats"],"metadata":{"id":"yozZCl9gYd7t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature corresponds to the \"technology related question\" point we have among our high urgency points. Words/phrases are all high frequency technology question related words that we discovered during annotation process. "],"metadata":{"id":"fu71g12l0NXv"}},{"cell_type":"code","source":["def tech_featurize(text):\n","    feats = {}\n","    \n","    words = nltk.word_tokenize(text.lower())\n","    tech_uni = [\"password\",\"login\",\"system\",\"urgent\"]\n","    tech_bi = [\"log in\",\"user id\",\"forgot password\",\"system update\",\"doesn't work\"]\n","    for word in words:\n","      if word in tech_uni:\n","        feats['techissue_'] = 1\n","    for bi in generate_N_grams(text.lower(),2):\n","      if bi in tech_bi:\n","        feats['techissue_'] = 1      \n","    return feats"],"metadata":{"id":"D_u2BqkfYf2G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature corresponds to the “professional/business related topic” point we have among our high and medium urgency points. Words/phrases are all high frequency professional topic related words that we discovered during annotation process. "],"metadata":{"id":"iPSyrYy10Q5m"}},{"cell_type":"code","source":["def prof_featurize(text):\n","    feats = {}\n","    \n","    words = nltk.word_tokenize(text.lower())\n","    prof_uni = [\"case\",\"deal\",\"client\",\"trade\",\"contract\"]\n","    prof_bi = [\"could you\"]\n","    for word in words:\n","      if word in prof_uni:\n","        feats['prof_'] = 1\n","    for bi in generate_N_grams(text.lower(),2):\n","      if bi in prof_bi:\n","        feats['prof_'] = 1      \n","    return feats"],"metadata":{"id":"PQ2HY61BYh5m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature corresponds to the \"asking question\" point we have among our high/medium urgency points. Words/pharses are all high frequency question related words that we discovered during annotation process. "],"metadata":{"id":"jWLRbzMK0V9H"}},{"cell_type":"code","source":["def question_featurize(text):\n","    feats = {}\n","    \n","    words = nltk.word_tokenize(text.lower())\n","    ques_uni = [\"?\",\"help\",\"favor\",\"what\",\"when\",\"know\",\"how\",\"why\",\"where\"]\n","    ques_bi = [\"could you\",\"do you\",\"what is\",\"can you\",\"is that\"]\n","    for word in words:\n","      if word in ques_uni:\n","        feats['question_'] = 1\n","    for bi in generate_N_grams(text.lower(),2):\n","      if bi in ques_bi:\n","        feats['question_'] = 1      \n","    return feats"],"metadata":{"id":"VVRo5gnXYkA-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature corresponds to the \"asking for people's opinion\" point we have among our high urgency points. Words/pharses are all high frequency opinion related words that we discovered during annotation process. "],"metadata":{"id":"ywBHU8fU0YWh"}},{"cell_type":"code","source":["def opinions_featurize(text):\n","    feats = {}\n","  \n","    words = nltk.word_tokenize(text.lower())\n","    op_bi = [\"have suggestions\",\"your opinion\",\"your thoughts\",\"your suggestion\",\"offer suggestion\"]\n","\n","    op_tri = [\"have any suggestions\",\"what's your thought\",\"what you think\",\"thoughts on\"]\n","    for bi in generate_N_grams(text.lower(),2):\n","      if bi in op_bi:\n","        feats['opinion_'] = 1\n","    for tri in generate_N_grams(text.lower(),3):\n","      if tri in op_tri:\n","        feats['opinion_'] = 1      \n","    return feats"],"metadata":{"id":"maBANv5TYmYr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature corresponds to the \"action required\" point we have among our high/medium urgency points. Words are all high frequency time related words that we discovered during annotation process. "],"metadata":{"id":"5gdCl6f30dL0"}},{"cell_type":"code","source":["def action_featurize(text):\n","    feats = {}\n","  \n","    words = nltk.word_tokenize(text.lower())\n","    action_uni = [\"send\",\"receive\",\"copy\",\"respond\",\"reply\"]\n","    for word in words:\n","      if word in action_uni:\n","        feats['action_'] = 1    \n","    return feats"],"metadata":{"id":"MLt0aIAzYoS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature corresponds to the \"casual tone\" point we have among our low urgency points. Words/phrases are all high frequency ones that indicates casuality that we discovered during annotation process. "],"metadata":{"id":"47xFkT8G0gb1"}},{"cell_type":"code","source":["def casual_featurize(text):\n","    feats = {}\n","  \n","    words = nltk.word_tokenize(text.lower())\n","    casual_uni = [\"haha\",\"lol\",\"lmao\",\"wtf\",\"wth\",\"hell\",\"fuck\",\"damn\",\"sorta\"]\n","    casual_bi = [\"sort of\",\"what's up\"]\n","    for word in words:\n","      if word in casual_uni:\n","        feats['casual_'] = 1\n","    for bi in generate_N_grams(text.lower(),2):\n","      if bi in casual_bi:\n","        feats['casual_'] = 1      \n","    return feats"],"metadata":{"id":"Gw5jACjGYsHF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","This feature corresponds to the \"personal events(not professional)\" point we have among our medium/low urgency points. Words are all high frequency personal topic related words that we discovered during annotation process. \n"],"metadata":{"id":"nMXglLLx0h3-"}},{"cell_type":"code","source":["def personal_featurize(text):\n","    feats = {}\n","  \n","    words = nltk.word_tokenize(text.lower())\n","    personal_uni = [\"reunion\",\"how's\"]\n","    personal_bi = [\"hang out\",\"how's life\",\"catch up\",\"get together\"]\n","    for word in words:\n","      if word in personal_uni:\n","        feats['personal_'] = 1\n","    for bi in generate_N_grams(text.lower(),2):\n","      if bi in personal_bi:\n","        feats['personal_'] = 1      \n","    return feats"],"metadata":{"id":"iEq9Er6kYuZg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature covers other important words and phrases that indicate low urgency when annotating."],"metadata":{"id":"GJ_TAZut0oDt"}},{"cell_type":"code","source":["def low_featurize(text):\n","    feats = {}\n","  \n","    words = nltk.word_tokenize(text.lower())\n","    low_uni = [\"print\",\"text\",\"fyi\",\"file\",\"resend\"]\n","    low_bi = [\"call me\",\"good job\",\"my comment\"]\n","    for word in words:\n","      if word in low_uni:\n","        feats['low_'] = 1\n","    for bi in generate_N_grams(text.lower(),2):\n","      if bi in low_bi:\n","        feats['low_'] = 1      \n","    return feats"],"metadata":{"id":"PVjRdY5yYu0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feature covers other important words and phrases that indicate medium urgency when annotating."],"metadata":{"id":"-0gOxz2w0qiz"}},{"cell_type":"code","source":["def medium_featurize(text):\n","    feats = {}\n","\n","    words = nltk.word_tokenize(text.lower())\n","    med_5 = [\"unless anyone has other opinions\",\"looking forward to your response\"]\n","    med_7 = [\"let me know if there's any question\",\"looking forward to hearing back from you\"]\n","    med_11 = [\"if you have any other question, reply to this email\"]\n","    for five in generate_N_grams(text.lower(),5):\n","      if five in med_5:\n","        feats['med_'] = 1   \n","    for seven in generate_N_grams(text.lower(),7):\n","      if seven in med_7:\n","        feats['med_'] = 1\n","    for eleven in generate_N_grams(text.lower(),11):\n","      if eleven in med_11:\n","        feats['med_'] = 1           \n","    return feats"],"metadata":{"id":"_Kcp2gudYySF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","def feature4(text):\n","  count = CountVectorizer()\n","  lower_text = text.lower()\n","  word_count=count.fit_transform([lower_text])\n","  tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n","  tfidf_transformer.fit(word_count)\n","  df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count.get_feature_names(),columns=[\"idf_weights\"])\n","  feats = df_idf.sort_values(by=['idf_weights']).to_dict()['idf_weights']\n","  return feats"],"metadata":{"id":"Ln0q_lIOyTEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combiner_function(text):\n","\n","    # Here the `all_feats` dict should contain the features -- the key should be the feature name, \n","    # and the value is the feature value.  See `simple_featurize` for an example.\n","    # binary_bow_featurize,feature1,feature2,time_featurize,tech_featurize,prof_featurize,question_featurize,opinions_featurize,action_featurize,casual_featurize,personal_featurize,low_featurize,medium_featurize\n","    \n","  all_feats={}\n","  #for feature in [binary_bow_featurize,feature1,feature2, time_featurize,tech_featurize,prof_featurize,question_featurize,opinions_featurize,action_featurize,casual_featurize,personal_featurize,low_featurize,medium_featurize]:\n","  for feature in [binary_bow_featurize,question_featurize, feature2, time_featurize, medium_featurize, opinions_featurize, casual_featurize, personal_featurize]:#feature2, time_featurize,tech_featurize,prof_featurize,question_featurize,opinions_featurize,action_featurize,casual_featurize,personal_featurize,low_featurize,medium_featurize]:\n","  #for feature in [binary_bow_featurize,question_featurize, feature2]:\n","    all_feats.update(feature(text))\n","  return all_feats"],"metadata":{"id":"ObEdP-5QY1UZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQWsaZjATWII"},"outputs":[],"source":["def confidence_intervals(accuracy, n, significance_level):\n","    critical_value=(1-significance_level)/2\n","    z_alpha=-1*norm.ppf(critical_value)\n","    se=math.sqrt((accuracy*(1-accuracy))/n)\n","    return accuracy-(se*z_alpha), accuracy+(se*z_alpha)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlbQPx1ZTWII"},"outputs":[],"source":["def run(trainingFile, devFile, testFile, ordinal_values):\n","\n","\n","    trainX, trainY, orig_trainY=load_ordinal_data(trainingFile, ordinal_values)\n","    trainX1, trainY1, orig_trainY1 =load_ordinal_data(trainingFile, ordinal_values)\n","    devX, devY, orig_devY=load_ordinal_data(devFile, ordinal_values)\n","    testX, testY, orig_testY=load_ordinal_data(testFile, ordinal_values)\n","    \n","    simple_classifier = OrdinalClassifier(ordinal_values, combiner_function, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY)\n","    simple_classifier.train()\n","    accuracy=simple_classifier.test()\n","\n","    lower, upper=confidence_intervals(accuracy, len(testY[0]), .95)\n","    print(\"Test accuracy for best dev model: %.3f, 95%% CIs: [%.3f %.3f]\\n\" % (accuracy, lower, upper))\n","    return simple_classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drMreN1rTWIJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611028129,"user_tz":420,"elapsed":7979,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"f3ef201c-5c54-4bf6-833f-06455d4d05c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test accuracy for best dev model: 0.825, 95% CIs: [0.772 0.878]\n","\n"]}],"source":["gid=23\n","trainingFile = \"/gdrive/MyDrive/info_159_Project/ap4/ap_data/%s/train.txt\" % gid\n","devFile = \"/gdrive/MyDrive/info_159_Project/ap4/ap_data/%s/dev.txt\" % gid\n","testFile = \"/gdrive/MyDrive/info_159_Project/ap4/ap_data/%s/test.txt\" % gid\n","    \n","# ordinal values must be in order *as strings* from smallest to largest, e.g.:\n","# ordinal_values=[\"G\", \"PG\", \"PG-13\", \"R\"]\n","\n","ordinal_values=[\"low\", \"medium\", \"high\"]\n","\n","cl = run(trainingFile, devFile, testFile, ordinal_values)"]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"AP1rLHHYNlfj"}},{"cell_type":"markdown","source":["### Analysis on Unbalanced Dataset"],"metadata":{"id":"J7JtwTKMbFsa"}},{"cell_type":"code","source":["def run_predict(trainingFile, devFile, testFile, ordinal_values):\n","\n","\n","    trainX, trainY, orig_trainY=load_ordinal_data(trainingFile, ordinal_values)\n","    devX, devY, orig_devY=load_ordinal_data(devFile, ordinal_values)\n","    testX, testY, orig_testY=load_ordinal_data(testFile, ordinal_values)\n","    \n","    simple_classifier = OrdinalClassifier(ordinal_values, combiner_function, trainX, trainY, devX, devY, testX, testY, orig_trainY, orig_devY, orig_testY)\n","    simple_classifier.train()\n","    pred_count, pred_list=simple_classifier.prediction()\n","\n","    return pred_count, pred_list, orig_testY\n"],"metadata":{"id":"cw3kbBJjh9TS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_count, pred_list, orig_testY = run_predict(trainingFile, devFile, testFile, ordinal_values)"],"metadata":{"id":"ennoLiHB4hAO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred_count # 0 is low, 1 is medium, 2 is high"],"metadata":{"id":"e7GgRESH5HCn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036115,"user_tz":420,"elapsed":32,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"349decc8-2a9b-446b-a6ac-02a30a907b49"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({0: 151, 1: 34, 2: 15})"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["pred_list.count(0), pred_list.count(1), pred_list.count(2)"],"metadata":{"id":"dI9fn4kv5IxC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036116,"user_tz":420,"elapsed":30,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"36a3fa46-25dc-4381-e8a1-3529546225ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(151, 34, 15)"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["orig_testY.count(\"low\"), orig_testY.count(\"medium\"), orig_testY.count(\"high\")"],"metadata":{"id":"TYLiP9CL5Nxp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036116,"user_tz":420,"elapsed":27,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"92ab56f1-b14d-4e8d-9cd4-9b5aad7b68e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150, 30, 20)"]},"metadata":{},"execution_count":78}]},{"cell_type":"markdown","source":["I think our dataset is very unbalance because the total length of the training dataset is 200, but over 50% of those are categorized as low, and only a small percentage of datasets are categorized as meidum or high. Since there are too many low categories, our models will create more noise which will result in many miscategorized label for low categories. I think our dataset is a good strategy for oversampling because we have too many low categories in our dataset, so it might result in overfitting when we run our model. Using oversampling, we can transform the minority dataset (e.g. medium and high) in our case, to have more medium and high categories in our test set.\n","\n","Moreover, we can use class weight to improve the imbalanced dataset, setting weight for three of our categories. We can apply smaller weight to the high category because it has the most data, and apply bigger weight to medium and low data. So we can balance our test set. "],"metadata":{"id":"FerxUbRRbEmX"}},{"cell_type":"markdown","source":["### Analysis on Confusion Matrix and F1 Score"],"metadata":{"id":"vEM6PENhZx8d"}},{"cell_type":"code","source":["new_pred = []\n","for i in range(len(pred_list)):\n","  if pred_list[i] == 0:\n","    new_pred.append(\"low\")\n","  elif pred_list[i] == 1:\n","    new_pred.append(\"medium\")\n","  elif pred_list[i] == 2:\n","    new_pred.append(\"high\")\n"],"metadata":{"id":"bUwiqhea6Le-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix(orig_testY, new_pred)"],"metadata":{"id":"t9-5OtlB9L-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036116,"user_tz":420,"elapsed":23,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"b8cca69f-b60a-46d9-9775-62b8a92aae50"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  9,   9,   2],\n","       [  4, 135,  11],\n","       [  2,   7,  21]])"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["# confusion matrix \n","y_actu = pd.Series(orig_testY, name='Actual')\n","y_pred = pd.Series(new_pred, name='Predicted')\n","df_confusion = pd.crosstab(y_actu, y_pred)\n","df_confusion"],"metadata":{"id":"g9YwEAwt99XK","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1650611036117,"user_tz":420,"elapsed":22,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"3f23fc8f-2a0d-457f-d689-f55db80ba40d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Predicted  high  low  medium\n","Actual                      \n","high          9    9       2\n","low           4  135      11\n","medium        2    7      21"],"text/html":["\n","  <div id=\"df-72c876a3-a7bb-40d5-afeb-430cf3b8b52f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Predicted</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>medium</th>\n","    </tr>\n","    <tr>\n","      <th>Actual</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>high</th>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>low</th>\n","      <td>4</td>\n","      <td>135</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>medium</th>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>21</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72c876a3-a7bb-40d5-afeb-430cf3b8b52f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-72c876a3-a7bb-40d5-afeb-430cf3b8b52f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-72c876a3-a7bb-40d5-afeb-430cf3b8b52f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["precision_low = 135/150\n","precision_low"],"metadata":{"id":"JCr4j849Gmjo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036117,"user_tz":420,"elapsed":10,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"5acc3649-f55d-41c0-f260-9cd5af1b32aa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["precision_medium = 21/30 \n","precision_medium"],"metadata":{"id":"Q-Kx8V6_HENm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036117,"user_tz":420,"elapsed":8,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"b8dc71d5-b95a-4559-9ccf-379a7b68f8e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["precision_high = 9/20 \n","precision_high"],"metadata":{"id":"gKJ46QGnHOjp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036386,"user_tz":420,"elapsed":275,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"7c923c6d-a5e9-4224-d263-f0e65e7643c3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.45"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":["recall_low = 135/151\n","recall_low"],"metadata":{"id":"UeQ-Q_oyHvRa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036386,"user_tz":420,"elapsed":33,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"c5d6ec52-7269-434e-e9cf-7f52d6ea4f0d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8940397350993378"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","source":["recall_medium = 21/34\n","recall_medium"],"metadata":{"id":"ToyiunBeJy0d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036387,"user_tz":420,"elapsed":30,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"d8b56080-264d-4f0f-99f3-90f7a408c134"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6176470588235294"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":["recall_high = 9/15\n","recall_high"],"metadata":{"id":"dCJGMgB7J5_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036387,"user_tz":420,"elapsed":27,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"d112f598-1328-4c35-b1a1-6ef972b5c952"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","source":["f1_low = (2 * precision_low * recall_low)/(precision_low + recall_low)\n","f1_low"],"metadata":{"id":"yjqVLzfkJ_pV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036387,"user_tz":420,"elapsed":25,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"0f76c6f7-dcce-4030-845b-9fd932fef271"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8970099667774086"]},"metadata":{},"execution_count":88}]},{"cell_type":"code","source":["f1_medium = (2 * precision_medium * recall_medium)/(precision_medium + recall_medium)\n","f1_medium"],"metadata":{"id":"FEFxGzRxKRvd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036387,"user_tz":420,"elapsed":22,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"e77b44ae-782d-41cf-c597-c6b84e22171e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.65625"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["f1_high = (2 * precision_high * recall_high)/(precision_high + recall_high)\n","f1_high"],"metadata":{"id":"HH8-GJZcKYfU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036388,"user_tz":420,"elapsed":20,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"e0f76a62-cd42-465d-f990-7f833e8d9f8c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5142857142857143"]},"metadata":{},"execution_count":90}]},{"cell_type":"markdown","source":["Based on the counts of each label in the true testY list, we noticed we had 150 low labels, 30 medium labels, and 20 high labels. This is an unbalanced class distribution, so only examining the accuracy of the model or the precision may not give us a sense of the model. We decided to calculate the F1 score which is a better calculation for the unbalanced classes model. For the low class, we got an F1 score around 0.89, 0.65 for the medium class and 0.51 for the high class. The model performed better for low class text since the F1 score for low class is pretty close to 1. It was doing ok for medium class but not as good as the low class. Based on the confusion matrix and F1 score, the model didn’t predict well on the high class. The model misclassified about 45% of the high class text into low class and 10% of the high class into medium class. It also misclassified about 23% of medium class text into low class and 6% of medium class into high class. For low class text, the model misclassifies about 7% into medium class and 2% into high class. Based on these result, we figured out that our model often mislabeled the high class text into low class. We may probably need to look into our guideline to make the edge between high and low class clearer. This may also be because our model isn’t able to detect some of the important features in high class and so it is making the model perform poorly. We may need to look into it and improve our model in the future for the high class portion.   \n","\n"," "],"metadata":{"id":"m28rvZpeKj5L"}},{"cell_type":"markdown","source":["### Analysis on category boundaries"],"metadata":{"id":"ZCukkZ1smUNv"}},{"cell_type":"code","source":["test_data = pd.read_csv(\"/gdrive/MyDrive/info_159_Project/ap4/ap_data/23/test.txt\", sep = \"\\t\", header = None)\n","test_data"],"metadata":{"id":"Sr8eOdUiODmF","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1650611036388,"user_tz":420,"elapsed":18,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"cf96f5f3-1a05-48f6-deba-132670acbaa1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       0       1  \\\n","0    565     low   \n","1     94     low   \n","2    186     low   \n","3    599     low   \n","4    564  medium   \n","..   ...     ...   \n","195  783     low   \n","196  822  medium   \n","197  868  medium   \n","198  583     low   \n","199  138     low   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2  \n","0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Thanks! DF  \n","1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Beavy and I changed the following deals for July and August. We took all the August activity to 0. This is how these deals should be balanced Deal 318562 with 318686. And 318691 with 318682. I'll look at the economics in the morning.  \n","2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Sean and Diana do not recognize this deal. They tried to check with the brokers, but couldn't get a hold of anyone. I'll let you know first thing in the morning whether or not this is good and what the deal number is. Kate  \n","3                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ---------------------- Forwarded by Eric Bass/HOU/ECT on 02/21/2001 08:13 AM --------------------------- Please join me and the Global Accounting leadership team in congratulating the following individuals on their promotions to: To Senior Tax Analyst Emily Allwardt (International Tax) Leon Branom (EBS Tax) Shanna Husser (EES Tax) there is my sweety pea Shilpa Mane (Corporate/London Tax) Todd Richards (Corp Tax) Michelle Thompson (Corp Tax)  \n","4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Hi, Elise. Sorry to bother you, but when Kali was unwrapping her presents last night, there were two without cards or tags (probably lost in transit!!). We narrowed it down to Laura and Sofi. So that we can get the thank you notes correct, did Laura give Kali a Skipper doll or a bath set in a blue bag? Thanks for your help. Susan P.S.- We ended up selling 131 boxes at the booth on Saturday.  \n","..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ...  \n","195  Lindy, You are correct they are physical plant receipt points. The Pecos Diamond plant is the straddle plant located on the Atoka Lateral. Thanks DS ----------------------------------- I don't think any of these are straddle plants. (The plant we discussed previously, I think, was the Peco Diamond which is a straddle plant.) Linda, am I correct on these 3 below? Sorry it took me so long to respond. Maybe you already have answers to this. -----Original Message----- From: Powers, Ken Sent: Wednesday, May 16, 2001 1:41 PM To: Donoho, Lindy Subject: Plant question I'm afraid I asked you this question before, but I can't find where I wrote down your answer. Sorry. Anyway, are any of these plants straddle plants, or are they \"real\" physical receipt points. These are the plants we're receiving gas from in May. 10703 GPM Artesia 60151 Amoco Abo 1190 Sid Richardson Keystone Ken Powers (402) 398-7065  \n","196                                                                                                                                                                                                                                                                                                                                                                                                                          Thanks for the attached deal information, Shouan. Looking at the data I feel confident that, once we are past the first day of the month, the gas daily options curveshift is valuing correctly. However, I am still concerned with the valuation from 10/31. Is there anyway to see same breakdown on 11/2 and 11/3 for 10/31's position. If you could send me that information in the same format I think we may have an answer for the changed deal value we saw fall out of my trader's book on 11/1. Susan  \n","197                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Tom, The system does not take my approval of this request.  \n","198                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     --------------------- Forwarded by Darron C Giron/HOU/ECT on 12/04/2000 04:13 PM --------------------------- -----Original Message----- - inspir.jpg  \n","199                                                                                                                                                                                                                                                                                                        ---------------------- Forwarded by Kay Mann/Corp/Enron on 06/02/2000 01:46 PM --------------------------- Jeff and Reynaldo Garcia spoke on this subject at length, and Rey convinced Jeff that spitting was necessary. AT that time the Enron Contracting entity was not specified to be Energia... If you want to unsplit the documents it will add additional time in excess geting a consolidation agreement completed I think. Please include Rey in your quest for closure. PS GE is now OK with the split. I need help getting closure on the Consolidation Agreement. I hope that we do not invent other issues as well.  \n","\n","[200 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-83b6a04f-a911-4a4d-84b8-b45d8883500f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>565</td>\n","      <td>low</td>\n","      <td>Thanks! DF</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>94</td>\n","      <td>low</td>\n","      <td>Beavy and I changed the following deals for July and August. We took all the August activity to 0. This is how these deals should be balanced Deal 318562 with 318686. And 318691 with 318682. I'll look at the economics in the morning.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>186</td>\n","      <td>low</td>\n","      <td>Sean and Diana do not recognize this deal. They tried to check with the brokers, but couldn't get a hold of anyone. I'll let you know first thing in the morning whether or not this is good and what the deal number is. Kate</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>599</td>\n","      <td>low</td>\n","      <td>---------------------- Forwarded by Eric Bass/HOU/ECT on 02/21/2001 08:13 AM --------------------------- Please join me and the Global Accounting leadership team in congratulating the following individuals on their promotions to: To Senior Tax Analyst Emily Allwardt (International Tax) Leon Branom (EBS Tax) Shanna Husser (EES Tax) there is my sweety pea Shilpa Mane (Corporate/London Tax) Todd Richards (Corp Tax) Michelle Thompson (Corp Tax)</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>564</td>\n","      <td>medium</td>\n","      <td>Hi, Elise. Sorry to bother you, but when Kali was unwrapping her presents last night, there were two without cards or tags (probably lost in transit!!). We narrowed it down to Laura and Sofi. So that we can get the thank you notes correct, did Laura give Kali a Skipper doll or a bath set in a blue bag? Thanks for your help. Susan P.S.- We ended up selling 131 boxes at the booth on Saturday.</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>783</td>\n","      <td>low</td>\n","      <td>Lindy, You are correct they are physical plant receipt points. The Pecos Diamond plant is the straddle plant located on the Atoka Lateral. Thanks DS ----------------------------------- I don't think any of these are straddle plants. (The plant we discussed previously, I think, was the Peco Diamond which is a straddle plant.) Linda, am I correct on these 3 below? Sorry it took me so long to respond. Maybe you already have answers to this. -----Original Message----- From: Powers, Ken Sent: Wednesday, May 16, 2001 1:41 PM To: Donoho, Lindy Subject: Plant question I'm afraid I asked you this question before, but I can't find where I wrote down your answer. Sorry. Anyway, are any of these plants straddle plants, or are they \"real\" physical receipt points. These are the plants we're receiving gas from in May. 10703 GPM Artesia 60151 Amoco Abo 1190 Sid Richardson Keystone Ken Powers (402) 398-7065</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>822</td>\n","      <td>medium</td>\n","      <td>Thanks for the attached deal information, Shouan. Looking at the data I feel confident that, once we are past the first day of the month, the gas daily options curveshift is valuing correctly. However, I am still concerned with the valuation from 10/31. Is there anyway to see same breakdown on 11/2 and 11/3 for 10/31's position. If you could send me that information in the same format I think we may have an answer for the changed deal value we saw fall out of my trader's book on 11/1. Susan</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>868</td>\n","      <td>medium</td>\n","      <td>Tom, The system does not take my approval of this request.</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>583</td>\n","      <td>low</td>\n","      <td>--------------------- Forwarded by Darron C Giron/HOU/ECT on 12/04/2000 04:13 PM --------------------------- -----Original Message----- - inspir.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>138</td>\n","      <td>low</td>\n","      <td>---------------------- Forwarded by Kay Mann/Corp/Enron on 06/02/2000 01:46 PM --------------------------- Jeff and Reynaldo Garcia spoke on this subject at length, and Rey convinced Jeff that spitting was necessary. AT that time the Enron Contracting entity was not specified to be Energia... If you want to unsplit the documents it will add additional time in excess geting a consolidation agreement completed I think. Please include Rey in your quest for closure. PS GE is now OK with the split. I need help getting closure on the Consolidation Agreement. I hope that we do not invent other issues as well.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83b6a04f-a911-4a4d-84b8-b45d8883500f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-83b6a04f-a911-4a4d-84b8-b45d8883500f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-83b6a04f-a911-4a4d-84b8-b45d8883500f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["np.where((np.array(orig_testY) == np.array(new_pred)) == False) # index of the uncorrect text"],"metadata":{"id":"m5bF_W9iOVfg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036388,"user_tz":420,"elapsed":17,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"165a5d22-119e-497e-eb97-d8327210edb2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([  2,   4,   6,  19,  26,  30,  35,  36,  38,  42,  52,  57,  58,\n","         62,  69,  86,  92,  97,  98, 104, 112, 115, 116, 117, 118, 119,\n","        129, 141, 143, 158, 163, 187, 195, 196, 197]),)"]},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["test_data.iloc[112][2]"],"metadata":{"id":"JWc4L8uVQS_4","colab":{"base_uri":"https://localhost:8080/","height":267},"executionInfo":{"status":"ok","timestamp":1650611036388,"user_tz":420,"elapsed":14,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"20c92115-c0dc-42cb-e49f-09a8ea56ab6a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'How many times do I have to say EMAIL IT? ---------------------- Forwarded by Kay Mann/Corp/Enron on 02/09/2001 01:57 PM --------------------------- Kay: Just to clarify. The \"\"blacklined changes\"\" in the fax I sent to you come directly from the document that you sent to us yesterday. We will let you know as soon as we hear anything further. Thanks, Karen Karen S. Way Piper Marbury Rudnick & Wolfe 203 N. LaSalle Chicago, Illinois 60601 email: karen.way@piperrudnick.com (ph) 312-368-2152 (fax) 312-630-6347 > -----Original Message----- > From: Way, Karen S. - CHI > Sent: Friday, February 09, 2001 11:45 AM > To: \\'kay.mann@enron.com\\' > Cc: Shindler, Donald A. - CHI; Townsend, Christopher J. - CHI > Subject: Titan-Schaffer Option > > Kay: > > This is to confirm that I have faxed to you at 713-646-3491 the following > documents: i) a copy of the cover fax-letter to Mr. Al Freehill, attorney > for the Schaffers; and 2) the blacklined changes to the Schaffer-Titan > Option for Mr. Freehill\\'s review per your changes yesterday. Please call > Don or me with any questions on this matter. > > Karen S. Way > Piper Marbury Rudnick & Wolfe > 203 N. LaSalle > Chicago, Illinois 60601 > email: karen.way@piperrudnick.com > (ph) 312-368-2152 > (fax) 312-630-6347 > ____________________________________________________________________________ The information contained in this communication may be confidential, is intended only for the use of the recipient named above, and may be legally privileged. If the reader of this message is not the intended recipient, you are hereby notified that any dissemination, distribution, or copying of this communication, or any of its contents, is strictly prohibited. If you have received this communication in error, please re-send this communication to the sender and delete the original message and any copy of it from your computer system. Thank you. For more information about Piper Marbury Rudnick & Wolfe, please visit us at http://www.piperrudnick.com/ ____________________________________________________________________________'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["np.array(new_pred)[195], np.array(orig_testY)[195]"],"metadata":{"id":"3Z_d9k2GQfe2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036389,"user_tz":420,"elapsed":14,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"01f5e21a-1c1c-4738-a5eb-cce5e0711992"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('medium', 'low')"]},"metadata":{},"execution_count":94}]},{"cell_type":"markdown","source":["In order to see how well our model performs, we got the index of the texts that the model predicted wrong and tried to look into the specific texts. We noticed that the model was able to detect work-related and profession-related text by counting the frequency of the work-related words. However, sometimes the work-related email can be informational based and no questions are asked in the email and there were no need to respond, but the high frequency of these words may cause the model think this is a high urgency email. We listed a few bullet points mention about this work-related features in both high and medium categories in the guideline. However, now we felt that we want to determine the urgency of the email, so we changed to whether or not the email is asking questions. Because of this, we may probably need to remove work-related features in our guideline and make whether or not the text contains question to have higher weight.\n","\n","Another problem is the forwarded emails. Some of the emails contain response and forwarded emails. In the guideline, we mentioned that when we tried to determine the urgency of an email that contained forwarded emails, we should prioritize the urgency of the response email. If there is no response email, then we should look into the most recent forwarded email. We use “---------Forwarded” to separate response and forwarded email. This can be recognized by a human. However, our model doesn’t have the ability to differentiate between response email and forwarded email. It takes all the text as input and trains the whole text. So sometimes when the response is low urgency but forwarded email is high urgency, our model will mislabeled it to be a high urgency email. We may need to improve our model to help it separate these email or we can clean our original email text to contain only the text we want to model to tain and remove all the redundant texts. \n","\n"],"metadata":{"id":"7W2TZB5ymKz4"}},{"cell_type":"markdown","source":["### Analysis on Biases Model"],"metadata":{"id":"xbVlbiw5Z4UJ"}},{"cell_type":"markdown","source":["In our dataset, there are some texts that include rhetorical questions (e.g.\"How many times do I have to say EMAIL IT?\"). The true testY labeled them all as low because they are not actually asking a question, but the model predicted it as medium. It leads to bias in our model because the model cannot tell the difference between rhetorical questions and non-rhetorical questions, so it will categorized it into the incorrect label. Also, text that is not English can also cause bias in our model. Our model cannot detect if the text is English, so it is harder for it to be categorized. "],"metadata":{"id":"ogoN2du0Zp1C"}},{"cell_type":"markdown","source":["### Analysis on Key Features"],"metadata":{"id":"9FHuoMdXooHo"}},{"cell_type":"code","source":["cl.printWeights(n=25)"],"metadata":{"id":"LbTKLKLOUxOZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650611036389,"user_tz":420,"elapsed":12,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"744b78a9-0fd4-478e-c19f-ea40ab59199a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\t2.244\t?\n","1\t1.145\tquestion_\n","1\t0.827\tcontract\n","1\t0.774\ttomorrow\n","1\t0.759\tpassword\n","1\t0.759\tplease\n","1\t0.674\tweek\n","1\t0.585\tdeal\n","1\t0.582\tafter\n","1\t0.563\tneed\n","1\t0.556\t05:47\n","1\t0.553\tsoon\n","1\t0.553\trespond\n","1\t0.531\t3:00\n","1\t0.529\tinformation\n","1\t0.520\tcould\n","1\t0.520\tthanks\n","1\t0.515\tlc\n","1\t0.511\tsometime\n","1\t0.487\tdo\n","1\t0.487\twhy\n","1\t0.485\tagreement\n","1\t0.482\t@\n","1\t0.481\tcopy\n","1\t0.478\tme\n","\n","0\t-0.702\t!\n","0\t-0.602\tthink\n","0\t-0.592\the\n","0\t-0.551\t:\n","0\t-0.519\tenron\n","0\t-0.508\tall\n","0\t-0.498\t.\n","0\t-0.488\twill\n","0\t-0.458\tguys\n","0\t-0.456\tgo\n","0\t-0.452\tscott\n","0\t-0.449\t&\n","0\t-0.445\tat\n","0\t-0.441\tlunch\n","0\t-0.440\tare\n","0\t-0.436\tthat\n","0\t-0.432\tfile\n","0\t-0.430\thad\n","0\t-0.426\tpl\n","0\t-0.421\tphone\n","0\t-0.418\tam\n","0\t-0.417\t's\n","0\t-0.407\there\n","0\t-0.395\tno\n","0\t-0.394\tthey\n","\n"]}]},{"cell_type":"markdown","source":["\n","*   According to the weights of the features, we can tell that the most important features leading to highs are \"?\", \"question\", \"contract\", \"please\", \"tomorrow\", \"password\". These features can generally be categorized into time sensitive, work-related, question, professional tone which relate to the high emergency points in our guideline. However, there are also features that have relatively high weights such as \"05:47\", \"3:00\", \"lc\", \"@\" that are not so informative when we are judging as human. This senario might be due to the relative small training data we have, so the model recognized some coincidents in our data. There are also common words like \"do\", \"me\" in that appears on the top of the feature list. This means, we should remove some stop words in our model. \n","*   We didn't remove punctuations because we believed they indicate whether the sentense is a question or not and that is an important point in judging the urgency to respond of emails. However, we should consider adding some regularization to our feature, so that \"?\" is not the single most important estimator of urgency in our model. \n","*   The features that have the smallest weights further exposed our model's reliance in stop words. While some of the features such as \"phone\", \"guys\" do make sense as they relates to \"request to call\", and \"casual/unprofessional tone\". The others are all stop words or other words that we didn't consider as strong indicators of low urgency when labeling manually. The lack of some of the features that we consider important such as \"call\",\"fyi\",\"haha\" might be the reason behind our difficulty in improving the model accurary.\n","*   Surprisingly, none of the key features are the features that appears on the top or bottom most list are bigram features. We think this is because there are too many noises in our model, so the really important bigram phrases are lost. \n","*   Only one of the features that we defined based on the annotation guideline appears to be in the key feature list. We think the possible reasons are too many noises/bad features in model, and not enough accurate domain knowledge included in those features.  \n","\n","\n","\n","\n"],"metadata":{"id":"yYGAVzWPkelx"}},{"cell_type":"markdown","source":["### Analysis On Common Model Mistakes"],"metadata":{"id":"-cY1X6yS0zOW"}},{"cell_type":"code","source":["def load_ordinal_data_new(filename, ordering):\n","    X = []\n","    Y = []\n","    orig_Y=[]\n","    id = []\n","    for ordinal in ordering:\n","        Y.append([])\n","        \n","    with open(filename, encoding=\"utf-8\") as file:\n","        for line in file:\n","            cols = line.split(\"\\t\")\n","            idd = cols[0]\n","            label = cols[1].lstrip().rstrip()\n","            text = cols[2]\n","\n","            X.append(text)\n","            id.append(idd)\n","            index=ordering.index(label)\n","            for i in range(len(ordering)):\n","                if index > i:\n","                    Y[i].append(1)\n","                else:\n","                    Y[i].append(0)\n","            orig_Y.append(label)\n","                    \n","    return X, Y, orig_Y, id"],"metadata":{"id":"HRKzJYLskiTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze(classifier):\n","    \n","    probs=classifier.log_reg.predict_proba(classifier.devX)\n","    \n","    predicts=classifier.log_reg.predict(classifier.devX)\n","    \n","    classes={}\n","    for idx, lab in enumerate(classifier.log_reg.classes_):\n","        classes[lab]=idx\n","\n","    mistakes={}\n","    for i in range(len(probs)):\n","        if predicts[i] != classifier.devY[0][i]:\n","            predicted_lab_idx=classes[predicts[i]]\n","            mistakes[i]=probs[i][predicted_lab_idx]\n","\n","    frame=[]\n","    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n","    for k, v in sorted_x:\n","        devX, devY, orig_devY, devids=load_ordinal_data_new(devFile, ordinal_values)\n","        idd=devids[k]\n","        text=devX[k]\n","        frame.append([idd, v, classifier.orig_devY[k], predicts[k], text])\n","\n","    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n","    pd.set_option('display.max_colwidth', None)\n","\n","    with option_context('display.max_colwidth', 400):\n","        display(df.head(n=20))"],"metadata":{"id":"Eu400hs3knUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analyze(cl)"],"metadata":{"id":"spFGL_HwkpNr","colab":{"base_uri":"https://localhost:8080/","height":1241},"executionInfo":{"status":"ok","timestamp":1650611036572,"user_tz":420,"elapsed":5,"user":{"displayName":"Jinglan Liu","userId":"09247368686056095984"}},"outputId":"f3dbc7fe-e60f-4041-c2c6-1e1b6758e1ad"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["     id  P(predicted class confidence) Human label  Prediction  \\\n","0     9                       0.998674        high           0   \n","1   303                       0.997984      medium           0   \n","2   469                       0.992540        high           0   \n","3   137                       0.992022      medium           0   \n","4    39                       0.991358      medium           0   \n","5   407                       0.986219        high           0   \n","6   802                       0.985440        high           0   \n","7   627                       0.983700      medium           0   \n","8   828                       0.982605      medium           0   \n","9   579                       0.981455      medium           0   \n","10  695                       0.969132        high           0   \n","11   25                       0.969113      medium           0   \n","12  509                       0.966901        high           0   \n","13  784                       0.961477      medium           0   \n","14  432                       0.959798         low           1   \n","15  657                       0.949267        high           0   \n","16   75                       0.946400      medium           0   \n","17  805                       0.943940        high           0   \n","18  537                       0.938878      medium           0   \n","19  801                       0.936306      medium           0   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                               Text  \n","0   \"How do we get the right focus on support for Calgary? Mark Taylor wanted to hold up transferring confirm responsibility to Calgary, so we still need to provide excellent service to them. While I can appreciate that Diane is out, that should actually be a red flag to us to pay even closer attention to the Calgary stuff. Every screw up looks big to them, and they can't understand how mistakes c...  \n","1   \"---------------------- Forwarded by Drew Fossum/ET&S/Enron on 04/03/2001 04:47 PM --------------------------- This is to confirm your attendance for the Friday, April 20, 2001, Executive Forum to be hosted by The Office of the Chairman. The Forum will begin at 3:00 p.m. and ends at 4:30 p.m in the Enron Building 50M. If you have any additional questions, please feel free to give me a call. Th...  \n","2                                                                                                                                                                                                                                 \"These two deals, 249262 & 565833 are changed for February out. I will tell the risk team that all new deals at NGPL/Nipsco should be at NIPS/NGPL - 9260. Is this correct? PL\"\\n  \n","3                                                                                                                                                                                                                                                                                                                               don't know yet probably thurs. are you still in tonight - i think probably 2 rows\\n  \n","4   \"Steve filled in the detail for me on the LG&E issue mentioned in the Law bullets this morning. A valve was left open between an LG&E facility and Northern for an approximate 3 month period (I'm not sure how recent). LG&E's gas loss data indicates that the amount of gas that inadvertently got into Northern was approx. 180,000 MMBtu, with a value of about $500,000. Rockey and Steve have talked ...  \n","5                                                                                                                                                                                                                                                                                                                                                    Any word. I've been waiting for a while on this issue. Kevin\\n  \n","6   \"---------------------- Forwarded by Vince J Kaminski/HOU/ECT on 04/05/2000 04:58 PM --------------------------- ---------------------- Forwarded by Vince J Kaminski/HOU/ECT on 03/13/2000 02:03 PM --------------------------- Hi Vince: I haven't talked to you in a while and wanted to touch base with you on a couple of issues. 1. Is there someone at Enron who would be interested in speaking to o...  \n","7                                                                                                                                                                                                                                                                                                                  I received your riddle from Amy Yueh and my answer is NOTHING! Let me know if I am right. Andy\\n  \n","8                                                                                                                               \"Lynn -- FYI, here is my draft of the fuel filing for TW. This is very much a work in progress and we're having a meeting tomorrow to discuss logistics. We've invited Darrell and Richard to the meeting; if there is anyone else you think we should include just let me know.\"\\n  \n","9                                                                                                                 Professor Ronn: I received an e-mail from Meg Brooks in the admissions office stating that I would be hearing from them this week. I hope things work out and I can be contributing to and learning from the program next year. Thanks again for all your help. Hope to hear from you soon. Ben\\n  \n","10  \"Hi Melba, Will you please create a new product that reads as follows: The interconnection between Alliance Pipeline Company and Vector Pipeline Company located in Will County, Illinois. I also would like a NGPL Nicor product set up that uses the Gas Daily Posting for \"\"Chicago LDC-Large end-users\"\" midpoint. I will need to have the Vector Alliance set up for tomorrow if possible. Thanks Melba...  \n","11                                                                                                                                                                                                                                                                                               These dates and times are confirmed. Please let me know if you have any additional questions. Thanks! Ben Rogers\\n  \n","12                                                                                                  \"Whatcha think?---------------------- Forwarded by Kay Mann/Corp/Enron on 06/06/2001 06:41 PM --------------------------- Hi Kay! BGE is issuing an RFP, and they have drafted the attached CA.....Can you take a peek and see if it would be acceptable? http://supplier.bge.com/rfp/ca.doc Thanks! Janelle\"\\n  \n","13                                                                                                                                                                                                                  \"Kevin -- I've looked at the pooling agreement and I agree with you -- I don't believe taking the compressor down is at all limited under the agreement. Any further questions, let me know.\"\\n  \n","14  \"---------------------- Forwarded by Vince J Kaminski/HOU/ECT on 10/27/2000 03:11 PM --------------------------- Norma, I have attached an updated org chart for my team. I think the following changes need to be made 1. Paulo Issler is reporting to Zimin Lu 2. Shalesh Ganjoo is reporting to me (who does the system show as his supervisor, Zimin?) Also, I just found out that Chonawee did not get ...  \n","15  \"I spoke with Steve Stojic about this and he had the same concern. Even though I was trying to keep this out of the new rate schedule requirements by not making it a rate schedule, I believe I still managed to cover many checklist items. One that is not covered is testimony similar to what might be filed in a rate case, in order to justify market based rates. We are planning to include a reven...  \n","16                                                                                                            \"In all the excitement of the day, I completely forgot to check on this deal. I know I got a call from one of you guys about the \"\"New Counterparty\"\" Diana Scholtes put in here; but I'm not sure now what I was supposed to check on. Give me a call in the morning and let me know. Sorry! Kate\"\\n  \n","17  \"Well I am looking for 24 Round trip tickets (48 one-ways) and was looking to spend around $200 per round trip ($100 for each one-way). What is your offer on that size? Thanks for you PROMPT attention because I am trying to book airfare today on one airline or another. ------------------------- Let's put it like this: How many do you want and at what price are you willing to pay? Content-type:...  \n","18                                                                                                                                                                                                                                                          Thanks for your list. I will add these to our Hot List. Let me know about the A/R. Thanks. Would love to hear about your trip in more detail. --Sally\\n  \n","19  \"Chris, Below Wei explains what the option class cd column should contain. Please fix this error before saving the file out tonight. If you have any questions let me know. Thanks, Robin ---------------------- Forwarded by Robin Rodrigue/HOU/ECT on 01/17/2001 02:52 PM --------------------------- Robin, I have set it up for this spreadsheet and I already ran it in stage database. The only error ...  "],"text/html":["\n","  <div id=\"df-7c839f56-add5-40f8-876d-ddcd828cde8c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>P(predicted class confidence)</th>\n","      <th>Human label</th>\n","      <th>Prediction</th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9</td>\n","      <td>0.998674</td>\n","      <td>high</td>\n","      <td>0</td>\n","      <td>\"How do we get the right focus on support for Calgary? Mark Taylor wanted to hold up transferring confirm responsibility to Calgary, so we still need to provide excellent service to them. While I can appreciate that Diane is out, that should actually be a red flag to us to pay even closer attention to the Calgary stuff. Every screw up looks big to them, and they can't understand how mistakes c...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>303</td>\n","      <td>0.997984</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>\"---------------------- Forwarded by Drew Fossum/ET&amp;S/Enron on 04/03/2001 04:47 PM --------------------------- This is to confirm your attendance for the Friday, April 20, 2001, Executive Forum to be hosted by The Office of the Chairman. The Forum will begin at 3:00 p.m. and ends at 4:30 p.m in the Enron Building 50M. If you have any additional questions, please feel free to give me a call. Th...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>469</td>\n","      <td>0.992540</td>\n","      <td>high</td>\n","      <td>0</td>\n","      <td>\"These two deals, 249262 &amp; 565833 are changed for February out. I will tell the risk team that all new deals at NGPL/Nipsco should be at NIPS/NGPL - 9260. Is this correct? PL\"\\n</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>137</td>\n","      <td>0.992022</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>don't know yet probably thurs. are you still in tonight - i think probably 2 rows\\n</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>39</td>\n","      <td>0.991358</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>\"Steve filled in the detail for me on the LG&amp;E issue mentioned in the Law bullets this morning. A valve was left open between an LG&amp;E facility and Northern for an approximate 3 month period (I'm not sure how recent). LG&amp;E's gas loss data indicates that the amount of gas that inadvertently got into Northern was approx. 180,000 MMBtu, with a value of about $500,000. Rockey and Steve have talked ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>407</td>\n","      <td>0.986219</td>\n","      <td>high</td>\n","      <td>0</td>\n","      <td>Any word. I've been waiting for a while on this issue. Kevin\\n</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>802</td>\n","      <td>0.985440</td>\n","      <td>high</td>\n","      <td>0</td>\n","      <td>\"---------------------- Forwarded by Vince J Kaminski/HOU/ECT on 04/05/2000 04:58 PM --------------------------- ---------------------- Forwarded by Vince J Kaminski/HOU/ECT on 03/13/2000 02:03 PM --------------------------- Hi Vince: I haven't talked to you in a while and wanted to touch base with you on a couple of issues. 1. Is there someone at Enron who would be interested in speaking to o...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>627</td>\n","      <td>0.983700</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>I received your riddle from Amy Yueh and my answer is NOTHING! Let me know if I am right. Andy\\n</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>828</td>\n","      <td>0.982605</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>\"Lynn -- FYI, here is my draft of the fuel filing for TW. This is very much a work in progress and we're having a meeting tomorrow to discuss logistics. We've invited Darrell and Richard to the meeting; if there is anyone else you think we should include just let me know.\"\\n</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>579</td>\n","      <td>0.981455</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>Professor Ronn: I received an e-mail from Meg Brooks in the admissions office stating that I would be hearing from them this week. I hope things work out and I can be contributing to and learning from the program next year. Thanks again for all your help. Hope to hear from you soon. Ben\\n</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>695</td>\n","      <td>0.969132</td>\n","      <td>high</td>\n","      <td>0</td>\n","      <td>\"Hi Melba, Will you please create a new product that reads as follows: The interconnection between Alliance Pipeline Company and Vector Pipeline Company located in Will County, Illinois. I also would like a NGPL Nicor product set up that uses the Gas Daily Posting for \"\"Chicago LDC-Large end-users\"\" midpoint. I will need to have the Vector Alliance set up for tomorrow if possible. Thanks Melba...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>25</td>\n","      <td>0.969113</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>These dates and times are confirmed. Please let me know if you have any additional questions. Thanks! Ben Rogers\\n</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>509</td>\n","      <td>0.966901</td>\n","      <td>high</td>\n","      <td>0</td>\n","      <td>\"Whatcha think?---------------------- Forwarded by Kay Mann/Corp/Enron on 06/06/2001 06:41 PM --------------------------- Hi Kay! BGE is issuing an RFP, and they have drafted the attached CA.....Can you take a peek and see if it would be acceptable? http://supplier.bge.com/rfp/ca.doc Thanks! Janelle\"\\n</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>784</td>\n","      <td>0.961477</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>\"Kevin -- I've looked at the pooling agreement and I agree with you -- I don't believe taking the compressor down is at all limited under the agreement. Any further questions, let me know.\"\\n</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>432</td>\n","      <td>0.959798</td>\n","      <td>low</td>\n","      <td>1</td>\n","      <td>\"---------------------- Forwarded by Vince J Kaminski/HOU/ECT on 10/27/2000 03:11 PM --------------------------- Norma, I have attached an updated org chart for my team. I think the following changes need to be made 1. Paulo Issler is reporting to Zimin Lu 2. Shalesh Ganjoo is reporting to me (who does the system show as his supervisor, Zimin?) Also, I just found out that Chonawee did not get ...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>657</td>\n","      <td>0.949267</td>\n","      <td>high</td>\n","      <td>0</td>\n","      <td>\"I spoke with Steve Stojic about this and he had the same concern. Even though I was trying to keep this out of the new rate schedule requirements by not making it a rate schedule, I believe I still managed to cover many checklist items. One that is not covered is testimony similar to what might be filed in a rate case, in order to justify market based rates. We are planning to include a reven...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>75</td>\n","      <td>0.946400</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>\"In all the excitement of the day, I completely forgot to check on this deal. I know I got a call from one of you guys about the \"\"New Counterparty\"\" Diana Scholtes put in here; but I'm not sure now what I was supposed to check on. Give me a call in the morning and let me know. Sorry! Kate\"\\n</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>805</td>\n","      <td>0.943940</td>\n","      <td>high</td>\n","      <td>0</td>\n","      <td>\"Well I am looking for 24 Round trip tickets (48 one-ways) and was looking to spend around $200 per round trip ($100 for each one-way). What is your offer on that size? Thanks for you PROMPT attention because I am trying to book airfare today on one airline or another. ------------------------- Let's put it like this: How many do you want and at what price are you willing to pay? Content-type:...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>537</td>\n","      <td>0.938878</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>Thanks for your list. I will add these to our Hot List. Let me know about the A/R. Thanks. Would love to hear about your trip in more detail. --Sally\\n</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>801</td>\n","      <td>0.936306</td>\n","      <td>medium</td>\n","      <td>0</td>\n","      <td>\"Chris, Below Wei explains what the option class cd column should contain. Please fix this error before saving the file out tonight. If you have any questions let me know. Thanks, Robin ---------------------- Forwarded by Robin Rodrigue/HOU/ECT on 01/17/2001 02:52 PM --------------------------- Robin, I have set it up for this spreadsheet and I already ran it in stage database. The only error ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c839f56-add5-40f8-876d-ddcd828cde8c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7c839f56-add5-40f8-876d-ddcd828cde8c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7c839f56-add5-40f8-876d-ddcd828cde8c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","source":["\n","*   First of all, we noticed that among the 20 most wrongly predicted labels, only one of them is wrongly predicted as medium, while all other 19 are wrongly predicted as low and none of them are wrongly predicted as high. This observation makes sense considering our unbalanced class size in our dataset.\n","*   Another common characteristic of these wrongly classified emails is they include forward emails. Forward email is a tricky part in our labeling task. And we established a guideline defining when should we judge base on the forwarded email and when should we decide base on the original email. However, this domain knowledge was not incorportated into our model through feature engineering, resulting our model to make the systematic problem where instead of looking only at texts in front of or after the forward division line, the model analyze the entire email as a whole and that impacted the model judgement. \n","*   Additionally, the messages all contain a lot of the stop words that are wrongly classified as important features indicating low urgency. It is hard to discover any other features if not setting aside the stop words in the messages. \n","*   Among these messages, there are actually lots of words that were picked up by our features. But because those features didn't make it to the top feature list. They are linked with small coefficients. And that causes the model's failure in recognizing them and correctly classifing the email. \n","*   Overall, our model has decent accuracy on low features because of the unbalanced datasize, there might exist some bias in the model and more information on low data points in traning set. However, less accuracy for high and medium urgency to respond emails because of not enough training data and unclear/complicated judgement guidelines when annotating manually. \n","*   Moving forward, the model could benefit from removing stop words, none relavent punctuations, better and clearer annotated data, back probagation, model ensemble, better features with more precise domain knowledge, and most importantly a more balanced dataset. \n","\n","\n","\n"],"metadata":{"id":"zqCvnKwwkuXj"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}