564	Explains Technical Concepts	An alternative representation for baseNPs  has been put tbrward by Ramshaw and Mar-  cus (1995). They have defined baseNP recog-  nition as a tagging task: words can be inside a  baseNP (I) or outside a baseNP (O). In the case  that one baseNP immediately follows another  baseNP, the first word in the second baseNP  receives tag B. Example:  Ino early1 trading1 ino Hongi Kongi  MondayB ,o gold1 waso quotedo ato  $I 366.501 anu ounce1 .o  This set of three tags is sufficient for encod-  ing baseNP structures since these structures are  nonrecursive and nonoverlapping.  Tjong Kiln Sang (2000) outlines alternative  versions of this tagging representation. First,  the B tag can be used for tile first word of ev-  ery baseNP (IOB2 representation). Second, in-  stead of the B tag an E tag can be used to  nlark the last word of a baseNP immediately  before another baseNP (IOE1). And third, the  E tag call be used for every noun phrase final  word (IOE2). He used the Ramshaw and Mar-  cus (1995) representation as well (IOB1). We  will use these tbur tagging representations and  the O+C representation for the system-internal  combination experiments.
93	Assumes Prior Knowledge	7 Incorporating heuristics  From the study of the erroneously extracted  semantic relations certain systematic errors were  detected. For example, adjectives, adverbs or  adjunctive nouns that occur interpolating in  otherwise similar contexts lead to the extraction of  spurious pairs. Consider ~br example the phrases:  "11 c~6~qml "nlg ztgl\]g "Cllg J~v~ixrll~" (= the increase  of the benzine price) and "11 c~6~qcn 1 zqg "cqa\]g  ~d0~31mlg zqg \[~cv~iwl?' (=the increase of the  di,sposal benzine price). Every algorithm based on  word adjacency data outputs as erroneous hits the  pairs {benzine-disposal} nd { increase-disposal}.  A rule that was applied to deal with this problem  is:  If wiGS m and wjGSn have s imi lar   contexts,  count the pair (wi,w j) as  a hit on ly  if wi~wj+3 and wj~wi+1.  Such contextual rules can be applied only using  the cross-correlation method for the context  similarity estimation (either pattern-based or  word-based).
185	Explains Technical Concepts	2. Background  A.  Re la t ive  and  Abso lu te  I l l - fo rm-   edness   Weischedel and Sondheimer (~,Veischedel, 1983)  have distinguished two types of ill-formedness: ab-  solute ill-formedness and relative ill-formedness.  Roughly speaking, an absolutely ill-formed input  is one which does not conform to the syntactic  and semantic onstraints of the natural language  or the sublanguage; a relatively ill-formed input is  one which is outside the coverage of a particular  natural language interface. Our concern is pri-  marily with relative ill-formedness. For complex  domains, we believe that it will be difficult to cre-  ate complete semantic models, and therefore that  relatively ill-formed input will be a serious prob-  lem - a problem that it will be hard for users to  remedy without suitable feedback.
598	Assumes Prior Knowledge	Abst ract   Summarization of multiple documents featur-  ing multiple topics is discussed. The exam-  ple trea.ted here consists of fifty articles about  the Peru hostage incident tbr \])ecember 1996  through April 1997. They include a. lot of top-  ics such as opening, negotiation, ending, and  so on. The method proposed in this paper is  based on spreading activation over documents  syntactically and semantically annotated with  GI)A (Global l)ocument Annotation) tags. The  method extracts important documents aald im-  portant parts therein, and creates a network  consisting of important entities and relations  among them. It also identifies cross-document  coreferences to replace expressions with more  concrete ones. The method is essentially multi~  lingua\] due to the language-independence of the  GDA tagset. This tagset can provide a stan-  dard fornm.t br the study on the transfbrmation  and/or generation stage of summarization pro-  cess, among other natural language processing  tasks.
563	Explains Technical Concepts	2.3. Task-Specific Language Model Estimation  The Switchboard Corpus \[11\] is a telephone database  consisting of spontaneous conversation  a number of different  topics. The Credit Card task is to spot 20 keywords and their  variants where both the keywords and the test set focus on a sub-  set of the Switchboard conversations pertaining to credit cards.  To estimate the language model for this task, we could (1) use a  small amount of task-specific training data that focuses only on  the credit card topic, (2) use a large amount of task-independent  training data, or (3) combine the task-specific training with the  task-indepondent training data.
181	Assumes Prior Knowledge	Abstract Using examples of the transfer-based MT system between Czech and Russian RUSLAN and the word-for-word MT system with morphological disambiguation between Czech and Slovak (~ESILKO we argue that for really close languages it is possible to obtain better translation quality by means of simpler methods. The problem of translation to a group of typologically similar languages using a pivot language is also discussed here.
650	Explains Technical Concepts	Decoding uses the cube-pruning algorithm of  (Huang and Chiang, 2007) with a 7-word distor- tion limit. Contrary to the usual implementation  of distortion limits, we allow a new phrase to end  more than 7 words past the first non-covered  word, as long as the new phrase starts within 7  words from the first non-covered word. Notwith- standing the distortion limit, contiguous phrases  can always be swapped. Out-of-vocabulary  (OOV) source words are passed through un- changed to the target. Loglinear weights are  tuned with Och's max-BLEU algorithm over lat- tices (Macherey et al, 2008); more details about  lattice MERT are given in the next section. The  second pass rescores 1000-best lists produced by  the first pass, with additional features including  various LM and IBM-model probabilities; ngram,  length, and reordering posterior probabilities and  frequencies; and quote and parenthesis mismatch  indicators. To improve the quality of the maxima  found by MERT when using large sets of partial- ly-overlapping rescoring features, we use greedy  feature selection, first expanding from a baseline  set, then pruning.
235	Explains Non-Technical Concepts	Extensive research as been carried out on methods  of inputting Kanji in an attempt to .realize rapid and easy  input. Among the methods proposed, Kana-Kanji  translation appears to be the most promising. In this  method, input sentences are entered in Kana using a  conventional typewriter keyboard, and those parts of the  sentences which should be written in Kanji are translated  into Kanji automatically. In this process a non-segmented  input form is desirable for users because there is no  custom of segmentation i writing Japanese sentences.  Therefore, the ultimate goal of a Kana-Kanji translation  scheme should be to achieve error-free translation from  non-segmented Kana input sentences.  This paper describes a system for achieving high  accuracy in the Kana-Kanji translation of non-segmented  input kana sentences.
28	Explains Non-Technical Concepts	8. EXPERIMENT DES IGN  For our pilot experiment, we created two data sets,  one based on relevant sentence judgments, the other  based on model summaries (Section 8.1). We then  defined a modified version of the 11-point average  recall precision (Section 8.2) to use as our evaluation  measure. We then performed experiments as  described in Section 9 to evaluate the effects of  MMR, query expansion, and compression.
614	Explains Non-Technical Concepts	For the first time we were able to perform a well-controlled  expefirnent to answer this question on the large vocabulary WSJ  corpus. The amount of training data is the same in both cases. In  one condition, there are 12 speakers (6 male and 6 female) with  600 sentences each. In the other case, there are 84 speakers with a  total of 7,200 sentences. In both cases, all of the sentence scripts  are unique. "nre speakers in both sets were selected randomly,  without any effort to cover the general population. In both cases,  we used separate models for male and female speakers.  In a second experiment, we repeated another experiment that  had previously been run only on the RM corpus. Instead of  pooling all of the training data (for one gender) and estimating  a single model, we trained on the speech of each speaker sep-  arately, and then combined all of the resulting models simply  by averaging the densities of the resulting models. We had pre-  viously found that this method worked well when each speaker  had a substantial mount of training speech (enough to estimate a  speaker-dependent model), and all of the speakers had the same  sentences in their training. But in this experiment, we also com-  puted separate speaker-dependent models for the speakers with  50-100 utterances, and each speaker had different sentences.
469	Assumes Prior Knowledge	Thus a query for car or automobile will retrieve ssentially identical results; vehicle will be less accurate but will still retrieve many of the same images. So while word choice may be a significant consideration for a system like that of Jang et al, 1999, its impact on PictureQuest is minimal. The use of WordNet as an aid to information retrieval is controversial, and some studies indicate it is more hindrance than help (e.g. Voorhees 1993, 1994, Smeaton, Kelledy and O'Donnell 1995). WordNet uses extremely fine-grained distinctions, which can interfere with precision even in monolingual information retrieval. In a cross-language application, the additional senses can add confounding mistranslations. If, on the other hand, WordNet expansion is constrained, the correct ranslation may be missed, lowering recall. In the PictureQuest application, we have tuned WordNet expansion levels and the corresponding weights attached to them so that WordNet serves to increase recall with minimal impact on precision (Flank 2000). This tuned expansion appears to be beneficial in the cross-language application as well. Gilarranz, Gonzalo and Verdejo (1997) point out that, for cross-language information retrieval, some precision is lost in any case, and WordNet is more likely to enhance cross-linguistic than monolingual applications.
339	Explains Technical Concepts	The SSM training algorithm \[16\] iterates between seg-  mentation and maximum likelihood parameter estima-  tion, so that during the parameter estimation phase of  each iteration, the segmentation f that pass gives a set  of known phonetic boundaries. Additionally, for a given  phonetic segmentation, the assignment of observations  to regions of the model is uniquely determined. SSM  training is similar to IIMM "Viterbi training", in which  training data is segmented using the most likely state  sequence and model parameters are updated using this  segmentation. Although it is possible to define an SSM  training algorithm equivalent to the Baum-Welch algo-  rithm for HMMs, the computation is prohibitive for the  SSM because of the large effective state space.
627	Assumes Prior Knowledge	In the current version of FCDCN the SNR is varied over a  range of 30 dB in 1-dB steps, with the lowest SNR set equal  to the estimated noise level. At each SNR compensation vec-  tors are computed for each of 8 separate VQ clusters.  Figure 1 illustrates ome typical compensation vectors  obtained with the FCDCN algorithm, computed using the  standard closetalking Sennheiser HMD-414 microphone and  the unidirectional desktop PCC-160 microphone used as the  target environment. The vectors are computed at the extreme  SNRs of 0 and 29 dB, as well as at 5 dB. These curves are  obtained by calculating the cosine transform of the cepstral  compensation vectors, so they provide an estimate of the  effective spectral profile of the compensation vectors. The  horizontal axis represents frequency, warped nonlinearly  according to the mel scale \[9\]. The maximum frequency cor-  responds to the Nyquist frequency, 8000 Hz. We note that he  spectral profile of the compensation vector varies with SNR,  and that especially for the intermediate SNRs the various VQ  clusters require compensation vectors of different spectral  shapes. The compensation curves for 0-dB SNR average to  zero dB at low frequencies by design.
805	Assumes Prior Knowledge	However, a major obstacle to this approach is the lack of parallel corpora for model training. Only a few such corpora exist, including the Hansard English-French corpus and the HKUST English- Chinese corpus (Wu, 1994). In this paper, we will describe a method which automatically searches for parallel texts on the Web. We will discuss the text mining algorithm we adopted, some issues in trans- lation model training using the generated parallel corpus, and finally the translation model's perfor- mance in CLIR.
638	Assumes Prior Knowledge	2 Para l le l  Text  M in ing  A lgor i thm The PTMiner system is an intelligent Web agent that is designed to search for large amounts of paral- lel text on the Web. The mining algorithm is largely language independent. It can thus be adapted to other language pairs with only minor modifications. Taking advantage ofWeb search engines as much as possible, PTMiner implements he following steps (illustrated in Fig. 1): 1 Search for candidate sites - Using existing Web search engines, search for the candidate sites that may contain parallel pages; 2 File name fetching - For each candidate site, fetch the URLs of Web pages that are indexed by the search engines; 3 Host crawling - Starting from the URLs col- lected in the previous tep, search through each candidate site separately for more URLs; 4 Pair scan - From the obtained URLs of each site, scan for possible parallel pairs; 5 Download and verifying - Download the parallel pages, determine file size, language, and charac- ter set of each page, and filter out non-parallel pairs.
553	Explains Technical Concepts	3 Further Development on Metaphori- cal Affect Detection  Without pre-defined constrained scripts, our  original system has been developed for 14-16  year old school students to conduct creative im- provisation within highly emotionally charged  scenarios. Various metaphorical expressions  were used to convey emotions (K?vecses,  1998), which are theoretically and practically  challenging and draw our attention.  Metaphorical language can be used to convey  emotions implicitly and explicitly, which also  inspires cognitive semanticists (K?vecses,  1998). In our previous study (Zhang et al  2008b; 2009), we detected affect from several  comparatively simple metaphorical affective  phenomena. Another type of comparatively  complex metaphor has also drawn our attention,  i.e. the cooking metaphor. Very often, the agent  himself/herself would become the victim of  slow or intensive cooking (e.g. grilled, cooked).  Or one agent can perform cooking like actions  towards another agent to realize punishment or  torture. Examples are as follows, ?he basted her  with flattery to get the job?, ?she knew she was  fried when the teacher handed back her paper?.
551	Assumes Prior Knowledge	The Cross-Language Multimedia Retrieval Application This paper offers several original contributions to the literature on cross- language information retrieval. First, the choice of application is novel, and significant because it simplifies the language problem enough to make it tractable. Because the objects retrieved are images and not text, they are instantly comprehensible to the user regardless of language issues. This fact makes it possible for users to perform a relevance assessment without he need for any kind of translation. More important, users themselves can select objects of interest, without recourse to translation. The images are, in fact, associated with caption information, but, even in the monolingual system, few users ever even view the captions. It should be noted that most of the images in PictureQuest are utilized for advertising and publishing, rather than for news applications. Users of history and news photos do tend to check the captions, and often users in publishing will view the captions. For advertising, however, what the image itself conveys is far more important than the circumstances under which it was created.
1	Explains Technical Concepts	3.2 UN Data While we already used the UN data in the lan- guage model for the Spanish?English and French? English language pairs, we now also add it to the translation model. The corpus is very large, four times bigger than the already used training data, but relatively out of domain, as indicated by the high perplexity and low interpolation weight during language model interpolation (recall Table 2). Adding the corpus to the four systems gives im- provements of +0.60 BLEU points on average. For details refer back to Table 1.
354	Explains Technical Concepts	During this process contextual data are not  maintained in memory; instead the detection of a  common pattern in both sentences results to the  storage of several hits (i.e. candidate silnilar word  pairs) or to the increase of their corresponding  similarity measure according to the pattern  similarity ot'their contexts.
897	Assumes Prior Knowledge	Event entities include named entities and  high-frequency entities. Named entities denote  people, locations, organizations, dates, etc.  High-frequency entities are common nouns or  NPs that frequently participate in news events.  Filatova and Hatzivassiloglou (2004) take the  top 10 most frequent entities and Li et al (2006) take the entities with frequency > 10. Rather  than using a fixed threshold, we reformulate  ?high-frequency? as relative statistics based on  (assumed) Gaussian distribution of the entities  and consider those with z-score > 1 as candidate  event entities.
365	Explains Technical Concepts	Our preferred implementation of the mixture  expert in the dynamic fusion function is a multilayer  feedforward neural network, with output nodes  corresponding to the linear weights of the linear  fusion function. However, given that our real goal is  to maximize precision, rather than to replicate the  weights exactly, a straightforward application of  backpropogation to train such a network to replicate  the target weights is inappropriate. The optimal  linear weights are likely to be on "plateaus" with  respect o precision, with little change in precision in  response to large changes in linear weights. We are  currently investigating alternative ways of training  the mixture xpert in the dynamic fusion model.
496	Assumes Prior Knowledge	INTRODUCTION  Information retrieval researchers have long  appreciated the value of combining, or ffilsing,  multiple retrieval systems' relevance scores for a set  of documents to improve retrieval performance.  However, it is only recently that researchers have  begun to consider adjusting the score fusion method  to the user's topic and initial results. This study  explores the value of fusing multiple retrieval  systems' scores in a manner that adjusts to: the  semantic and syntactic features of the user's natural  language query, the various systems' biases toward  long or short documents, and the extent to which the  scores produced by the multiple systems are  statistically independent.
423	Explains Technical Concepts	3.6 German?English For the German?English language direction, we used two additional processing steps that have shown to be successful in the past, and again re- sulted in significant gains. We split large words based on word frequen- cies to tackle the problem of word compounds in German (Koehn and Knight, 2003). Secondly, we re-order the German input to the decoder (and the German side of the training data) to align more closely to the English target language (Collins et al, 2005). The two methods improve +0.58 and +0.52 over the baseline individually, and +1.34 when com- bined. See also Table 8.
516	Explains Non-Technical Concepts	3. TWO-STAGE AD-HOC STRATEGY  Automatic ad-hoc retrieval refers to the  environment where a user attempts to retrieve relevant  documents from an existing collection by issuing 'any'  query. We have experimented only with natural  language queries that are derived from TREC topics. It  is a difficult problem because the query wordings are  unknown beforehand, and its topical content is  unpredictable? Moreover, there will not be any  example relevant documents that a system can rely on  for training purposes like in a routing situation.  To improve the accuracy of ad-hoc retrieval, it is  now a common practice to adopt a 2-stage retrieval  strategy. Under the right circumstances this can give  substantial improvements over single stage.
934	Explains Non-Technical Concepts	1. Czech-to-Russian MT system RUSLAN 1.1 History The first attempt o verify the hypothesis that related languages are easier to translate started in mid 80s at Charles University in Prague. The project was called RUSLAN and aimed at the translation of documentation i the domain of operating systems for mainframe computers. It was developed in cooperation with the Research Institute of Mathematical Machines in Prague. At that time in former COMECON countries it was obligatory to translate any kind of documentation to such systems into Russian. The work on the Czech-to-Russian MT system RUSLAN (cf. Oliva (1989)) started in 1985. It was terminated in 1990 (with COMECON gone) for the lack of funding.
567	Explains Technical Concepts	Retrieval Systems  We used five retrieval systems to generate  relevance scores for query-document pairs:  Fuzzy Boolean (FB). This system translates a query  into a Boolean expression in which the terminals are  single terms, compound nominals, and proper nouns;  instantiates the terminals in the expression with the  document's tfidfweights; and applies fuzzy Boolean  semantics to resolve the instantiated expression into a  scalar elevance score.
583	Explains Technical Concepts	A significant amount of recent work has shown the power of discriminatively-trained probabilistic graphical models for NLP tasks (Lafferty et al, 2001; Sutton and McCallum, 2007; Wainwright and Jordan, 2008). The superiority of graphical model is its ability to represent a large number of random variables as a family of probability dis- tributions that factorize according to an underly- ing graph, and capture complex dependencies be- tween variables. And this progress has begun to make the joint learning approach possible. In this paper we study and formally define the joint problem of entity identification and relation extraction from encyclopedia text, and we propose a joint paradigm in a single coherent framework to perform both subtasks simultaneously. This framework is based on undirected probabilistic graphical models with arbitrary graphical struc- ture. We show how the parameters in this model can be estimated efficiently. More importantly, we propose a new inference method ? collective it- erative classification (CIC), to find the maximum a posteriori (MAP) assignments for both entities and relations. We perform extensive experiments on real-world data from Wikipedia for this task, and substantial gains are obtained over state-of- the-art probabilistic pipeline and joint models, il- lustrating the promise of our approach.
373	Explains Non-Technical Concepts	5.7 Co l lec t ion  Enr ichment  fo r  Ch inese  IR   In Section 4.1, we observed that collection  enrichment is an effective strategy to improve English  ad-hoc retrieval, especially for short queries. Here, we  study if this is also true for Chinese.  The TREC5 Chinese collection came from two  sources: 24,988 documents from XinHua News  Agency (xh) and 139,801 from Peoples' Daily  newspaper (pd). In PIRCS, they were segmented into  sub-documents of 38,287 and 193,240 items  respectively. We use the combined TREC5 and 6  queries numbering 54, and do retrieval with the xh  collection as the target but enriched with pd, and vice  versa. Some queries do not have any relevants in one  of the sub-collections and the actual number of queries  for evaluation is less. This is done for the both long  and short (title only) versions of the queries. Results  are tabulated in Table 8.
492	Explains Non-Technical Concepts	In our application, although the hidden story  sub-themes used in the scenarios are not that  dramatic, they are still highly emotionally  charged and used as the signals for potential  changes of emotional context for each character.  E.g. In the school bullying scenario (which is  mainly about the bully, Mayid, is picking on the  new comer to the school, Lisa. Lisa?s friends,  Elise and Dave, are trying to stop the bullying.  The school teacher, Mrs Parton, also tries to  find out what is going on), the director mainly  provided interventions based on several main  sub-themes of the story to push the improvisa- tion forward, i.e. ?Mayid starts bullying Lisa?,  ?why Lisa is crying?, ?why Mayid is so nasty/a  bully?, ?how Mayid feels when his uncle finds  out about his behavior? etc. From the inspection  of the recorded transcripts, when discussing the  topic of ?why Lisa is crying?, we noticed that  Mayid (the bully) tends to be really aggressive  and rude, while Lisa (the bullied victim) tends  to be upset and other characters (Lisa?s friends  and the school teacher) are inclined to show  anger at Mayid. For the improvisation of the  hidden story sub-theme ?why Mayid is so nas- ty/a bully?, the big bully changes from rude and  aggressive to sad and embarrassed (e.g. because  he is abused by his uncle), while Lisa and other  characters become sympathetic (and sometimes  caring) about Mayid. Usually all characters are  trying to create the ?improvisational mood? ac- cording to the guidance of the hidden story sub- themes (provided via director?s intervention).
57	Assumes Prior Knowledge	1 Introduction This paper presents the phrase-based machine translation system developed at RALI in order to participate in both the French-English and English-French translation tasks. In these two tasks, we used all the corpora supplied for the con- straint data condition apart from the LDC Giga- word corpora.
436	Mathematically-Oriented Paragraph	Therefore, we produce a new set of results for  the evaluation of the updated affect detection  component with metaphorical and context-based  affect detection based on the analysis of some  recorded transcripts of school bullying scenario.  Generally two human judges (not engaged in  any development stage) marked up the affect of  150 turn-taking user input from the recorded  another 4 transcripts from school bullying sce- nario (different from those used for the training  of Markov chains). In order to verify the effi- ciency of the new developments, we provide  Cohen?s Kappa inter-agreements for EMMA?s  performance with and without the new devel- opments for the detection of the most common- ly used 12 affective states. In the school bully- ing scenario, EMMA played a minor bit-part  character (Lisa?s friend: Dave). The agreement  for human judge A/B is 0.45. The inter- agreements between human judge A/B and  EMMA with and without the new developments  are presented in Table 1.    Human  Judge A  Human  Judge B  EMMA (pre- vious version)  0.38 0.30  EMMA (new  version)  0.40 0.32  Table 1: Inter-agreements between human  judges and EMMA with and without the new  developments
210	Explains Non-Technical Concepts	4. Conclusions The accuracy of the translation achieved by our system justifies the hypothesis that word-for- word translation might be a solution for MT of really closely related languages. The remaining problems to be solved are problems with the one- to many or many-to-many translation, where the lack of information in glossaries and dictionaries sometimes causes an unnecessary translation error.
574	Assumes Prior Knowledge	6.4 Experimental Results Table 1 shows the performance of entity identifi- cation and Table 2 shows the overall performance of relation extraction 3, respectively. Our model substantially outperforms all baseline models on the overall F-measure for entity identification, re- sulting in an relative error reduction of up to 38.97% and 28.83% compared to CRF+CRF and DCRF, respectively. For relation extraction, the improvements on the F-measure over CRF+CRF and DCRF are 4.68% and 3.75%. McNemar?s paired tests show that all improvements of our model over baseline models are statistically sig- nificant. These results demonstrate the merits of our approach by capturing tight interactions between entities and relations to explore mutual benefits. The pipeline model CRF+CRF per- forms entity identification and relation extraction independently, and suffers from problems such as error accumulation. For example, CRF+CRF cannot extract the member of relation between the secondary entity Republican and the princi- pal entity George W. Bush, since the organiza- tion name Republican is incorrectly labeled as a miscellaneous. By modeling interactions between two subtasks, enhanced performance is achieved, as illustrated by DCRF. Unfortunately, training a DCRF model with unobserved nodes (hidden variables) makes this approach difficult to opti- 3Due to space limitation, we only present the overall per- formance and omit the performance for 53 relation types. Table 2: Comparative performance of our model, CRF+CRF, and DCRF models for relation extrac- tion.
796	Explains Technical Concepts	1. Int roduct ion  Natural language interfaces have achieved a lim-  ited success in small, well circumscribed omains,  such as query systems for simple data bases. One  task in constructing such an interface is identi-  fying the relationships which exist in a domain,  and the possible linguistic expressions of these re-  lationships. As we set our sights on more complex  domains, it will become much harder to develop  a complete or nearly complete catalog of the rele-  vant relationships and linguistic expressions; ub-  stantial gaps will be inevitable. In consequence,  many inputs will be rejected because they fail to  match the semantic/linguistic model we have con-  structed for the domain.
531	Explains Technical Concepts	5 Conclusion Feature extraction for entities is an important  task for opinion mining. The paper proposed a  new method to deal with the problems of the  state-of-the-art double propagation method for  feature extraction. It first uses part-whole and  ?no? patterns to increase recall. It then ranks the  extracted feature candidates by feature impor- tance, which is determined by two factors: fea- ture relevance and feature frequency. The Web  page ranking algorithm HITS was applying to  compute feature relevance. Experimental results  using diverse real-life datasets show promising  results. In our future work, apart from improv- ing the current methods, we also plan to study  the problem of extracting features that are verbs  or verb phrases.
132	Explains Non-Technical Concepts	The question we are addressing in this paper is how to collect and analyse relevant corpora. We be- gin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wiz- ard of Oz-dialogues, especially focusing on the eco- logical validity of the methods. We then describe a method called 'distilling dialogues', which can serve as a supplement to the other two.
828	Explains Technical Concepts	However, the IVR systems are unwieldy to use. Often a user's needs are not covered by the options provided by the system forcing the user to hit 0 to transfer to a human operator. In addition, frequent users often memorize the sequence of options that will get them the desired information. Therefore, any change in the options greatly inconveniences these users. Moreover, there are users that always hit 0 to speak to a live operator because they prefer to deal with a human instead of a machine. Finally, as customer service providers continue to rapidly add functionality to their IVR systems, the size and complexity of these systems continues to grow proportionally. In some popular systems like the IVR system that provides customer service for the Internal Revenue Service (IRS), the user is initially bombarded with 10 different options with each option leading to sub-menus offering a further 3- 5 options, and so on. The total number of nodes in the tree corresponding to the IRS' IVR system is quite large (approximately 100) making it extremely complex to use.
524	Explains Non-Technical Concepts	If using the above conditions it is not possible to  choose a single combination, then word frequency  information is used. throughout this process, unmatched  noun Bunsetsu are left in the stack and are assumed to  depend on verbs which occur later in the path. This case  frame matching is repeated everytime a verb is  encountered in the path. The parsing result of the path is  obtained when the scanning reaches the end of the path.  The same parsing is tried for other paths constructed in  the previous tep. Then the most suitable path is selected  among the successfully parsed paths by measuring the  degree of fit for conditions (3) and (4) above. The result is  the text of the Kana-Kanji translation.
758	Explains Technical Concepts	In this paper we demonstrate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging stage, the parser builds a complete packed chart, storing all sentences consistent with the assigned supertags and the parser?s CCG combinatory rules, with no chart pruning whatsoever. The use of chart pruning techniques, typically some form of beam search, is essential for practical parsing us- ing Penn Treebank parsers (Collins, 1999; Petrov and Klein, 2007; Charniak and Johnson, 2005), as well as practical parsers based on linguistic for- malisms, such as HPSG (Ninomiya et al, 2005) and LFG (Kaplan et al, 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to al- low the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspa- per sentences. Hence it is an obvious question whether chart pruning techniques can be prof- itably applied to the CCG parser. Some previous work (Djordjevic et al, 2007) has investigated this question but with little success.
802	Assumes Prior Knowledge	As an illustrative example, Figure 1 shows the task of entity identification and relationship ex- traction from encyclopedic documents. Here, Abraham Lincoln is the principal entity. Our task consists of assigning a set of pre-defined en- tity types (e.g., PER, DATE, YEAR, and ORG) to segmentations in encyclopedic documents and assigning a set of pre-defined relations (e.g., birth day, birth year, and member of) for each identified secondary entity to the principal entity.
392	Explains Technical Concepts	3 Anatomy o f  the  bus  route  orac le The main components of the bus route information systems are: ? A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser. ? A knowledge base (KB), divided into a semantic KB and an application KB ? A query processor, contalng a routing logic sys- tem, and a route data base. The system is bilingual and contains a double set of dictionary, morphology and grammar. Actually, it detects which language is most probable by count- ing the number of unknown words related to each language, and acts accordingly. The grammars are surprisingly similar, but no effort is made to coa- lesce them. The Norwegian grammar is slightly big- ger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
5	Explains Non-Technical Concepts	The following is a small portion of one rec- orded transcript used for the training of the  Markov chain. The human annotators have  marked up the affect expressed in each turn- taking input.   DIRECTOR: why is Lisa crying?  Elise Brown [caring]: lisa stop cryin  Lisa Murdoch [disagree]: lisa aint crying!!!!   Dave Simons [caring]: i dunno! y u cryin li- sa?  Mayid Rahim [rude]: cuz she dnt realise she  is lucky to b alive   Elise Brown [angry]: beat him up! itss onlii  fat..he'll go down straight away  Mayid Rahim [insulting]: lisa, y u crying? u  big baby!  Mrs Parton [caring]: lisa, r u ok?  For example, the emotional context for May- id from the above example is: ?rude? and ?insult- ing? (we use one letter to represent each emo- tional label, thus in this example, i.e. ?R I?), and  in the similar way, the emotional contexts for  other characters have been derived from the  above example, which are used as the training  data for the Markov chain for the topic ?why  Lisa is crying?. We have summarized the emo- tional context for each story sub-theme for each  character from 4 school bullying transcripts and  used them for the training of the Markov chain.  The topics considered at the training stage in- clude: ?Mayid starts bullying?, ?why is Lisa  crying?, ?why is Mayid nasty/a bully? and ?how  does Mayid feel if his uncle knew about his be- havior?
746	Assumes Prior Knowledge	Figure 9: Scope of the Framework's Transformations  For example, in Figure 9, starting with the  "Input Sentence LI" and passing through  Parsing, Conversion, Transfer, DSyntS  Realization and SSyntS Realization to  "Generated Sentence L2" we obtain an Ll-to-L2  MT system. Starting with "Sentence Planning"  and passing through DSyntS Realization, and  SSyntS Realization (including linearization and  inflection) to "Generated Sentence LI", we  obtain a monolingual NLG system for L1.  So far the framework has been used successfully  for building a wide variety of applications in  different domains and for different languages:
623	Assumes Prior Knowledge	2 Orthography  The researchers mentioned above use finite-state transducers for  stipulating correspondences between surface segments, and un-  derlying segments. In contrast, the system described in this pa-  ll am indebted to Lauri Karttunen and Fernando Pereir~ for all their  help. Laurl supplied the initial English automat~ on which the orthographic  grammar was based, while Fernando furnished some of the Prolog code. Both  provided many helpful suggestion~ and explanations as well. I would also like  to thank Kimmo Koskennlemi for his comments on an earlier draft of this  paper.
981	Explains Technical Concepts	The advantages of senones include not only better param-  eter sharing but also improved pronunciation optimization.  Clustering at the granularity of the state rather than the entire  model (like generalized triphones \[21\]) can keep the dissimi-  lar states of two models apart while the other corresponding  states are merged, and thus lead to better parameter shar-  ing. In addition, senones give us the freedom to use a larger  number of states for each phonetic model to provide more  detailed modeling. Although an increase in the number of  states will increase the total number of free parameters, with  senone sharing redundant s ates can be clustered while others  are uniquely maintained.
675	Assumes Prior Knowledge	1 Introduction  Portage, the statistical machine translation sys- tem of the National Research Council of Canada  (NRC), is a two-pass phrase-based system. The  translation tasks to which it is most often applied  are Chinese to English, English to French (hen- ceforth ?E-F?), and French to English (hence- forth ?F-E?): in recent years we worked on Chi- nese-English translation for the GALE project  and for NIST evaluations, and English and  French are Canada?s two official languages. In  WMT 2010, Portage scored 28.5 BLEU (un- cased) for F-E, but only 27.0 BLEU (uncased)  for E-F. For both language pairs, Portage tru- ecasing caused a loss of 1.4 BLEU; other WMT  systems typically lost around 1.0 BLEU after  truecasing. In Canada, about 80% of translations  between English and French are from English to  French, so we would have preferred better results  for that direction. This paper first describes the  version of Portage that participated in WMT  2010. It then analyzes problems with the system  and describes the solutions we found for some of  them.
275	Explains Technical Concepts	7 Hypergraph-based Discriminative Training Discriminative training with a large number of features has potential to improve the MT perfor- mance. We have implemented the hypergraph- based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. The minimum-risk objec- tive can be optimized by a gradient-based method, where the risk and its gradient can be computed using a second-order expectation semiring. For optimization, we use both L-BFGS (Liu et al, 1989) and Rprop (Riedmiller and Braun, 1993). We have also implemented the average Percep- tron algorithm and forest-reranking (Li and Khu- danpur, 2009b). Since the reference translation may not be in the hypergraph due to pruning or in- herent defficiency of the translation grammar, we need to use an oracle translation (i.e., the transla- tion in the hypergraph that is most simmilar to the reference translation) as a surrogate for training. We implemented the oracle extraction algorithm described by Li and Khudanpur (2009a) for this purpose.
938	Explains Technical Concepts	3.5 Feature Ranking  Although the HITS algorithm can rank features  by feature relevance, the final ranking is not  only determined by relevance. As we discussed  before, feature frequency is another important  factor affecting the final ranking. It is highly  desirable to rank those correct and frequent  features at top because they are more important  than the infrequent ones in opinion mining (or  even other applications). With this in mind, we  put everything together to present the final  algorithm that we use. We use two steps:  Step 1:  Compute feature score using HITS  without considering frequency. Initially, we use  three feature indicators to populate feature  candidates, which form a directed bipartite  graph. Each feature candidate acts as an  authority node in the graph; each feature  indicator acts as a hub node. For node s in the  graph, we let ?? be the hub score and ?? be the  authority score. Then, we initialize ?? and ?? to  1 for all nodes in the graph. We update the  scores of ??  and ??  until they converge using  power iteration. Finally, we normalize ??  and  compute the score S for a feature. Step 2: The final score function considering  the feature frequency is given in Equation (6).   ? ? ????????????????                       (6) where ???????  is the frequency count of  ture?, and S(f) is the authority score of the can- didate feature f. The idea is to push the frequent  candidate features up by multiplying the log of  frequency. Log is taken in order to reduce the  effect of big frequency count numbers.
361	Assumes Prior Knowledge	It is important for the distilling process to have at least an outline of the dialogue system that is under development: Will it for instance have the capacity to recognise users' goals, even if not explicitly stat- ed? Will it be able to reason about the discourse domain? What services will it provide, and what will be outside its capacity to handle?
591	Explains Technical Concepts	The result, shown in Table 7 as 'sw.c+bi' column, was  a further improvement of about 2 to 4% compared with  the best of the two base precision without combination  for both short and long queries. The price to pay is the  doubling of time and space. If  for some applications  the last bit of effectiveness is important, his is a viable  approach. Moreover, this strategy could be realized by  having both retrievals performed in parallel on separate  hardware, thus without affecting the time of retrieval  too much.
352	Explains Technical Concepts	In another step of analysis we will look more  closely at other kinds of realization of nuclear  stresses, such as bitonal pitch accents, to es-  tablish whether they reflect linguistic meanings.  What also remains to be investigated is the as-  signment of pitch accents other than the nuclear  stress. Nuclear stress can be predicted on the  basis of linguistic and pragmatic information,  but it is not clear under which conditions other  pitch accents hould be placed. Our observation  above (See. 4) that pitch accents other than the  nuclear stress are typically placed on the first  syllable of a foot may be a possible motivation.  We are aware that there is controversy among  researchers about rhythm. However, if it turns  out that rhythm is a useful concept in the pre-  diction of non-nuclear pitch accents, then we  will consider including it in our approach.
526	Assumes Prior Knowledge	5.2 German-English For the German to English translation system, the baseline system already uses short-range re- ordering rules and the discriminative word align- ment. This system applies only the language model trained on the News corpus. By adding the possibility to model long-range reorderings with POS-based rules, we could improve the system by 0.6 BLEU points. Adding the big language model using also the English Gigaword corpus we could improve by 0.3 BLEU points. We got an addi- tional improvement by 0.1 BLEU points by adding lattice phrase extraction.
414	Explains Technical Concepts	Spreading activation is carried out in this  network to assess the importance of the ele-  ments. Spreading activation has been applied  to summarization of single GDA-tagged docu-  ments (Hasida et al, 1987; Naga.o and Hasida,  1998). The main conjecture of the present study  is that the merit of spreading activation in that  it evaluates importances of semantic entities  is greater in summarization of multiple docu-  ments with multiple topics, because smnmariza-  tion techniques using docnment structures do  not; apply here, as mentioned earlier.
237	Explains Technical Concepts	3.7 Translation Model Interpolation Finally, we explored a novel domain adaption method for the translation model. Since the in- terpolation of language models is very success- ful, we want to interpolate translation models sim- ilarly. Given interpolation weights, the resulting translation table is a weighted linear interpolation of the individual translation models trained sepa- rately for each domain.
893	Explains Technical Concepts	Abstract  NRC?s Portage system participated in the Eng- lish-French (E-F) and French-English (F-E)  translation tasks of the ACL WMT 2010 eval- uation. The most notable improvement over  earlier versions of Portage is an efficient im- plementation of lattice MERT. While Portage  has typically performed well in Chinese to  English MT evaluations, most recently in the  NIST09 evaluation, our participation in WMT  2010 revealed some interesting differences be- tween Chinese-English and E-F/F-E transla- tion, and alerted us to certain weak spots in  our system. Most of this paper discusses the  problems we found in our system and ways of  fixing them. We learned several lessons that  we think will be of general interest.
552	Assumes Prior Knowledge	2.2 HSQL - Help System for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic ef- fort interfaces to databases. The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Swe- den, Denmark, Finland and Norway (Amble et al, 1990). The aim of HSQL was to build a natural language interface to SQL databases for the Scandi- navian languages Swedish, Danish and Norwegian. These languages are very similar, and the Norwe- gian version of CHAT-80 was easily extended to the other Scandinavian languages. Instead of Geogra- phy, a more typical application area was chosen to be a query system for hospital administration. We decided to target an SQL database of a hospital ad- ministration which had been developed already. The next step was then to change the domain of discourse from Geography to hospital adminis- tration, using the same knowledge representation techniques used in CHAT-80. A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
204	Assumes Prior Knowledge	In this example, the elevator is decelerating, so the acceleration vector should face the opposite direction from the velocity vector. (If the acceleration vector went the same direction as the velocity vector, the speed of the elevator would increase and it would crash into the ground.) This is an important issue in beginning physics; it occurs in five Andes problems. When such errors occur, Andes turns the incorrect item red and provides hints to students in the lower-left corner of the screen. A sample of these hints, shown in the order a student would encounter them, is shown in Fig. 2. But hints are an output-only form of natural language; the student can't take the initiative or ask a question. In addition, there is no way for the system to ask the student a question or lead the student through a multi-step directed line of reasoning. Thus there is no way to use some of the effective rhetorical methods used by skilled human tutors, such as analogy and reductio ad absurdum. Current psychological research suggests that active methods, where students have to answer questions, will improve the performance of tutoring systems.
789	Explains Non-Technical Concepts	Figure 5: Effect of stop lists in C-E translation. ing source. Because these words exist in most align- ments, the statistical model cannot derive correct translations for them. More importantly, their ex- istence greatly affects the accuracy of other transla- tions. They can be taken as translations for many words.
12	Assumes Prior Knowledge	Figure 2: An alignment example using pure length-based method. pus which is built manually. In our case, the prob- abilistic translation model will be used for CLIR. The requirement on our translation model may be less demanding: it is not absolutely necessary that a word t with high p(tls ) always be a true trans- lation of s. It is still useful if t is strongly related to s. For example, although "railway" is not a true translation of "train" (in French), it is highly useful to include "railway" in the translation of a query on "train". This is one of the reasons why we think a less controlled parallel corpus can be used to train a translation model for CLIR.
232	Explains Technical Concepts	3.2 Attributes  We treat the word extraction problem as the  problem of word/nou-word string disambiguation.  The next step is to identify the attributes that are  able to disambiguate word strings flom non-word  strings. The attributes used for the learning  algorithm are as follows.
514	Explains Technical Concepts	3.2 The Parser System The Grammar System The grammar is based on a simple grammar for statements, while questions and commands are de- rived by the use of movements. The grammar formalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisa- tion of Definite Clause Grammars. Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the ba- sic constituents being a form of verb complements. As for Extraposition grammars, a grammar is trans- lated to Definite Clause Grammars, and executed as such.
172	Explains Non-Technical Concepts	We also fit a single linear static fusion function to  the 197 training queries, again using all the data from  those queries. The performance of this static fusion  function on all of the documents for the 50 test  queries constitutes the baseline for the second  research question. To answer this research question,  we will compare the performance of the dynamic  fusion function for the 50 test queries to this baseline.
174	Explains Technical Concepts	Finally, adding MFCDCN to CMN improves the error rate  from 21.4% to 16.2%, and the use of IMFCDCN provides a further eduction in error ate to 16.0% for this task.  3.2. Results from the "Stress Test" Evaluation  In addition to the evaluation described above, a second  unofficial "stress-test" evaluation was conducted in Decem-  ber, 1992, which included spontaneous speech, utterances  containing out-of-vocabulary words, and speech from  unknown microphones and environments, all related to the  Wall Street Journal domain.
662	Explains Non-Technical Concepts	2 Related Work In MDS, information ordering is often realized  on the sentence level and treated as a coherence  enhancement task. A simple ordering criterion  is the chronological order of the events  represented in the sentences, which is often  augmented with other ordering criteria such as  lexical overlap (Conroy et al, 2006), lexical cohesion (Barzilay et al, 2002) or syntactic  features (Lapata 2003).
403	Explains Non-Technical Concepts	Abst ract We report on a method for utilising corpora col- lected in natural settings. It is based on distilling (re-writing) natural dialogues to elicit the type of dialogue that would occur if one the dialogue par- ticipants was a computer instead of a human. The method is a complement toother means uch as Wiz- ard of Oz-studies and un-distilled natural dialogues. We present he distilling method and guidelines for distillation. We also illustrate how the method af- fects a corpus of dialogues and discuss the pros and cons of three approaches in different phases of dia- logue systems development.
592	Explains Technical Concepts	4. Subl inear  Computat ion   Fast match methods require much less computation for each  word than a detailed match. But to reduce the computation  for speech recognition significantly for very large vocabu-  lary problems, we must change the computation from one  that is linear with the vocabulary to one that is essentially  independent of the vocabulary size.
607	Explains Non-Technical Concepts	Abst rac t   One cause of failure in natural anguage in-  terfaces is semantic overshoot; this is re-  flected in input sentences which do not cor-  respond to any semantic pattern in the sys-  tem. We describe a system which provides  helpful feedback in such cases by identifying  the "semantically closest" inputs which the  system would be able to understand.
629	Explains Non-Technical Concepts	The usual approach has been to work in three steps. First analyse real human dialogues, and based on these, in the second phase, design one or more Wizard of Oz-studies. The final step is to fine-tune the system's performance on real users. A good ex- ample of this method is presented in Eskenazi et al (1999). But there are also possible problems with this approach (though we are not claiming that this was the case in their particular project). Eskenazi et al. (1999) asked a human operator to act 'computer- like' in their Wizard of Oz-phase. The advantage is of course that the human operator will be able to perform all the tasks that is usually provided by this service. The disadvantage is that it puts a heavy burden on the human operator to act as a comput- er. Since we know that lay-persons' ideas of what computers can and cannot do are in many respects far removed from what is actually the case, we risk introducing some systematic distortion here. And since it is difficult to perform consistently in similar situations, we also risk introducing non-systematic distortion here, even in those cases when the 'wiz- ard' is an NLP-professional.
868	Explains Non-Technical Concepts	2 Related Work  There is well-known research work in the re- lated fields. ConceptNet (Liu and Singh, 2004)  is a toolkit to provide practical textual reasoning  for affect sensing for six basic emotions, text  summarization and topic extraction. Shaikh et  al. (2007) provided sentence-level textual affect  sensing to recognize evaluations (positive and  negative). They adopted a rule-based domain- independent approach, but they haven?t made  attempts to recognize different affective states  from open-ended text input.
720	Explains Technical Concepts	Specifically we have developed two different  algorithms in respect o the context consideration  they employ: Word-based and Pattern-based. The  former acquires word-based contextual data  (extended up to sentence boundaries), according to  the distributional similarity of which, word  similarity relations are extracted. The latter detects  common patterns throughout he corpus that  indicate possible word similarities. For example,  consider the sentence fragments:  "...while the S&I' index inched up 0.3%."  "The DAX #Mex inched up O. 70 point to close..."  Although their syntactic structures are different,  the common contextual pattern (appearing beyond  immediately adjacent words) indicates a possible  similarity between the tokens 'S&P' and 'DAX'.  Word pairs that persistently appear such context  similarity throughout the corpus (frequently  observed in technical texts) are confidently  indicated as semantically similar. Our method  captures such context similarity and extracts a  proportionate measure about semantic similarity  between lexical items.
315	Explains Technical Concepts	We used 217,562 sentences for training. When  these sel~t, ences were all extracted from a raw corlms ,  the agreement rate was 87.64% for "pair of modifiers"  and was 75.77% for "Colnplete agreement." When  the 217,562 training sentences were sentences fl'oln  the tagged cortms (17,562 sentences) used in our for-  real exl)eriment aInl froln a raw cortms, the agree-  " e S :~ ment rate for "pair of lno(hfi.r, was 87.66% and  for "Complete agreement" was 75.88%. These rates  were about 0.5% higher than those obtained when we  used only sentences from a tagged corlms. Thus, we  can acquire word order by adding inforlnation froln  a rmv corpus even if we do not have a large tagged  corpus. The results also indicate that the parser ac-  curacy is not so significant for word order acquisition  and that an accuracy of about 90% is sufficient.
44	Explains Technical Concepts	3.3 Cross-Linguistic Differences  In contrast o the similarities between German  and Chinese speech repairs lncntioned in the  sections above, differences can also be identified.  Some differences can bc noted through a  comparison of repair syntax in German and  Mandarin Chinese. It is more colnnlon for NPs  in German to be repaired directly within NPs,  whereas in Chinese NPs are often repaired  within a more complex syntactic context, i.e.  Chinese repairs arc composed of more than one  phrasal category. To investigate the syntactic  and morphological distribution of speech repairs  in both languages, the length of retracing in both  languages i examined.
480	Explains Technical Concepts	Just as information retrieval algorithms approx-  imate document relatedness by examining various  string matchings between the query and the text,  we approximate certain classes of coreference be-  tween the query and the text by examining lin-  guistic information. These coreference r lations in-  clude identity of reference and part-whole relations  for nominal and verbal phrases3 This moves us a  step closer to reasoning at a more appropriate l vel  of generalization, for summarization, which is still  technologically feasible. Below are examples indi-  cating the classes of relatedness that we are trying  to capture.
30	Explains Non-Technical Concepts	The algorithm for computing the score of a keyword  combine information from acoustic, language, and duration. One  key limitation of this approach is that keywords are only hypoth-  esized if they are included in the Viterbi baektrace. This does not  allow the system builder to operate ffectively at high false alarm  levels if desired. We are eousidering other algorithms for hypoth-  esizing "good score" keywords that are on high scoring paths.  We introduced an algorithm for smoothing language  model probabilities. This algorithm combines mall task-specific  language model training data with large task-independent lan-  guage training data, and provided a 14% reduction in test set per-  plexity.
750	Explains Non-Technical Concepts	Once the rules are broken into the more primitive allowed-type  and disallowed-type rules, there are several ways in which one  could try to match them against a string of surface characters  in tile recognition process. One way wonld be to wait until a  pair of characters was encountered that was the main pair for a  rule, and tficn look backwards to see if the left context of the  rule matches the current analysis path. If it does, put the right  context on hold to see whether it will ultimately be matched.  Another posslblility would be to continually keel) track of the  left contexts of rnles that are matching the characters at hand,  so that when tbe main character of a rule is encountered, the  program already knows that the left context has been matched.  The right context still needs to be pnt on hold and dealt with  the same way as in the other scheme.
501	Explains Technical Concepts	4.2 Discussion  The FSA M suggested above is suitable for the  syntactic haracteristics of speech repairs in both  German and Chinese. Repair syntax has been  taken into consideration from a procedural point  of view, instead of simply dcscribing the  sequential structures. In this modcl, probabilities  (for instance, word frequency or acoustic  features) on the arcs can be implemented to  operate a parsing system, which can deal with  speech repairs, ttowcver, speech data of  appropriate size are needed to obtain significant  probabilities.
379	Explains Technical Concepts	3.2 Syntactic Formalization  83.4% out of 147 specific NP repairs in German  start at phrase-initial positions and end at  phrase-final positions. In the Chinese data, only  thrcc NP-repairs among the 84 single NP-repairs  were not traced back to file phrase-initial  position. Phrasal boundaries play a role while  speech repairs are produced in both languages,  especially phrase-initial positions before the  rcparandum. The syntactic structure of the  maiority of German and Chinese repairs in NPs  can bc fonnally described by means of phrasal  modelling.
906	Explains Non-Technical Concepts	Our developments have been incorporated in- to an affect detection component, which can  detect affect and emotions from literal text input  and has been embedded in an intelligent conver- sational agent, engaged in a drama improvisa- tion with human users under loose scenarios  (school bullying and Crohn?s disease). The con- versational AI agent also provides appropriate  responses based on the detected affect from us- ers? input in order to stimulate the improvisa- tion. In both scenarios, the AI agent plays a mi- nor role in drama improvisation. E.g. it plays a  close friend of the bullied victim (the leading  role) in school bullying scenario, who tries to  stop the bullying.
860	Explains Non-Technical Concepts	Since the development of the exact algorithm, there have  been several approximations developed that are much faster,  with varying degrees of accuracy \[2, 3, 7, 8\]. The most  recent algorithm \[9\] empirically retains the accuracy of the  exact algorithm, while requiring little more computation than  that of a simple 1-best search.
533	Explains Non-Technical Concepts	It can be seen in Figure 4 that RASTA filtering provides  only a modest improvement in errors using secondary  microphones, and degrades peech from the CLSTLK  microphone. CMN, on the other hand, provides almost as  much improvement in recognition accuracy as MFCDCN,  without degrading speech from the CLSTLK microphone.  We do not yet know why our results using CMN are so  much better than the results obtained using RASTA. In con-  trast, Schwartz et al obtained approximately comparable  results using these two procedures \[13\].
167	Explains Technical Concepts	The network  with three levels of query Q, term T and document D  nodes are connected with bi-directional weighted edges  as shown in Fig.la for retrieval. Fig.lb shows the  network for performing learning where both the edge  weights and the architecture can adapt? Learning takes  place when some relevant documents are known for a  query. The basic model evaluates a retrieval status  value (RSV) for each query document pair (qa di) as a  combination of a document-focused QTD process that  spreads activation from query to document hrough  common terms k, and an analogous query-focused  DTQ process operating vice versa, as follows:  RSV = cx*Z k Wik *S(qak/La).-{- (1-(X)*ZkWak * S(dik/Li)  where 0_<ct_<l is a combination parameter for the two  processes, qak and d~k are the frequency of term k in a  query or document respectively, La, Li are the query or  document lengths, and S(.) is a sigmoid-like function  to suppress outlying values? A major difference of our  model from other probabilistic approaches i to treat a  document or query as non-monolithic, but constituted  of conceptual components (which we approximate as  terms). This leads us to formulate in a collection of  components rather than documents, and allows us to  account for the non-binary occurrence of terms in  items in a natural way. For example, in the usual  discriminatory weighting formula for query term k: wak  = log \[p*(1-q)/(1-p)/q\], p = Pr(term k present \[  relevant) is set to a query 'self-learn' value of qak /La  based on the assumption that a query is relevant o  itself, and q = Pr(term k present I -relevant) is set to  Fk/M, the collection term frequency of k, Fk, divided by  the total number of terms M used in the collection?  This we call the inverse collection term frequency  ICTF. It differs from the usual inverse document  frequency IDF in that the latter counts only the  Fig.lb Query-Focused Learning & Expansion  presence and absence of terms in a document, ignoring  the within-document term frequency. Moreover, as the  system learns from relevant documents, p can be  trained to a value intermediate between the basic self-  learn value and that given by the known relevants  according to a learning procedure \[1\]. Our system  also uses two-word adjacency phrases as terms to  improve on the basic single word representation.  Documents of many thousands or more words long can  have adverse ffect on retrieval. PIRCS deals with the  problem by simply segmenting long documents into  approximately equal sub-documents of 550-word size  and ending on a paragraph boundary. For the final  retrieval ist, retrieval status values (RSV) of the top  three sub-documents of the same document are  combined with decreasing weights to return a final  RSV. This in effect favors retrieval of longer  documents that contain positive evidence in different  sub-parts of it. PIRCS has participated in all previous  TREC 1-6 blind retrieval experiments and consistently  returned some of the best results, see for example \[2\].
797	Explains Technical Concepts	We have also analyzed affect detection per- formance based on previously collected (other)  transcripts from user testing by calculating  agreements via Cohen?s Kappa between two  human judges and between human judges and  the AI agent with and without the new devel- opment respectively in order to verify the effi- ciency of the metaphorical and contextual affect  sensing.
110	Explains Technical Concepts	The MINDS summarization system is com-  posed of four stages. First we have an Input Pro-  cess stage, whose main function is to get the  relevant text in the document in UNICODE encod-  ing. The second stage is a Document Structuring  Stage, where paragraph and sentence recognition,  and word tokenization are performed. All the infor-  mation about the document structure is stored in a  "Document Object" that will be used in the Sum-  marization-Translation stage. In the Summariza-  tion-Translation Stage, the text is summarized  using sentence xtraction techniques, where the  sentence scoring and ranking is mainly based on  text-structure based heuristics supplemented by  word frequency analysis methods and in some  cases by information from a Name Recognition  module. Once the summary is ready in the original  language, MINDS uses MT engines from other  ongoing CRL projects to translate the summary to  English. The final stage is the Output Process that  generates the summary output form; SGML,  HTML, or Plain text. This may also involve con-  version from UNICODE to the original encoding  of the document.
520	Explains Non-Technical Concepts	7. SUMMARY  Our contributions in SPHINX-II include improved feature  representations, multiple-codebook semi-continuous hidden  Markov models, between-word senones, multi-pass earch  algorithms, and unified acoustic and language modeling. The  key to our success is our data-driven unified optimization ap-  proach. This paper characterized our contributionsby percent  error rate reduction on the 5000-word WSJ task, for which we  reduced the word error rate from 20% to 5% in the past year  \[2\].
679	Explains Technical Concepts	We may now describe how the rules should be read. The first  rule should be read roughly as, "a morpheme boundary \[+\] at the  lexical level corresponds to an \[el at the surface level whenever  it is between an \[x\] and an \[s\], or between a \[z\] and an \[s\], or  between a lcxical \[y\] corresponding to a surface \[i\] and an \[s\], or  between an \[ s h\] and an \[s\] or between a\[e h\] and an \[s\]." This  means, for instance, that the string of lexical characters \[c h u r  e h + s\] corresponds to the string of surface characters \[c h u r c  h e s\] (forgetting for the moment about the possibility that other  rules might also obtain). The second rule is identical to the first  except for an added \[o\] in tile left context.
449	Explains Technical Concepts	2.2 Translation memory is the key The main goal of this paper is to suggest how to overcome these obstacles by means of a combination of an MT system with commercial MAHT (Machine-aided human translation) systems. We have chosen the TRADOS Translator's Workbench as a representative system of a class of these products, which can be characterized as an example-based translation tools. IBM's Translation Manager and other products also belong to this class. Such systems uses so-called translation memory, which contains pairs of previously translated sentences from a source to a target language. When a human translator starts translating a new sentence, the system tries to match the source with sentences already stored in the translation memory. If it is successful, it suggests the translation and the human translator decides whether to use it, to modify it or to reject it.
88	Assumes Prior Knowledge	2 Baseline System The baseline systems for the translation directions German-English and English-German are both de- veloped using Discriminative Word Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al, 2007) for extracting phrase pairs and generating the phrase table from the discrimi- native word alignments. The difficult reordering between German and English was modeled us- ing POS-based reordering rules. These rules were learned using a word-aligned parallel corpus. The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all lan- guages.
383	Explains Non-Technical Concepts	6. SUMMARY  We have reported on several methods that result in some reduction  in word error rate on the 5K-word WSJ test. In addition, we have  described several experiments that answer questions related to  training scenarios, recognition search strategies, and microphone  independence. In particular, we verified that there is no reason to  collect speech from a large number of speakers for estimating a  speaker-independent model Rather, the same results can be ob-  tained with less effort by collecting the same amount of speech  from a smaller number of speakers. We determined that the N-  best rescoring paradigm can degrade somewhat when the error  rate is very high and the sentences are very long. We showed that  a simple blind deconvolution preprocessing of the cepstral fea-  tures results in a better microphone independence method than the  more complicated RASTA method. And finally, we introduced  a new microphone adaptation algorithm that achieves improved  accuracy by adapting to one of several codebook transformations  derived from several known microphones.
755	Assumes Prior Knowledge	At the most 34 utterances where removed from one single dialogue and that was from a dialogue with discussions on where to find a parking lot, i.e. discussions outside the capabilities of the applica- tion. There was one more dialogue where more than 30 utterances were removed and that dialogue is a typical example of dialogues where distillation actu- ally is very useful and also indicates what is normal- ly removed from the dialogues. This particular dia- logue begins with the user asking for the telephone number to 'the Lost property office' for a specific bus operator. However, the operator starts a discussion on what bus the traveller traveled on before provid- ing the requested telephone number. The reason for this discussion is probably that the operator knows that different bus companies are utilised and would like to make sure that the user really understands his/her request. The interaction that follows can, thus, in that respect be relevant, but for our pur- pose of developing systems based on an overall goal of providing information, not to understand human interaction, our dialogue system will not able to han- dle such phenomenon (JSnsson, 1996).
690	Explains Technical Concepts	4.2 Evaluation Metrics  Besides precision and recall, we adopt the pre- cision@N metric for experimental evaluation  (Liu, 2006). It gives the percentage of correct  features that are among the top N feature candi- dates in a ranked list. We compare our method?s  results with those of double propagation which  ranks extracted candidates only by occurrence  frequency.
794	Assumes Prior Knowledge	Abstract  Temporal expressions in texts contain  significant temporal information. Under- standing temporal information is very  useful in many NLP applications, such as  information extraction, documents sum- marization and question answering.  Therefore, the temporal expression nor- malization which is used for transform- ing temporal expressions to temporal in- formation has absorbed many research- ers? attentions. But previous works,  whatever the hand-crafted rules-based or  the machine-learnt rules-based, all can  not address the actual problem about  temporal reference in real texts effective- ly. More specifically, the reference time  choosing mechanism employed by these  works is not adaptable to the universal  implicit times in normalization. Aiming  at this issue, we introduce a new refer- ence time choosing mechanism for tem- poral expression normalization, called  reference time dynamic-choosing, which  assigns the appropriate reference times to  different classes of implicit temporal ex- pressions dynamically when normalizing.  And then, the solution to temporal ex- pression defuzzification by scenario de- pendences among temporal expressions  is discussed. Finally, we evaluate the  system on a substantial corpus collected  by Chinese news articles and obtained  more promising results than compared  methods.
719	Explains Technical Concepts	In sum, it appears that for our selection of  retrieval systems, there is a potential for improving  retrieval through query-specific fusion.  One way to exploit this opportunity is to use  initially-retrieved documents to adjust the weights of  the single overall static fusion function, as in \[3\].  Although we tried several ways of updating fusion  function coefficients with relevance feedback, we  were unable to exploit any of the apparent potential  to improve retrieval performance in this way.  distribution of the retrieval systems' retrieval scores  for the query enumerated above. We are currently  working on building such a dynamic fusion function.
561	Explains Technical Concepts	P i tch  movement .  While in ToBI, the prim-  itives of description of pitch movement are dis-  tinct highs (It) and lows (L), where a particular  pitch movement is described by a sequence of  highs and/or lows in the pitch, in SFC the prim-  itive of description is the tune, i.e., a relative  concept, such as a rising, falling or level tune.  Nuc lear  stress.  While in ToBI, the mmlear  stress is marked by the last starred tone in the  sequence of tones and is thus only implicitly in-  dicated in the annotation, SFG marks nuclear  stress explicitly by marking up the Tonic)  While there is a basic match in terms of ac-  counting for the pitch movement and we cast  thus expect to be able to recast ToBI tone se-  quences as SFC tones, we may encounter some  problems due to the non-acknowledgement of  tile unit of foot in ToBI on the one hand, and  due to ToBI marking up pitch accents other  ICE Sec. 2.1, however: the nuclear stress in Tom is  by definition the last starred tone.
375	Explains Technical Concepts	In this paper we describe and compare the performance ofa  series of cepstrum-based procedures that enable the CMU  SPHINX-II speech recognition system to maintain a high  level of recognition accuracy over a wide variety of acousti-  cal environments. The most recently-developed algorithm  is multiple fixed codeword-dependent c pstral normaliza-  tion (MFCDCN). MFCDCN is an extension of a similar  algorithm, FCDCN, which provides an additive nviron-  mental compensation to cepstral vectors, but in an environ-  ment-speci f ic  fashion \[5\]. MFCDCN is less  computationally complex than the earlier CDCN algorithm,  and more accurate than the related SDCN and BSDCN  algorithms \[6\], and it does not require domain-specific  paining to new acoustical environments. In this paper we  describe the performance of MFCDCN and related algo-  rithms, and we compare it to the popular RASTA approach  to robustness.
177	Mathematically-Oriented Paragraph	2.2 Compar i son  w i th  Koskenn iemi ' s  Rules   Koskenniemi \[1983, 1984\] describes three types of rules, as exem-  plified below:  R4) a > b :=:*- c/d c / f -  g/h i/j  RS) a > b ~= old e/f- g/h i/j  R6) a > b ~ e/d e l l -  g/h i/j.  Rule R4 says that if a lexical \[a\] eorresponds to a surface \[b\],  then it must be within tile context given, i.e., it must be preceded  by \[c/d eft\] and followed by \[g/h i/j|. This corresponds exactly  to tile rule given below:  RV) a/b allowed in context old e/ f_  g/h i/j.  The rule introduced as R5 and repeated below says that if a  lexieal \[a\] occurs following \[c/d e/f|  and preceding \[g/h i/j|, then  it must correspond to a surface \[b\]:  RS) a > b e-= e/d e/ f_  g/h i/j.  'rhe corresponding rule in the formalism being proposed here  would look approximately ike this:  R10) a/sS disallowed in context e/d c / f -  g/h i/j,  where sS is some set of characters to which  \[a\]  can correspond that does not include \[b\].  A comparison of each system's third type of rule involves com-  post|on of rules and is the subject of the next section.
680	Explains Non-Technical Concepts	In Karttunen and Wittenlmrg \[1983\] there is a single rule listed  to describe the data. However, the rule makes use of a diacritic  (') for showing stress, and words in the lexicon must contain this  diacritic in order for the rule to work. The same thing could  be done in the system being described here, but it was deemed  undesirable to allow words in the lexicon to contain diacritics en-  coding information such as stress. Instead, the following rules are  used. Ultimately, the goal is to have some sort of general mech-  anism, perhaps negative rule features, for dealing with this sort  of thing, but for now no such mechanism has been implemented.
424	Assumes Prior Knowledge	We first explain why the stop-list of the target lan- guage has to be applied. On the left side of Fig. 5, if the Chinese word C exists in the same alignments with the English word E more than any other Chi- nese words, C will be the most probable translation for E. Because of their frequent appearance, some Chinese stopwords may have more chances to be in the same alignments with E. The probability of the translation E --+ C is then reduced (maybe ven less than those of the incorrect ones). This is the reason why many English words are translated to "~ '  (of) by the translation model trained without using the Chinese stop-list.
413	Explains Technical Concepts	Instead of taking into account such restrictions,  we considered the fi'equencies of occurrence of  the most fi'equent words of the entire written  language. It has been shown that they are more  reliable stylistic discriminators as regards the  combination of classification accuracy and the  number of the required common words that have  to be taken into account. Note that when dealing  with multivariate models, the reduction of the  required parameters i a vein crucial factor for  attaining reliable results and mininfizing the  computational cost.
816	Assumes Prior Knowledge	4.2.1 Sunshine WebTran Server Using the Sunshine WebTran server (Anonymous, 1999b), an online Engiish-Chinese MT system, to translate the 54 English queries, we obtained an average precision of 0.2001, which is 50.3% of the mono-lingual precision. The precision is higher than that obtained using the translation model (0.1804) or the dictionary (0.1427) alone, but lower than the precison obtained using them together (0.2232).
761	Explains Technical Concepts	3.1 Double Propagation  As we described above, double propagation is  based on the observation that there are natural  relations between opinion words and features  due to the fact that opinion words are often used  to modify features. Furthermore, it is observed  that opinion words and features themselves have  relations in opinionated expressions too (Qiu et  al., 2009). These relations can be identified via  a dependency parser (Lin, 1998) based on the  dependency grammar. The identification of the  relations is the key to feature extraction.  Dependency grammar: It describes the de- pendency relations between words in a sentence.  After parsed by a dependency parser, words in a  sentence are linked to each other by a certain  relation. For a sentence, ?The camera has a  good lens?, ?good? is the opinion word and  ?lens? is the feature of camera. After parsing,  we can find that ?good? depends on ?lens? with  relation mod. Here mod means that ?good? is  the adjunct modifier for ?lens?. In some cases,  an opinion word and a feature are not directly  dependent, but they directly depend on a same  word. For example, from the sentence ?The lens  is nice?, we can find that both feature ?lens? and  opinion word ?nice? depend on the verb ?is? with the relation s and pred respectively. Here s means that ?lens? is the surface subject of ?is? while pred means that ?nice? is the predicate of  the ?is? clause.         3.2 Part-whole relation  As we discussed above, a part-whole relation is  a good indicator for features if the class concept  word (the ?whole? part) is known. For example,  the compound nominal ?car hood? contains the  part-whole relation. If we know ?car? is the  class concept word, then we can infer that  ?hood? is a feature for car. Part-whole patterns  occur frequently in text and are expressed by a  variety of lexico-syntactic structures (Girju et  al, 2006; Popescu and Etzioni, 2005). There are  two types of lexico-syntactic structures convey- ing part-whole relations: unambiguous structure  and ambiguous structure. The unambiguous  structure clearly indicates a part-whole relation.  For example, for sentences ?the camera consists  of lens, body and power cord.? and ?the bed  was made of wood?. In these cases, the detec- tion of the patterns leads to the discovery of real  part-whole relations. We can easily find features  of the camera and the bed. Unfortunately, this  kind of patterns is not very frequent in a corpus.  However, there are many ambiguous expres- sions that are explicit but convey part-whole  relations only in some contexts. For example,  for two phrases ?valley on the mattress? and  ?toy on the mattress?, ?valley? is a part of ?mat- tress? whereas ?toy? is not a part of ?mattress?. Our idea is to use both the unambiguous and  ambiguous patterns. Although ambiguous pat- terns may bring some noise, we can rank them  low in the ranking procedure. The following  two kinds of patterns are what we have utilized  for feature extraction.
248	Assumes Prior Knowledge	   From the formulas, we can see that the author- ity score estimates the importance of the content  of the page, and the hub score estimates the val- ues of its links to other pages. An authority  score is computed as the sum of the scaled hub  scores that point to that page. A hub score is the  sum of the scaled authority scores of the pages  it points to. The key idea of HITS is that a good  hub points to many good authorities and a good  authority is pointed by many good hubs. Thus,  authorities and hubs have a mutual reinforce- ment relationship.
491	Explains Non-Technical Concepts	2.2 Preprocessing and postprocessing  We used our own English and French pre- and  post-processing tools, rather than those available  from the WMT web site. For training, all English  and French text is tokenized with a language- specific tokenizer and then mapped to lowercase.  Truecasing uses an HMM approach, with lexical  probabilities derived from ?mono? and transition  probabilities from a 3-gram LM trained on tru- ecase ?mono?. A subsequent rule-based pass ca- pitalizes sentence-initial words. A final detokeni- zation step undoes the tokenization.
35	Explains Technical Concepts	4.3. N-Best Paradigm  In 1989 we developed the N-best Paradigm method for combining  knowledge sources mainly as a way to integrate speech recogni-  tion with natural language processing. Since then, we have found  it to be useful for applying other expensive speech knowledge  sources as well, such as cross-word models, tied-mixture densi-  ties, and trigram language models. The basic idea is that we first  find the top N sentence hypotheses using a less expensive model,  such as a bigram grarnmar with discrete densities, and within-  word context models. And then we rescore ach of the resulting  hypotheses with the more complex models, and finally we pick  the highest scoring sentence as the answer.
873	Explains Technical Concepts	As testing ground in this study we used a part of  the WSJ corpus classified into four low-level  genres that can be grouped into two higher-level  genres. The automated classification model  based on discriminant analysis applied to the  frequencies of occurrence of the most frequent  words of the BNC, that represent he most  frequent words of the entire written English  language, managed to capture the stylistic  homogeneity in both levels.
603	Explains Technical Concepts	8 History of the Framework and Comparison  with Other Systems  The framework represents a generalization of  several predecessor NLG systems based on  Meaning-Text Theory: FoG (Kittredge and  Polgu~re, 1991), LFS (Iordanskaja et al, 1992),  and JOYCE (Rambow and Korelsky, 1992).  The framework was originally developed for the  realization of deep-syntactic structures in NLG  (Lavoie and Rambow, 1997). It was later  extended for generation of deep-syntactic  structures from conceptual interlingua (Kittredge  and Lavoie, 1998). Finally, it was applied to  MT for transfer between deep-syntactic  structures of different languages (Palmer et al,  1998). The current framework encompasses the  full spectrum of such transformations, i.e. from  the processing of conceptual structures to the  processing of deep-syntactic structures, either  for NLG or MT.
402	Explains Non-Technical Concepts	This paper presents results of a comparative  stud), of speech repairs with the goal of  examining and modelling repair syntax by  looking into empirical cross-linguistic spccch  data. In this paper, the phenomena of speech  repairs are introduced first, followed by an  empirical cross-linguistic analysis of speech  repairs in German and Mandarin Chinese, which  have different language typologies. Speech data,  therefore, were collected to look for linguistic  sequences and particularities of spontaneous  speech, which usually cause difficulties for  language dialogue systems. Syntactic patterns  found in the comparative analysis have  subsequently been formalised to make clear the  internal structures of speech repairs. Formal  modelling in FSA should finally show the  fonnal characteristics of repair sequences in  these two language systems.
517	Assumes Prior Knowledge	We also performed a pilot experiment with five  users who were undergraduates from various  disciplines. The purpose of the study was to find out  if they could tell what was the difference between the  standard ranked document order retrieved by  SMART and a MMR reranked order with X = ..5.  They were asked to perform nine different search  tasks to find information and asked various questions  about the tasks. They used two methods to retrieve  documents, known only as R and S. Parallel tasks  were constructed so that one set of users would  perform method R on one task and method S on a  similar task. Users were not told how the documents  were presented only that either "method R" or  "method S" were used and that they needed to be try  to distinguish the differences between methods. After  each task we asked them to record the information  found. We also asked them to look at the ranking for  method R and method S and see if they could tell any  difference between the two. The majority of people  said they preferred the method which gave in their  opinion the most broad and interesting topics. In the  final section they were asked to select a search  method and use it for a search task. 80% (4 out of 5)  chose the method MMR to use. The person who  chose Smart stated it was because "it tends to group  more like stories together." The users indicated a  differential preference for MMR in navigation and for  locating the relevant candidate documents more  quickly, and pure-relevance ranking when looking at  related ocuments within that band. Three of the five  users clearly discovered the differential utility of  diversity search and relevance-only search. One user  explicitly stated his strategy:  "Method R \[relevance only\] groups items  together based on similarity and Method S  \[MMR re-ranking\] gives a wider array. I would  use Method S \[MMR re-ranking\] to find a topic  ... and then use Method R \[relevance-only\] with  a specific search from Method S \[MMR re-  rankingl to yield a lot of closely related items."    5. SUMMARIZ ING  DOCUMENTS  LONGER  The MMR-passage selection ':'method for  summarization works better for longer documents  (which typically contain more inherent passage  redundancy across document sections such as  abstract, introduction, conclusion, results, etc.). To  demonstrate the quality of summaries that can be  obtained for long documents, we summarized an  entire dissertation containing 3,772 sentences with a  generic topic query constructed by expanding the  thesis title (Figure 1). In contrast, Figure 2 shows the  results of a more specialized query with a larger L  value to focus summarization less on diversity and  more on topic.  The above example demonstrates the utility of  query relevance in summarization a d the incremental  utility of controlling summary focus via the lambda  parameter. It also highlights a shortcoming of  summarization by extraction, namely coping with  antecedent references. Sentence \[2621\] refers to  coefficients "s", "c", and "t," which do not make  sense outside the framework that defines them. Such  referential problems are ameliorated with increased  passage length, for instance using paragraphs rather  than sentences. However, longer-passage s lection  also implies longer summaries. Another solution  co-reference r solution \[25\].
866	Explains Technical Concepts	Finally, we may note that NLP-based indexing has  also a positive effect on overall performance, but the  improvements are relatively modest, particularly on  the expanded queries. A similar effect of reduced ef-  fectiveness of linguistic indexing has been reported  also in connection with improved term weighting  techniques.
511	Assumes Prior Knowledge	The content of a syntax field is often at least par-  tlally predictable. This fact allows us to employ as an  aid to users wishing to write their own dictionary rules  which add information to the lexicon during the compi-  lation process. Recall that, in our analysis of English,  the lnflectablllty of a word is governed by the value in  that word's category for INFL. Completion Rules (CRs)  can be written that will add the specification ( INFL- )   to any entry already Including (PLU +) (for e.g. men),  (AFORM ER) (for e.g. worse), (VFORM ING), etc,, thus  removing the need to state Individually that a given  word cannot be inflected.
903	Explains Technical Concepts	3.3. Weight Optimi~ation  After making several changes to the system, we reoptimized the  relative weights for the acoustic and language models, as weU as  the word and phoneme insertion penalties. These weights were  optimized on the development test set automaticaUy using the  N-best lists \[4\]. Optimization of these weights reduced the word  error by 0.4%.
601	Explains Non-Technical Concepts	7 Conc lus ion   Porting an existing system to a new domain presents  an important problem in IE. Effective techniques are  needed to minimize the time and complexity of the  process, and to extricate the porting process from  low-level system details, so that it can be undertaken  by non-expert users. In this report, we have described  our approach to the problem, based on:  ? example-based acquisition of scenario-specific  patterns,  ? system-aided generalization of acquired pat-  terns, at the semantic and syntactic level.  The experience we have gained from implementing  this strategy leads us to believe in its overall useful-  ness.
290	Assumes Prior Knowledge	A priori, it would seem that both the English and Chinese stop-lists hould be applied to eliminate the noise caused by them. Interestingly, from our ob- servation and analysis we concluded that for better precision, only the stop-list of the target language should be applied in the model training.
310	Explains Technical Concepts	3.1. Silence Detection  Even though the training speech is read from prompts, there are  often short pauses either due to natural sentential phrasing, read-  ing disfiuency, or nmning out of breath on long sentences. Nat-  urally, the orthographic transcription that is provided with each  utterance does not indicate these pauses. But it would be incor-  rect to model the speech as ff there were no pauses. In particular,  phonetic models that take into account acoustic oarticulation be-  tween words (cross-word models) do not function properly ff they  are confounded by unmarked pauses between words.
194	Explains Non-Technical Concepts	Unlike document information retrieval, text  summarization evaluation has not extensively  addressed the performance of different methodologies  by evaluating the effects of different components.  Most summarization systems use linguistic  knowledge as well as a statistical component \[3, 5,  16, 23\]. We applied the monolingual information  retrieval method of query expansion \[20, 27, 28\] to  summarization, using parts of the document to expand  our queries. We also performed compression  experiments. We used a modified version of the 11-  pt average recall/precision (Section 9.2) to evaluate  our results.
686	Explains Technical Concepts	In this paper we discuss the rationale for the use of reactive planning as well as the use of the hierarchical task network (HTN) style of plan operators. Then we describe APE (the Atlas Planning Engine), a dialogue planner we have implemented to embody the above concepts. We demonstrate he use of APE by showing how we have used it to add a dialogue capability to an existing ITS, the Andes physics tutor. By showing dialogues that Atlas-Andes can generate, we demonstrate the advantages of this architecture over the finite-state machine approach to dialogue management.
849	Explains Technical Concepts	At this point if the lexical resources are avail-  able, an optional sentence length reduction can be  carried out using information from a tagging stage.  This sentence length reduction includes the elimi-  nation of adjectives from noun phrases, keeping  only the head noun in a noun phrase, eliminating  adverbs from verb phrases and eliminating most of  the prepositional phrases. However, if a word  selected for elimination is a key word, proper noun,  the name of a place, a date or a number, the word is  kept in the sentences. If this word happens to be in  a prepositional phrase, then the prepositional  phrase is kept in the sentence.  Once the scoring process is done, the sentences  are ranked and a summary is generated using the  sentences with the higher scores that together do  not exceed a predetermined percentage of the doc-  ument's length. This summary is written in the  document's original language, so a machine trans-  lation system is used to produce an English version  of the summary.
519	Assumes Prior Knowledge	* System utterances are made more computer-l ike and do not include irrelevant information. The latter is seen in $9 in the dialogue in figure 3 where the provided information is not relevant. It could also be possible to remove $5 and re- spond with $7 at once. This, however, depends on if the information grounded in $5-U6 is need- ed for the 'system' in order to know the arrival time or if that could be concluded from U4. This in turn depends on the system's capabili- ties. If we assume that the dialogue system has a model of user tasks, the information in $5-U6 could have been concluded from that. We will, in this case, retain $5-U6 as we do not assume a user task model (Dahlb/ick and JSnsson, 1999) and in order to stay as close to the original di- alogue as possible.
953	Explains Non-Technical Concepts	5 Implementation of At las-Andes 5.1 Architecture of Atlas-Andes The first system we have implemented with APE is a prototype Atlas-Andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues. Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar. Robust natural language understanding in Atlas-Andes is provided by Ros6's CARMEL system (Ros6 2000); it uses the spelling correction algorithm devised by Elmi and Evens (1998).
512	Explains Non-Technical Concepts	After developing this algorithm, we found that a similar algo-  rithm had been developed at CMU \[12\]. There were four differ-  ences between the MFCDCN method and our method. First, we  grouped the several different microphones into six microphone  types rather than modeling them each separately. Second, we  modified the covariances as well as the means of each Gaussian,  in order to reflect he increased uncertainty in the codebook trans-  formation. Third, we used an independent microphone classifier,  rather than depend on the transformed codebook itself to perform  microphone selection. And fourth, the CMU algorithm used an  SNR-dependent transformation, whereas we used only a single  transformation. The first difference is probably not important.  We believe that the second and third differences favor our al-  gorithm, and the fourth difference clearly favors the MFCDCN  algorithm. Further experimentation will be needed to determine  the best combination of algorithm features.
728	Explains Technical Concepts	SUMMARY  In this paper we have reported on recent work on the iden-  tification of non-linguistic speech features from recorded sig-  nals using phone-based acoustic likelihoods. The inclusion  of this technique in speech-based systems, can broaden the  scope of applications of speech technologies, and lead to  more user-friendly systems.
971	Explains Non-Technical Concepts	1. INTRODUCTION  Ordinary Japanese sentences are written using a  combination of Kana, which are Japanese phonogramic  characters, and Kanji, which are ideographic Chinese  characters. Nouns, verbs and other independent words  are generally written in Kanji. On the other hand,  dependent words such as postpositons, and auxiliary  verbs, etc., are written in Kana. While there are about  fifty Kana, there are several thousand Kanji, thus making  it difficult to input Japanese sentences into a computer  system.
917	Explains Non-Technical Concepts	The use of a large-vocabulary continuous-speech recog-  nition system allows the system designer agreat dealof lexibil-  ity in choosing the keywords that hey would like to select for the  particular application. If the desired keyword is already in the  lexicon, then searching for the keyword can be achieved by look-  ing for the word in the transcription generated by the recognizer.  If the word is not in the lexicon, the word can be easily added to  the system since triphone models have already been trained.  The ability to transerihe spontaneous speech and search  for relevant keywords will play an important role in the future  development of simple spoken language applications. Such sys-  tems will be easily portable to new domains. Since the operating  point for our speech recognizer is typically one which has a low  insertion rate, there is little chance for a keyword false alarm.  Future experimentation will determine the effectiveness of such  understanding systems for human-computer interaction.
340	Explains Non-Technical Concepts	Discussion  We view the results of the first evaluation as  promising in that they compare favorably with  inter-assessor consistency using the entire docu-  ment. \[15\] reports unanimous relevance judgments  by three assessors for 71.7% of the documents. In-  terpolating this figure to two assessors yields an  80.1% agreement figure. Using summaries which  on average are only 17.2% of the original docu-  ment, our assessors matched the TREC assessors  for 75.0% of the documents.  The second evaluation yielded a much lower re-  call figure while precision remained comparable.  This, however, is also the case when the same asses-  sors judgments on the full documents are compared  to those of the TREC assessors. These results are  as follows:  Precision 83.5% 167/(167+33)  Recall 63.5% 167/(167+96)  Compression 100.0%  Accuracy 69.3% (167+124)/420
760	Explains Non-Technical Concepts	Abstract In this paper, we investigate the problem of en- tity identification and relation extraction from en- cyclopedia articles, and we propose a joint discrim- inative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simulta- neously. This modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual ben- efits, as well as a great flexibility to incorporate a large collection of arbitrary, overlapping and non- independent features. We show the parameter es- timation algorithm of this model. Moreover, we propose a new inference method, namely collec- tive iterative classification (CIC), to find the most likely assignments for both entities and relations. We evaluate our model on real-world data from Wikipedia for this task, and compare with current state-of-the-art pipeline and joint models, demon- strating the effectiveness and feasibility of our ap- proach.
123	Explains Non-Technical Concepts	1. INTRODUCTION  Many advanced speech recognition techniques cannot be  developed or used in practical speech recognition systems  because of their extreme computational requirements. Simpler  speech recognition techniques can be used to recognize speech in  reasonable time, but they compromise word recognition  accuracy. In this paper we aim to improve the speed/accuracy  trade-off in speeeh recognition systems using progressive s arch  techniques.
303	Assumes Prior Knowledge	2 Portage system description  2.1 Core engine and training data  The NRC system uses a standard two-pass  phrase-based approach. Major features in the  first-pass loglinear model include phrase tables  derived from symmetrized IBM2 alignments and  symmetrized HMM alignments, a distance-based  distortion model, a lexicalized distortion model,  and language models (LMs) that can be either  static or else dynamic mixtures. Each phrase ta- ble used was a merged one, created by separately  training an IBM2-based and an HMM-based  joint count table on the same data and then add- ing the counts. Each includes relative frequency  estimates and lexical estimates (based on Zens  and Ney, 2004) of forward and backward condi- tional probabilities. The lexicalized distortion  probabilities are also obtained by adding IBM2  and HMM counts. They involve 6 features (mo- notone, swap and discontinuous features for fol- lowing and preceding phrase) and are condi- tioned on phrase pairs in a model similar to that  of Moses (Koehn et al, 2005); a MAP-based  backoff smoothing scheme is used to combat  data sparseness when estimating these probabili- ties. Dynamic mixture LMs are linear mixtures  of ngram models trained on parallel sub-corpora  with weights set to minimize perplexity of the  current source text as described in (Foster and  Kuhn, 2007); henceforth, we?ll call them ?dy- namic LMs?.
880	Assumes Prior Knowledge	As mentioned in (Polgu~re, 1991), the high level  of abstraction of the ConcSs makes them a  suitable interlingua for multilingual NLG since  they bridge the semantic discrepancies between  languages, and they can be produced easily from  the domain data. However, most off-the-shelf  parsers available for MT produce only syntactic  structures, thus the DSyntS level is often more  suitable for transfer.
741	Explains Technical Concepts	8 Conclusion and Future Work In this paper, we investigate the compound IE task of identifying entities and extracting relations be- tween entities in encyclopedia text. And we pro- pose a unified framework based on undirected, conditionally-trained probabilistic graphical mod- els to perform all relevant subtasks jointly. More importantly, we propose a new algorithm: CIC, to enable approximate inference to find the MAP assignments for both segmentations and relations. As we shown, our modeling offers several advan- tages over previous models and provides a natural formalism for this compound task. Experimental study exhibits that our model significantly outper- forms state-of-the-art models while also running much faster than the joint models. In addition, the superiority of the CIC algorithm is also discussed and compared. We plan to improve the scalability of our approach and apply it to other real-world problems in the future.
644	Assumes Prior Knowledge	Abstract We describe the progress we have made in the past year on Joshua (Li et al, 2009a), an open source toolkit for parsing based machine translation. The new functional- ity includes: support for translation gram- mars with a rich set of syntactic nonter- minals, the ability for external modules to posit constraints on how spans in the in- put sentence should be translated, lattice parsing for dealing with input uncertainty, a semiring framework that provides a uni- fied way of doing various dynamic pro- gramming calculations, variational decod- ing for approximating the intractable MAP decoding, hypergraph-based discrimina- tive training for better feature engineering, a parallelized MERT module, document- level and tail-based MERT, visualization of the derivation trees, and a cleaner pipeline for MT experiments.
190	Explains Technical Concepts	Second, as in last year?s evaluation, we auto- matically extracted and aligned parallel sentences from comparable in-domain corpora. This year we used the AFP and APW news texts since there are available in the French and English LDC Gi- gaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into En- glish using our first stage SMT system. These En- glish sentences were then used to search for trans- lations in the English AFP and APW texts of the Gigaword corpus using information retrieval tech- niques. The Lemur toolkit (Ogilvie and Callan, 2001) was used for this purpose. Search was lim- ited to a window of ?5 days of the date of the French news text. The retrieved candidate sen- tences were then filtered using the Translation Er- ror Rate (TER) with respect to the automatic trans- lations. In this study, sentences with a TER be- low 65% for the French?English system and 75% for the English?French system were kept. Sen- tences with a large length difference (French ver- sus English) or containing a large fraction of num- bers were also discarded. By these means, about 15M words of additional bitexts were obtained to include in the French?English system, and 21M words to include in the English?French system. Note that these additional bitexts do not depend on the translation direction. The most suitable amount of additional data was just different in the French?English and English?French transla- tion directions.
102	Explains Non-Technical Concepts	Although the weight estimation test set is strictly speak-  ing part of the training data, we find that for most ex-  periments, the bias in this type of testing is small enough  to allow us to make comparisons between systems when  both are run on the weight-training set. Accordingly  some of the experiments reported below are only run on  the weight training test set. Of course, final evaluation  of a system must be on an independent test set.
657	Assumes Prior Knowledge	Our experiments on these two corpora produced the results hown in Tab. 1. The precision of mono- lingual IR is given as benchmark. In both E-C and C-E CLIR, the translation model achieved around 40% of monolingual precision. To compare with the dictionary-based approach, we employed a Chinese- English dictionary, CEDICT (Denisowski, 1999), and an English-Chinese online dictionary (Anony- mous, 1999a) to translate queries. For each word of the source query, all the possible translations given by the dictionary are included in the translated query. The Chinese-English dictionary has about the same performace as the translation model, while the English-Chinese dictionary has lower precision than that of the translation model.
569	Mathematically-Oriented Paragraph	The second factor was the module of syntactic analysis of Czech. There were several reasons of parsing failures. Apart from the common inability of most rule-based formal grammars to cover a particular natural anguage to the finest detail of its syntax there were other problems. One of  them was the existence of non-projective constructions, which are quite common in Czech even in relatively short sentences. Even though they account only for 1.7?/'o f syntactic dependencies, every third Czech sentence contains at least one, and in a news corpus, we discovered as much as 15 non-projective dependencies; see also Haji6 et al. (1998). An example of a non-projective construction is "Soubor se nepodafilo otev~it." \[lit.: File Refl. was_not._possible to_open. - It was not possible to open the file\]. The formalism used for the implementation (Q-systems) was not meant to handle non-projective constructions. Another source of trouble was the use of so-called semantic features. These features were based on lexical semantics of individual words. Their main task was to support a semantically plausible analysis and to block the implausible ones. It turned out that the question of implausible combinations of  semantic features is also more complex than it was supposed to be. The practical outcome of the use of semantic features was a higher atio of parsing failures - semantic features often blocked a plausible analysis. For example, human lexicographers a signed the verb 'to run' a semantic feature stating that only a noun with semantic features of a human or other living being may be assigned the role of subject of this verb. The input text was however full of sentences with 'programs' or 'systems' running etc. It was of course very easy to correct he semantic feature in the dictionary, but the problem was that there were far too many corrections required. On the other hand, the fact that both languages allow a high degree of word-order freedom accounted for a certain simplification of  the translation process. The grammar elied on the fact that there are only minor word-order differences between Czech and Russian.
857	Assumes Prior Knowledge	Additionally, we randomly selected a contrast  question sample of 22 questions rated  unproblematic with regard to incorrect  presuppositions by all three raters. Examples (1)  and (2) are questions rated as problematic by at  least two raters; examples (3) and (4) present  questions that do not contain presuppositions.  (1) Is that the same place you USUALLY go  when you need routine or preventive care, such as  a physical examination or check up?  (2) How much do your parents or parent know  about your close friends' parents?  (3) From date to December 31, did you take one  or more trips or outings in the United States, of at  least one mile, for the PRIMARY purpose of  observing, photographing, orfeeding wildlife?  (4) Are you now on full-time active duty with the  armed forces?  Example (1) presupposes the habit of making  use of routine / preventive care; (2)  presupposes that the respondent has close  friends.
426	Explains Technical Concepts	Summarization of such a large event, or mul-  tiple documents about multiple topics, is the  concern of this paper. Summarization of multi-  ple documents containing nmltiple topics is an  unexplored research issue. Some previous tud-  ies on summarization (McKeown and Radev,  1995; Barzilay et al, 1999; Mani and Bloedorn,  1999) deal with multiple docmnents about a sin-  gle topic, but not about multiple topics 1.  In order to smnmarize lnultiple docmne, nts  with multiple topics, one needs a general,  semantics-oriented method for evaluating im-  portance. Summarization of a single document  may largely exploit the doculnent structure. As  an extreme example, the first paragraph of a  newspaper article often serves as a smmnary of  the entire article. On the other hand, summa.-  rization of multiple, documents in general must  be more based on their semantic structures, be-  cause the, re is no overall consistent document  structure across them.
962	Explains Non-Technical Concepts	6. LANGUAGE MODELING  Language Modeling is used in Sphinx-II at two different  points. First, it is used to guide the beam search. For that  purpose we used a conventional backoff bigram for that pur-  pose. Secondly, it is used to recalculate linguistic scores for  the top N hypotheses, as part of the N-best paradigm. We  concentrated most of our language modeling effort on the  latter.
296	Explains Technical Concepts	3.3 Feature Model for Syntactic Discriminants In practice, there are different ways of finding dis- criminants from the parse forest. For instance, the [incr tsdb()] system supports both syntax- based and semantics-based discriminants. The syntax-based discriminants are extracted from the derivation trees of the HPSG analyses. All HPSG rule applications (unary or binary) and choices of lexical entries are picked as candidate dis- criminants and checked for effectiveness. The semantics-based discriminants, on the other hand, represent the differences on the semantic struc- tures (MRS in the cases of DELPH-IN2 gram- 1http://tadm.sourceforge.net/ 2http://www.delph-in.net/ mars). With a few exceptions, many DELPH-IN HPSG treebanks choose to use the syntactic dis- criminants which allow human annotators to pick the low-level constructions. The above proposed ranking model works for different types of dis- criminants (and potentially a mixture of different discriminant types). But for the evaluation of this paper, we show the feature model designed for the syntactic discriminants only.
470	Explains Non-Technical Concepts	2.1 Human ratings  We used human ratings as the standard against  which to evaluate the performance of DP. Three  raters rated about 90 questions from 12  questionnaires provided by the Census Bureau.  DP currently does not use context. To have a fair  test of its performance, the questions were  presented to the human raters out of context, and  they were instructed to rate them as isolated  questions. Ratings were made on a four-point  scale, indicating whether the question contained  no presupposition (1), probably contained no  presupposition (2), probably contained a  presupposition (3), or definitely contained a  presupposition (4). We transformed the ratings  into Boolean ratings by combining ratings of 1 and  2 ("no problem") versus ratings of 3 and 4  ("problem"). We obtained very similar results for  analyses of the ratings based on the four-point and  the Boolean scale. For simplicity, we just report  the results for the Boolean scale.
992	Explains Non-Technical Concepts	Abstract Treebank annotation is a labor-intensive and time-consuming task. In this paper, we show that a simple statistical ranking model can significantly improve treebank- ing efficiency by prompting human an- notators, well-trained in disambiguation tasks for treebanking but not necessarily grammar experts, to the most relevant lin- guistic disambiguation decisions. Experi- ments were carried out to evaluate the im- pact of such techniques on annotation ef- ficiency and quality. The detailed analysis of outputs from the ranking model shows strong correlation to the human annotator behavior. When integrated into the tree- banking environment, the model brings a significant annotation speed-up with im- proved inter-annotator agreement.?
224	Assumes Prior Knowledge	1.2 System description The system was rule-based, implemented in Colmerauer's Q-systems. It contained a full- fledged morphological and syntactic analysis of Czech, a transfer and a syntactic and morphological generation of Russian. There was almost no transfer at the beginning of the project due to the assumption that both languages are similar to the extent that does not require any transfer phase at all. This assumption turned to be wrong and several phenomena were covered by the transfer in the later stage of the project (for example the translation of the Czech verb "b~" \[to be\] into one of the three possible Russian equivalents: empty form, the form "byt6" in future tense and the verb "javljat6sja"; or the translation of verbal negation).
693	Assumes Prior Knowledge	1 Introduction  In Cross-Language Information Retrieval  (CLIR), most of users? queries are generally  composed of short terms, in which there are  many Out-of-Vocabulary (OOV) terms like  Named Entities (NEs), new words, terminolo- gies and so on. The translation quality of OOV  term directly influences the precision of query- ing relevant multilingual information. There- fore, OOV term translation has become a very  important and challenging issue in CLIR.  With the increasing growth of Web informa- tion which includes multilingual hypertext re- sources with abundant topics, it appears that  Web information can mitigate the problem of  the restricted OOV term translation accuracy  (Lu and Chien, 2002). However, how to select  the correct translations from Web information  and locate the appropriate translation resources  rapidly is still the main goal for OOV term  translation. Hence, finding the effective feature  representation and the optimal ranking pattern  for translation candidates is the core part for  the Web-based OOV term translation.
236	Explains Non-Technical Concepts	Abstract We report on efforts to build large-scale translation systems for eight European language pairs. We achieve most gains from the use of larger training corpora and basic modeling, but also show promising results from integrating more linguistic an- notation.
353	Explains Technical Concepts	The core summarization problem is taking a  single text and producing a shorter text in the same  language that contains all the main points in the  input text. We are using a robust, graded approach  to building the core engine by incorporating statis-  tical, syntactic and document structure analyses  among other techniques. We have developed a sys-  tem design which allows the parameterization both  of the summarization process and of necessary  information about he languages being processed.  Document structure analysis (Salton & Singal  94, Salton et al 95) is important for extracting the  topic of a text. In a statistical analysis for example  (Paice 90, Paice & Jones 93), titles and sub-titles  would be given a more important weight than the  body of the text. Similarly, the introduction and  conclusion for the text itself and for each section  are more important han other paragraphs, and the  first and last sentences in each paragraph are more  important han others. The applicability of these  depends on the style adopted in a particular  domain, and on the language: the stylistic structure  and the presentation of arguments vary signifi-  cantly across genres and languages. Structure anal-  ysis must be tailored to a particular type of text in a  particular language. In the MINDS system docu-  ment structure analysis involves the following sub-  tasks:  ? Language Identification  ? Document Structure Parsing  ? Multilingual Sentence Segmentation  ? Text Structure Heuristics
238	Explains Technical Concepts	3.1 Good Turing Smoothing Traditionally, we use raw counts to estimate con- ditional probabilities for phrase translation. How- ever, this method gives dubious results for rare counts. The most blatant case is the single oc- currence of a foreign phrase, whose sole English translation will receive the translation probability 1 1 = 1.
566	Explains Technical Concepts	A bunsetsu having wide dependency tends to  precede a bunsetsu having narrow dependency.  A bunsetsu having wide dependency is defined  as a bunsetsu which does not rigidly restrict its  modifiee. For example, the bunsetsu "~btqlo_c  (to Tokyo)" often depends on a bunsetsu whicll  contains a verb of motion such as "ihu (go)"  while the bunsetsu "watashi_.qa (I)" can depend  on a bunsetsu which contains any kind of verb.  Here, the bunsetsu "watashi_ga (I)" is defined as  a bunsetsu having wider dependency than 1;11o  tmnsetsu ':Tok~./o_c (to Tokyo)." We call the  concept of how rigidly a modifier restricts its  modifiee the width of dependency.
993	Explains Technical Concepts	3.1. The SSM and "Viterbi" Training with Tied Mixtures  The SSM is characterized by two components: a fam-  ily of length-dependent distribution functions and a de-  terministic mapping function that determines the dis-  tribution for a variable-length observed segment. More  specifically, in the work presented here, a linear time  warping function maps each observed frame to one of  m regions of the segment model. Each region is de-  scribed by a tied Gaussian mixture distribution, and  the frames are assumed conditionally independent given  the length-dependent warping. The conditional inde-  pendence assumption allows robust estimation of the  model's tatistics and reduces the computation ofdeter-  mining a segment's probability, but the potential of the  segment model is not fully utilized. Under this formu-  lation, the SSM is similar to a tied-mixture tIMM with  a phone-length-dependent, constrained state trajectory.  Thus, many of the experiments reported here translate  to HMM systems.
448	Assumes Prior Knowledge	2. BYBLOS  All of the experiments that will be described were performed  using the BBN BYBLOS speech recognition system. This sys-  tem introduced an effective strategy for using context-dependent  phonetic hidden Markov models (HMM) and demonstrated their  feasibility for large vocabulary, continuous peech applications  \[2\]. Over the years, the core algorithms have been refined with  improved algorithms for estimating robust speech models and us-  ing them effectively to search for the most likely sentence.  The system can be trained using the pooled speech of many  speakers or by training separate models for each speaker and then  averaging the resulting models.
997	Explains Non-Technical Concepts	We propose instead to provide the user with  sentences which are semantically close to the orig-  inal input (in a sense to be defined below) and  are acceptable inputs to the system. Such feed-  back may occasionally be confusing, but we ex-  pect that more often it will be helpful in showing  the system's capabilities and suggesting possible  rephrasings.
227	Explains Technical Concepts	For example, the phrase ?the engine of the car? contains the part-whole relation that ?engine? is  part of ?car?. This relation is very useful for  feature extraction, because if we know one  object is part of a product class, this object  should be a feature. ?no? pattern is another  extraction pattern. Its basic form is the word  ?no? followed by a noun/noun phrase, for  instance, ?no noise?. People often express their  short comments or opinions on features using  this pattern. Both types of patterns can help find  features missed by double propagation. As for  the low precision problem, we present a feature  ranking approach to tackle it. We rank feature  candidates based on their importance which  consists of two factors: feature relevance and  feature frequency. The basic idea of feature  importance ranking is that if a feature candidate  is correct and frequently mentioned in a corpus,  it should be ranked high; otherwise it should be  ranked low in the final result. Feature frequency  is the occurrence frequency of a feature in a  corpus, which is easy to obtain. However,  assessing feature relevance is challenging. We  model the problem as a bipartite graph and use  the well-known web page ranking algorithm  HITS (Kleinberg, 1999) to find important  features and rank them high. Our experimental  results show superior performances. In practical  applications, we believe that ranking is also  important for feature mining because ranking  can help users to discover important features  from the extracted hundreds of fine-grained  candidate features efficiently.
540	Explains Technical Concepts	2.3 System configurations for WMT 2010  In the weeks preceding the evaluation, we tried  several ways of arranging the resources available  to us. We picked the configurations that gave the  highest BLEU scores on WMT2009 Newstest.  We found that tuning with lattice MERT rather  than N-best MERT allowed us to employ more  parameters and obtain better results.   E-F system components:  1. Phrase table trained on ?domain?;   2. Phrase table trained on GigaFrEn;   3. Lexicalized distortion model trained on  ?domain?;   4. Distance-based distortion model;  5. 5-gram French LM trained on ?mono?;   6. 4-gram LM trained on French half of  GigaFrEn;   7. Dynamic LM composed of 4 LMs, each  trained on the French half of a parallel  corpus (5-gram LM trained on ?domain?,  4-gram LM on GigaFrEn, 5-gram LM on  news-commentary and 5-gram LM on  UN).    3 Details of lattice MERT (LMERT)  Our system?s implementation of LMERT (Ma- cherey et al, 2008) is the most notable recent  change in our system. As more and more features  are included in the loglinear model, especially if  they are correlated, N-best MERT (Och, 2003)  shows more and more instability, because of  convergence to local optima (Foster and Kuhn,  2009). We had been looking for methods that  promise more stability and better convergence.  LMERT seemed to fit the bill. It optimizes over  the complete lattice of candidate translations af- ter a decoding run. This avoids some of the prob- lems of N-best lists, which lack variety, leading  to poor local optima and the need for many de- coder runs.
983	Explains Non-Technical Concepts	3 A Method  of  Us ing  In fo rmat ion on  D ia logue  Par t i c ipants This section describes how to use information on dialogue participants, such as participants' social roles and genders. First, we describe TDMT, which we also used in our experiment. Second, we mention how to modify transfer rules and transfer dictionary entries according to information on dialogue participants.
743	Explains Non-Technical Concepts	In the next section we discuss a novel technology  for cross document coreference. Like the summa-  rization system just discussed, it takes within doc-  ument coreference annotated text, produces sum-  maries in a very similar form to the above, and  individuates entities based on the similarity of the  summaries produced.
432	Explains Non-Technical Concepts	3 Method  3.1 The Corpus   The eorl)us was obtained from tlm recorded (lat~  which colnes with Italliday (1970). We inv(;sti-  gated tones 1, 2, and/l, and tone sequen('es 1 &  1, l&  2, 2 & l, 2 & 2, l & 4, mid4  & 1. A  total of 290 utterances were analysed (= 1700  words of text, approx. 350 tone groul)s). The  utter~mces ranged fl:om inono- and polysyllabic  words to sentences. The utterances varied in  tone, number of feet, the position of the Tonic,  and whether there were silent t)eats in the tone  group. Also, some of the utteran(:es had a pre-  tonic segmenl;, others did not.
221	Explains Technical Concepts	Relationships between proper nouns are made on  the basis of string matches, acronym matching, and  dictionary lookup. Acronyms are determined either  through a table lookup or an appositive construc-  tion occurring in the document which designates  the acronym for a specific proper noun. A proper  noun in the query is considered associated with  a proper noun in the document if it matches the  string or acronym of the proper noun in the docu-  ment or it appears in the definition of the proper  noun in the document. A reverse dictionary lookup  often allows cities to be associated with the country  they are in.
702	Explains Technical Concepts	Query Features  Several items of information might be available  about the query independently of any particular  retrieval approach or its representation f the query,  the documents, or their similarity:  Query Length (QLEN). The number of tokens in the  natural anguage query.  Query Terms' Specificity (QTSP). The average  inverse document frequency (IDF) of the quartile of  the query's terms with the highest IDF's.  Number of Proper Nouns (QNPN).  Number of Compound Nominals (QNCN).  Query Terms" Synonymy (QTSY). Over all terms in  the query, the average of the number of words in the  svnset for the correct sense of the query term in  WordNet. WordNet is a semantic knowledge base  that distinguishes words by their senses, and groups  word:senses that are synonymous to each other into  synsets.  Query Terms' Polyscmv (QTPL), Over all terms in  query, the average number of senses for the query  term in WordNet,
390	Mathematically-Oriented Paragraph	2.3  Ru le  Compos i t ion  and  Decompos i t ion   In Koskennlemi's systems, rule composition is fairly straightfor-  ward. Samples of the three types of rules are repeated here:  R4) a>b=:~e/de / f  g /h i / j   R5) a > b ?=== e/d e/ f_  g/h i/j  R6) a > b ~ e/d e / f _  g/h i/j  If a grammar contains the two rules, R4 and RS, they can be  replaced by tile single rule R6.  In contrast, the composition of rules in the system proposed  here is slightly more complicated. We need the notion of a default  correspondence. The default correspondence for any alphabetic  character is itself. In other words, in the absence of any rules,  an alphabetic haracter will correspond to itself. There may also  be characters that are not alphabetic, e.g., the \[+\] representing a  morpheme boundary, currently the only non-alphabetic charac-  ter in this system. Other conceivable non-alphabetic characters  would be an accent mark for representing stress, or say, a hash  mark for word boundarics. The default for these characters is  that they correspond to 0 (zero). Zero is ttle name for the null  character used ill this system.
902	Assumes Prior Knowledge	Abstract Given the increasing need to process mas- sive amounts of textual data, efficiency of NLP tools is becoming a pressing concern. Parsers based on lexicalised grammar for- malisms, such as TAG and CCG, can be made more efficient using supertagging, which for CCG is so effective that every derivation consistent with the supertagger output can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of deriva- tions for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a wide- coverage CCG parser that can process al- most 100 sentences per second, with lit- tle or no loss in accuracy over the baseline with no pruning.
9	Explains Non-Technical Concepts	Consequently, the cases where machine translation can play a helpful role in this context is when, for a seg- ment to be translated, there is no exact match and the available fuzzy matches do not exceed a certain thresh- old. This threshold in our case is expected to be 85% or lower. To this end, there exists a system called ECMT (European Commission Machine Translation; also ac- cessible to other European institutions) which is a rule- based system.
554	Explains Non-Technical Concepts	4.4 Performance  A second potential concern was performance:  would the increased number of records in the chart  cause unacceptable degradation of system speed?  5 A similar observation is made by Charniak et al  (forthcoming), who find that the number ot' final parses  caused by additional POS tags is far less than the  theoretical worst case in reality.
665	Explains Technical Concepts	2.2. Language Modeling  The DECIPI-IER m system uses a probabilistie finite state  grammar (PFSG) to constrain allowable word sequences. In the  ATIS, WSL and Credit Card tasks, we use a word-based bigram  grammar, with the language model probabilities timated using  Katz's back-off bigrarn algorithm \[12\]. All words that are not in  the specified vocabulary that are in the language model training  data are mapped to the background word model. The background  word model is treated like all the other words in the recognizer,  with bigram language model probabilities on the grammar t ansi- T  tions between words.
26	Explains Technical Concepts	2.2 SFG  According to SFG the unit to which intonation  is attr ibuted is the tone group. A tone group  consists of.feet, and feet consist of syllables. A  tone group carries a tune or tone, which can be  falling (tone 1), rising (tone 2), level (tone 3),  faning-risiug (tone 4), or rising-f~lling (tone 5).  See Fig. 2 giving these five options with their  approximate pragmatic meanings. The exam-  ples in Fig. 3 show how tone is annotated in  SFG: the nmnber gives the kind of tone, the  double slashes snark the tone group boundaries  and the single slashes mark feet. Also, there  may be combinations of different; tones in one  utterance, e.g., tone 4 followed by tone 1 (ex-  ample (c) in Fig. 3).
22	Explains Technical Concepts	So far, optimal fusion coefficients for a query  have been determined using full knowledge of the  relevance of the documents for the query. In the  retrospective retrieval setting, these relevance  judgments will not be available beforehand, and thus  cannot be used to adjust the fusion model to the  query. For the retrospective setting, we seek to  construct a dynamic fusion function that can adjust  the way it fuses the five systems' relevance scores for  a query-document pair using additional inputs. These  inputs include the Icatures of the query, features of  the retrieved documents, and features of the joint  to the linear fusion model and the individual retrieval  systems.
31	Explains Technical Concepts	The core engine is designed in such a way that  as additional resources, such as lexical and other  knowledge bases or text processing and MT  engines, become available from other ongoing  research efforts they can be incorporated into the  overall multi-engine MINDS system. The most  promising components are part of speech tagging,  anaphora resolution, and semantic methods to  allow concept identification to supplement word  frequency analysis. Part of speech tagging has  already been used to perform sentence length  reduction by stripping out "superfluous" words and  phrases. The other methods will be used to main-  tain document coherence, and to improve sentence  selection and reduction.
325	Explains Technical Concepts	Comparing HTML structures seems to be a sound way to evaluate candidate pairs since parallel pairs usually have similar HTML structures. However, we also noticed that parallel texts may have quite dif- ferent HTML structures. One of the reasons is that the two files may be created using two HTML ed- itors. For example, one may be used for English and another for Chinese, depending on the language handling capability of the editors. Therefore, cau- tion is required when measuring structure difference numerically.
923	Explains Technical Concepts	Instead we developed a much simpler, less costly method to  select among the microphones. For each of the seven micro-  phone types (Senrtheiser plus six alternate types) we estimated  a mixture density consisting of eight Gaussians. Then, given a  sentence from an unknown microphone, we computed the prob-  ability of the data being produced by each of the seven mixture  densities. The one with the highest likelihood was chosen, and  we then used the transformed codebook corresponding to the cho-  sen microphone type. We found that on development data this  microphone selection algorithm was correct about 98% of the  time, and had the desirable property that it never misclassified  the Sennheiser data.
104	Explains Technical Concepts	4.5 Bilingual POS language model The main advantage of POS-based information is that there are less data sparsity problems and therefore a longer context can be considered. Con- sequently, if we want to use this information in the translation model of a phrase-based SMT system, the POS-based phrase pairs should be longer than the word-based ones. But this is not possible in many decoders or it leads to additional computa- tion overhead.
967	Assumes Prior Knowledge	Coupling the system to an SQL database. After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geog- raphy questions. HSQL produced a Prolog-like code FOL (First Order Logic) for execution. A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
605	Assumes Prior Knowledge	Both models are trained using a very similar pipeline as for the phrase model. The main dif- ference is that the translation rules do not have to be contiguous phrases, but may contain gaps with are labeled and co-ordinated by non-terminal sym- bols. Decoding with such models requires a very different algorithm, which is related to syntactic chart parsing.
234	Explains Technical Concepts	In this paper, we propose a method for acquiring  from corpora the relationship between the conditions  itemized above and word order in Japanese. The  method uses a model which automatically discovers  what the tendency of the word order in Japanese is  by using various kinds of information in and around  the target bunsetsus. This model shows us to what  extent each piece of information contributes to decid-  ing the word order and which word order tends to be  selected when several kinds of information conflict.  The contribution rate of each piece of information in  deciding word order is efficiently learned by a model  within a maximum entrot)y (M.E.) framework. The  performance of the trained model can be evaluated  according to how many instances of word order se-  lected by the model agree with those in the original  text. Because the word order of the text in the corpus  is correct, the model can be trained using a raw co>  pus instead of a tagged corpus, if it is first analyzed  by a parser. In this paper, we show experimental re-  sults demonstrating that this is indeed possible even  when the parser is only 90% accurate.  This work is a part of the corpus based text gen-  eration. A whole sentence can be generated in the  natural order by using the trained model, given de-  pendencies between bunsetsus. It could be helpful  for several applications uch as refinement support  and text generation in machine translation.
998	Explains Non-Technical Concepts	2 Related work  Hu and Liu (2004) proposed a technique based  on association rule mining to extract product  features. The main idea is that people often use  the same words when they comment on the  same product features. Then frequent itemsets  of nouns in reviews are likely to be product fea- tures while the infrequent ones are less likely to  be product features. This work also introduced  the idea of using opinion words to find addi- tional (often infrequent) features.
738	Explains Technical Concepts	From the experiment data, we find the preci- sion and the recall almost below 80%, and the  classification performance is not expected. The  reason is that we do not consider some special  application situations beforehand, which result  in classifying errors. For example, the Global  Time should be taken as the Local Time when it  appears in the dialog or speech that marks boun- daries by a pair of quotation marks. So we intro- duce some revising patches shown in Table 4 to  deal with this issue. Here the second and the  fourth patches make corresponding temporal  expressions be treated as non-target times that  need not be processed. In addition, Time Set and  Non-Specific are taken as the other classes ex- cept the Implicit Time and the Explicit Time.  The final results with revising patches are shown  in Table 5. Obviously, revising patches make the  classification be more adapted for the real texts,  and the performance evaluation is promising.
272	Explains Technical Concepts	Fig.2 Two-Stage Retrieval and Methods of Improvements  stage retrieval, the raw query which is a user-provided  description of information eeds is directly employed  by the retrieval algorithm to assign a retrieval status  value (RSV) to each document in a collection, and the  ranked list of documents is interpreted as the f'mal  retrieval result. In a 2-stage strategy, this initial ranked  list is interpreted as but an intermediate step. The set  of n top-ranked ocuments of the initial retrieval is  assumed relevant, even though the user has not made  any judgment. These 'pseudo-relevant' documents are  then used to modify the weight of the initial query  according to some learning procedure, as well as to  expand the query with terms from these documents  based on some selection criteria like frequency of  occurrence. The modified query is then used to do a  second retrieval, and the resultant ranked list becomes  the final result. This helps because if the raw query is  reasonable and the retrieval engine is any good, the  initial top n documents can be considered as defining  the topical domain of the user need and should have a  reasonable density of relevant or highly related  documents, and the procedure simulates real relevance  feedback.
456	Explains Technical Concepts	The work presented here reports develop- ments on affect detection from one particular  comparatively complex metaphorical phenome- non with affect implication, i.e. the cooking me- taphor (?the lawyer grilled the witness on the  stand?, ?I knew I was cooked when the teacher  showed up at the door?)  (http://knowgramming.com/cooking_metaphors. htm). Since context plays an important role in  the interpretation of the affect conveyed by the  user during the interaction, we have used lin- guistic contextual analysis and cognitive emo- tional modeling based on Markov chain model- ing and a dynamic algorithm to interpret affect  from context in our application.
712	Assumes Prior Knowledge	1 Introduction t In many natural language processing systems currently in use,  the morphological phenomena re handled by programs which  do not interpret any sort of rules, but rather contain references  to specific morphemes, graphemes, and grammatical categories.  Recently Kaplan, Kay, Koskennicmi, and Karttunen have shown  how to construct morphological analyzers in which the descrip-  tions of the orthographic and syntactic phenomena are separable  from the code. This paper describes a system that builds on  their work in the area of phonology/orthography and also has  a well defined syntactic omponent which applies to the area of  computational morphology for the first time some of the tools  that have bccn used in syntactic analysis for quite a while.  This paper has two main parts. The first deals with the or-  thographic aspects of morphological analysis, the second with  its syntactic aspects. The orthographic phenomena constitute  a blend of phonology and orthography. The orthographic rules  given in this paper closely resemble phoImlogical rules, both in  form and fimctlon, but because their purpose is the description of  orthographic facts, the words orthography and orthographic will  be used in preference to phonology and phonological.  The overall goal of the work described herein is the devel-  opment of a flexible, usable morphological analyzer in which the  rules for both syntax and spelling arc (1) separate from the code,  and (2) descriptively powerful enough to handle the phenomena  encountered when working with texts of written language.
2	Assumes Prior Knowledge	2.3 P re l iminary  compar i son   On a technical level, the major differences we  can observe between the ToBI and SFG annota-  tion schemata of intonation are the following.  Uni ts .  While there is a rough cor-  respondence between the intonation  phrase/intermediate phrase in ToB~ and  the tone group in SFG (cf. Harrington &  Cassidy (1999)), in Tom the refit of the foot is  not acknowledged.
785	Explains Technical Concepts	From a different point of view, the rankings of the discriminants show annotators? confidence on various ambiguities. The clearly uneven distri- bution over discriminants can also provide gram- mar writers with interesting feedback, helping with the improvement of the linguistic analysis. We would also like to integrate confidence mea- sures into the computer-assisted treebank annota- tion process, which could potentially help annota- tors make difficult decisions, such as whether to reject all trees for a sentence.
780	Explains Non-Technical Concepts	2.1 Data The database we used for our system was the same as the one used by the operators at the call center. This database consists of the most common parts and was built by the operators themselves. However, the data contained in the database is not clean and there are several types of errors including mis-spellings, use of non- standard abbreviations, use of several different abbreviations for the same word, etc. The database consists of approximately 7000 different parts. For each part, the database contains its identification umber, a description, and the product (machine type) that it is used in. The descriptions consist of approximately 60,000 unique words of which approximately 3,000 are words which either are non-standard abbreviations or are unique to the medical domain (example: collimator).
622	Assumes Prior Knowledge	1 In t roduct ion   The pallet describes the main results of a con>  parison of /;he ToB: (Tone-and-Break-Indices)  ai)proach (Pierrehumbert, 1.9801 Silverman el;  al.., 19961 to annotating English speech data  with information about intonation and one of  the British School approaches (e.g., Brazil et al  (1980)), Systenfie Fmmtional Grammar (SFO;  (Halliday, 19671 Halliday, 1970)). The goal of  this comparison is the definition of a mapping  between the two systems.
443	Explains Non-Technical Concepts	Conclusion  This paper has shown that speech repairs not  only play a decisive role in speech processing  technology systems, they also provide empirical  evidence and insights into the inherent linguistic  characteristics of languages. Based on the results  of corpus analysis, similar syntactic features of  speech repairs ill German and Chinese were  identified and the repair syntax was formally  modelled by means of phrasal modelling and  finite state automata. Discrepancy at the  morphological evel of both languages was  shown and more detailed investigations are  necessary. Further analyses on acoustic-prosodic  features of cross-linguistic data am CmTently  being can'ied out.
399	Explains Technical Concepts	2 In tonat ion  Annotat ion   The majority of text-to-speech systems that al-  low for the manipulation of an input string so  as to control intonation employ the ToBI system  (Silverman et al, 19961, which is based on the  autosegmental-metrical approach originally set  up by Pierrehumbert (19801 to describe Amer-  ican English intonation. Versions of ToBI for  other languages have been developed, e.g., Grice  et al (19961 for German, and are also widely  used in computational contexts. One major the-  oretical difference between the ToBI approach  and the British School approaches, uch as the  one advocated by SFG, is that in the latter there  is a built-in focus on the relation between into-  mttion and nmaning. In spG, intonation con-  tours are distinguished according to their di, f fcrcntial meanings, i.e., they label pitch move-  ments that are commonly interpreted by the  speakers of (British) English as having quite  different pragmatic purport (cf. Teich et al  (1997)). This is what snakes the SFO approach  attractive in the context of concept-to-speech  generation, in which it is crucial to be able  to represent criteria for selecting an intonation  contour appropriate in a given context. TOBI,  on the other hand, is a phonetic-phonological  annotation scheme tbr intonation. Since it is  widely used, there exist nmnerous tools sup-  porting analysis with a high degree of analyt-  ical rigor. It seems theretbre doubly significant  to combine the two approaches in an attempt o  achieve high-quality synthesized speech output.  While clearly some fimdamental theoretical  ditferences exist between the ToBI and SFG ap-  proaches, more technically there is a basic com-  mortality. Any annotation scheme tbr intonation  nmst establish three principal constructs for the  representation f intonation: the units of into-  nation, a set of categories that describe the pitch  movement occurring in that unit, and a set of  labels that mark the nuclear stress oi1 which the  pitch movement is realised.
855	Assumes Prior Knowledge	There are two main reasons which bring down these rates. One reason is that TDMT does not know who or what the agent of the action in the utterance is; agents are also needed to se- lect polite expressions. The other reason is that there are not enough rules and transfer dictio- nary entries for the clerk. It is easier to take care of the latter problem than the former problem. If we resolve the lat- ter problem, that is, if we expand the transfer rules and the transfer dictionary entries accord- ing to the "participant's social role" (a clerk and a customer), then the recall rate and the preci- sion rate can be improved (to 86% and 96%, respectively, as we have found). As a result, we can say that our method is effective for smooth conversation with a dialogue translation system.
914	Explains Technical Concepts	The transduction module consists of three  processing steps: lexico-structural pre-  processing, main lexico-structural processing,  and lexico-structural post-processing. Each of  these steps is driven by a separate grammar, and  all three steps draw on a common feature data  base and lexicon. The grammars, the lexicon  and the feature data base are referred to as the  linguistic resources (even if they sometimes  apply to a conceptual representation). All  linguistic resources are represented in a  declarative manner. An instantiation of the tree  transduction module consists of a specification  of the linguistic resources.  Input Dependency Structure  ~ L exlco-Structural Preproce~ing  Intermediate Dependency StructttreL_~  Lexico-Structm'al Processing  Intermediate + Dependency Structure  ~ Lexico-Structural  Postprocessing  Output / /~   Dependency SUucturc
29	Explains Technical Concepts	We define progressive s arch techniques as those which  can be used to efficiently implement other, computationally  burdensome t chniques. They use results of a simple and fast  speech recognition technique to constrain the search space of a  following more accurate but slower unning technique. This may  be done iteratively---each progressive search pass uses a  previous pass' constraints o run more ettieiently, and provides  more constraints for subsequent passes.
499	Explains Technical Concepts	The Construction Integration (CI) model by  Kintsch (1998) provides a good example for how  such reference ambiguity can be resolved. CI uses  a semantic network that represents an entity in the  discourse focus (such as "this person") through  higher activations of its links to other concept  nodes. Perhaps models such as the CI model can  be integrated into the QUAID model to perform  context analyses, in combination with tools like  Latent Semantic Analysis (LSA, Landauer &  Dumais, 1997), which represents text units as  vectors in a high-dimensional semantic space.  LSA measures the semantic similarity of text units  (such as questions) by computing vector cosines.  This feature may make LSA a useful tool in the  detection of a previous question that establishes a  presupposed ntity in a later question.  However, questionnaires differ from connected  discourse, such as coherent stories, in aspects that  make the present problem rather more difficult.  Most importantly, the referent for "this person"  may have been established in question umber 1,  and the current question containing the  presupposition "this person" is question umber  52. A DIP system would have to handle a flexible  amount of context, because the distance between  questions establishing the correctness of a  presupposition a d a question building up on it can  vary. On the one hand, one could limit the  considered context to, say, three questions and risk  missing the critical question. On the other hand, it  is computationally expensive to keep the complete  previous context in the systems "working  memory" to evaluate the few presuppositions  which may refer back over a large number of  questions. Solving this problem will likely require  comparing a variety of different settings.
96	Explains Technical Concepts	2 Background Large-scale full syntactic annotation has for quite some time been approached with mixed feelings by researchers. On the one hand, detailed syn- tactic annotation serves as a basis for corpus- linguistic study and data-driven NLP methods. Especially, when combined with popular super- vised machine learning methods, richly annotated language resources, like, for instance, treebanks, play a key role in modern computational linguis- tics. The public availability of large-scale tree- banks in recent years has stimulated the blossom- ing of data-driven approaches to syntactic and se- mantic parsing.
214	Explains Technical Concepts	3.1. The Word-Life Algorithm  We implemented the following algorithm to generate a  word-lattice as a by-product of the beam search used in  recognizing a sentence with the DECCIPHER TM system\[4-7\].  1. For each frame, insert into the table Active(W, t) all  words W active for each time t. Similarly construct  tables End(W, t) and Transitions(W~, W 2, t) for all  words ending at time t, and for all word-to-word  transition at time t.  2. Create a table containing the word-lives used in the  sentence, WordLives(W, T~tan, Tend). A word-life for  word W is defined as a maximum-length interval  (frame Tstar t to Ten d) during which some phone in  word W is active. That is,  W E Active (W,  t), Tstar t~ t ~ Ten d  3. Remove word-lives from the table if the word never  ended between T, tan and Te~, that is, remove  WordLives(W, Tsta, ~, Tend) if there is time t between  Tstar t and Te,ut where End(W, 0 is true.  4. Create a finite-state graph whose nodes correspond  to word-lives, whose arcs correspond to word-life  transitions stored in the Transitions table. This finite  state graph, augmented by language model  probabilities, can be used as a grammar for a  subsequent recognition pass in the progressive  search.
807	Assumes Prior Knowledge	 In TREC-7, the GE/Rutgers/SICS/Helsinki team  has performed runs in the main ad-hoc task. We  used two retrieval engines, SMART and InQuery,  built into the stream model architecture. The pro-  cessing of TREC data was performed at Helsinki us-  ing the commercial Functional Dependency Gram-  mar (FDG) text processing toolkit. Six linguistic  streams have been produced, as described below.  Processed text streams were sent via ftp to Rutgers  for indexing using their version of Inquery system.  Additionally, 4 steams produced by GE NLToolset  for TREC-6 were reused in SMART indexing.  Adhoc topics were processed at GE using both  automatic and manual topic expansion. We used the  interactive Query Expansion Tool to expand topics  with automatically generated summaries of top 30  documents retrieved by the original topic. Manual  intervention was restricted to accept/reject decisions  on summaries. We observed time limit of 10 minutes  per topic.
388	Assumes Prior Knowledge	Word choice can be a major issue as well for cross-language retrieval systems. Some ambiguity problems can be resolved through the use of a part-of-speech tagger on the captions. As Resnik and Yarowsky (in press) observe, part-of-speech tagging considerably reduces the word sense disambiguation problem. However, some ambiguity remains. For example, the decision to translate a word as car, automobile, or vehicle, may dramatically affect retrieval accuracy. The PictureQuest system uses a semantic net based on WordNet (Fellbaum 1998) to expand terms.
667	Explains Technical Concepts	Future  Goa ls   Central to the future of this research program is  the CAMP software system. We are continually re-  fining and extending the software to better capture  the coreference relations that we need and to re-  duce genre dependent aspects of the system. We  are currently exploring visualization interfaces to  both within and cross-document coreference which  we believe will provide strong motivation for im-  portance of corefence annotation of free text data-  bases. In addition, we are interested in generating  cross-document summaries based on similar tech-  niques to our within document summarization sys-  tem.
483	Explains Non-Technical Concepts	6 Conclusion This paper presented our statistical machine trans- lation system developed for the translation task us- ing Moses. Our submitted runs were generated from models trained on all the corpora made avail- able for the workshop, as this method had pro- vided the best results in our experiments. This system was enhanced using IR methods which exploits news monolingual copora, N-best list reranking and a French grammatical checker. This was our first participation where such a huge amount data was involved. Training models on so many sentences is challenging from an engi- neering point of view and requires important com- putational resources and storage capacities. The time spent in handling voluminous data prevented us from testing more approaches. We suggest that the next edition of the workshop could integrate a task restraining the number of parameters in the models trained.
460	Assumes Prior Knowledge	An approach such as the one described in (Nasr  et al, 1998; and Palmer and al., 1998) seems to  be solving a part of the problem when it uses  corpus analysis techniques for automatically  creating a first draft of the lexical transfer  dictionary using statistical methods. However,  the remaining work is still based on handcrafting  because the developer must refine the rules  manually. The current framework offers no  support for merging handcrafted rules with new  lexical rules obtained statistically while  preserving the valid handcrafted changes and  deleting the invalid ones. In general, a better  integration of linguistically based and statistical  methods during all the development phases is  greatly needed.
779	Assumes Prior Knowledge	5.2 Level tagging A binary tag cannot take effect when there is any chart cell in the corresponding column or diagonal that contains constituents. For example, the begin tag for the word ?card? in Figure 3 cannot be 0 be- cause ?card? begins a two-word constituent ?card games?. Hence none of the cells in the column can be pruned using the binary begin tag, even though all the cells from the third row above are empty. We propose what we call a level tagging approach to address this problem. Instead of taking a binary value that indicates 4The baseline differs slightly to the previous section be- cause gold-standard POS tags were used for the beam-search experiments.
507	Explains Technical Concepts	In the opening section of this paper we argued  that the quality of the initial search topic, or user's  information need statement is the ultimate factor  in the performance of an information retrieval sys-  tem. This means that the query must provide a suf-  ficiently accurate description of what constitutes the  relevant information, as well as how to distinguish  this from related but not relevant information. We  also pointed out that today's NLP techniques are  not advanced enough to deal effectively with seman-  tics and meaning, and instead they rely on syntactic  and other surface forms to derive representations of  content.
120	Assumes Prior Knowledge	Abstract  In this paper we describe a method of acquiring word  order fl'om corpora. Word order is defined as the or-  der of modifiers, or the order of phrasal milts called  'bunsetsu' which depend on the stone modifiee. The  method uses a model which automatically discovers  what the tendency of the word order in Japanese is  by using various kinds of information in and around  the target bunsetsus. This model shows us to what  extent each piece of information contributes to de-  ciding the word order mid which word order tends to  be selected when several kinds of information con-  flict. The contribution rate of each piece of informa-  tion in deciding word order is eiIiciently learned by a  model within a maximum entropy framework. The  performance of this traiimd model can be ewfluated  by checking how many instances of word order st-  letted by the model agree with those in the original  text. In this paper, we show t, hat even a raw cor-  pits that has not been tagged can be used to train  the model, if it is first analyzed by a parser. This  is possible because the word order of the text in the  corpus is correct.
261	Explains Non-Technical Concepts	4. EXPERIMENT  4.1 System Implementat ion  The experimental system developed by the authors  is shown in Figure 4. The system consists of three  subsystems: a translation control program, a  morphological analysis program and a syntactic and  semantic analysis program. The total size of the system is  about 35K steps in PL/I. Two large dictionaries are also  developed: a word dictionary of 160,000 entries and a case  frame dictionary of 4,600 verbs.
64	Assumes Prior Knowledge	The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997). Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today. The most im- portant extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
782	Explains Technical Concepts	To address the above issue, this paper  presents two pivot language-based translitera- tion strategies for low-density language pairs.  The first one is system-based strategy (Khapra  et al, 2010), which learns a source-pivot mod- el from source-pivot data and a pivot-target  model from pivot-target data, respectively. In  decoding, it first transliterates a source name to  N-best pivot names and then transliterates each  pivot names to target names which are finally  re-ranked using the combined two individual  model scores. The second one is model-based  strategy. It learns a direct source-target transli- teration model from two independent1 source- pivot and pivot-target name pair corpora, and  then does direct source-target transliteration.  We verify the proposed methods using the  benchmarking data released by the  NEWS20092 (Li et al, 2009a, 2009b). Expe- riential results show that without relying on  any source-target parallel data the system- based pivot strategy performs quite well while  the model-based strategy is less effective in  capturing the phonetic equivalent information.  The remainder of the paper is organized as  follows. Section 2 introduces the baseline me- thod. Section 3 discusses the two pivot lan- guage-based transliteration strategies. Experi- mental results are reported at section 4. Final- ly, we conclude the paper in section 5.
821	Explains Technical Concepts	In order to allow a multitude of techniques to  contribute to sentence selection, the core engine  adopts a flexible method of scoring the sentences  in a document by each of the techniques and then  ranking them by combining the different scores.  Text-structure based heuristics provide the main  method for ranking and selecting sentences in a  document. These are supplemented by word fre-  quency analysis methods.
867	Explains Technical Concepts	Although Fa?ade (Mateas, 2002) included  shallow natural language processing for charac- ters? open-ended utterances, the detection of  major emotions, rudeness and value judgements  is not mentioned. Zhe and Boucouvalas (2002)  demonstrated an emotion extraction module  embedded in an Internet chatting environment.  It used a part-of-speech tagger and a syntactic  chunker to detect the emotional words and to  analyze emotion intensity for the first person  (e.g. ?I?). The detection focused only on emo- tional adjectives and first-person emotions, and  did not address deep issues such as figurative  expression of emotion. There is also work on  general linguistic cues useful for affect detec- tion (e.g. Craggs and Wood, 2004).
582	Explains Technical Concepts	Noticeably, this inference algorithm is also used to efficiently compute the marginal probabil- ity P (y|x) during parameter estimation (the nor- malization constant Z(x) can also be calculated via approximation techniques). As can be seen, this algorithm is simple to design, efficient and scales well w.r.t. the size of data.
137	Explains Non-Technical Concepts	Abstract In this paper, we describe Exodus, a joint pilot project of the European Commission?s Directorate-General for Translation (DGT) and the European Parliament?s Directorate- General for Translation (DG TRAD) which explores the potential of deploying new ap- proaches to machine translation in European institutions. We have participated in the English-to-French track of this year?s WMT10 shared translation task using a system trained on data previously extracted from large in- house translation memories.
