417	Assumes Prior Knowledge	PREVIOUS WORK  The ability to improve retrieval performance by  using multiple retrieval systems has been  documented extensively (e.g,, \[1\], \[31, \[41). It is only  recently, however, that researchers have turned their  attention to the possibility of adjusting the manner in  which results are combined to the specific query at  hand. Researchers have reported success in using  initial relevance judgments to adjust the way in which  results are combined \[3\], and have also reported  success in using the joint distribution of relevance  scores from multiple marchers (among other things)  to predict when to combine the results of multiple  systems \[6\]. The purpose of the current research is to  explore the use of the joint distribution of relevance  scores, semantic and syntactic features of queries.  and the length of retrieved ocuments to predict how  to combine the results of several retrieval systems.
495	Assumes Prior Knowledge	4.3 Emotion Modeling in Communication  Context  There are also other aspects which may influ- ence the affect conveyed in the communication  context. E.g. in our application, the affect con- 1483 veyed by the speaking character himself/herself  in the recent several turn-taking, the ?improvisa- tional mood? that the speaking character is  created, and emotions expressed by other cha- racters, especially by the contradictory ones  (e.g. the big bully), have great potential to influ- ence the affect conveyed by the current speak- ing character (e.g. the bullied victim). Some- times, the story themes or topics also have po- tential impact to emotions or feelings expressed  by characters. For example, people tend to feel  ?happy? when involved in discussions on posi- tive topics such as harvest or raising salary,  while people tend to feel ?sad? when engaged in  the discussions on negative themes such as  economy breakdown, tough examination etc.
378	Explains Non-Technical Concepts	The parallel Web pages we collected from vari- ous sites are not all of the same quality. Some are highly parallel and easy to align while others can be very noisy. Aligning English-Chinese parallel texts is already very difficult because of the great differ- ences in the syntactic structures and writing sys- tems of the two languages. A number of alignment techniques have been proposed, varying from statis- tical methods (Brown et al, 1991; Gale and Church, 1991) to lexical methods (Kay and RSscheisen, 1993; Chen, 1993). The method we adopted is that of Simard et al (1992). Because it considers both length similarity and cognateness as alignment cri- teria, the method is more robust and better able to deal with noise than pure length-based methods. Cognates are identical sequences of characters in cor- responding words in two languages. They are com- monly found in English and French. In the case of English-Chinese alignment, where there are no cog- nates shared by the two languages, only the HTML markup in both texts are taken as cognates. Be- cause the HTML structures of parallel pages are nor- mally similar, the markup was found to be helpful for alignment.
670	Explains Non-Technical Concepts	7 Lessons Learned Using the Framework  Empirical results obtained from the applications  listed in Section 6 have shown that the approach  used in the framework is flexible enough and  easily portable to new domains, new languages,  and new applications. Moreover, the time spent  for development was relatively short compared  to that formerly required in developing similar  types of applications. Finally, as intended, the  limited computational power of the transduction  module, as well as careful implementation,  including the compilation of declarative  linguistic knowledge to Java, have ensured  efficient run-time behavior. For example, in the  MT domain we did not originally plan for a  separate conversion step from the parser output  to DSyntS. However, it quickly became apparent  that there was a considerable gap between the  output of the parsers we were using and the  DSyntS representation that was required, and  furthermore, that we could use the tree  transduction module to quickly bridge this gap.  Nevertheless, our tree transduction-based  approach has some important limitations. In  particular, the framework requires the developer  of the transformation rules to maintain them and  specify the order in which the rules must be  applied. For a small or a stable grammar, this  does not pose a problem. However, for large or  rapidly changing grammar (such as a transfer  grammar in MT that may need to be adjusted  when switching from one parser to another), the  burden of the developer's task may be quite  heavy. In practice, a considerable amount of  time can be spent in testing a grammar after its  revision.
345	Explains Technical Concepts	3 Domain adaptation As the only news parallel corpus provided for the workshop contains 85k sentence pairs, we must resort to other parallel out-of-domain cor- pora in order to build reliable translation models. If in-domain and out-of-domain LMs are usually mixed with the well-studied interpolation tech- niques, training translation models from data of different domains has received less attention (Fos- ter and Kuhn, 2007; Bertoldi and Federico, 2009). Therefore, there is still no widely accepted tech- nique for this last purpose.
289	Explains Non-Technical Concepts	1 I n t roduct ion Recently, various dialogue translation systems have been proposed (Bub and others, 1997; Kurematsu and Morimoto, 1996; Rayner and Carter, 1997; Ros~ and Levin, 1998; Sumita and others, 1999; Yang and Park, 1997; Vi- dal, 1997). If we want to make a conversation proceed smoothly using these translation sys- tems, it is important o use not only linguis- tic information, which comes from the source language, but also extra-linguistic nformation, which does not come from the source language, but, is shared between the participants of the conversation.
380	Explains Technical Concepts	We propose a new algorithm: collective it- erative classification (CIC) to perform approxi- mate inference to find the maximum a posteriori (MAP) segmentation and relation assignments of our model in an iterative fashion. The basic idea of CIC is to decode every target hidden variable based on the assigning labels of its sampled vari- ables, where the labels might be dynamically up- dated throughout the iterative process. Collective classification refers to the classification of rela- tional objects described as nodes in a graphical structure, as in our model.
242	Explains Technical Concepts	2.2. Best-First Stack Search  The true best-first search keeps a sorted stack of the highest  scoring hypotheses. At each iteration, the hypothesis with  the highest score is advanced by all possible next words,  which results in more hypotheses on the stack. The best-first  search has the advantage that it can theoretically minimize  the number of hypotheses considered if there is a good func-  tion to predict which theory to follow next. In addition, it  can take very good advantage of a fast match algorithm at  the point where it advances the best hypothesis.  The main disadvantage is that there is no guarantee as to  when the algorithm will finish, since it may keep backing  up to shorter theories when it hits a part of the speech that  doesn't match well. In addition it is very hard to compare  theories of different length.
47	Explains Technical Concepts	Our approach contrasts with the previous  approaches in that the word-breaking component  itself does not perform the selection of the best  segmentation analysis at all. Instead, the  word-breaker returns all possible words that span  the given string in a word lattice, and the best word  sequence is determined by applying the syntactic  rules for building parse trees. In other words, there  is no task of selecting the best segmentation per se;  the best word-breaking analysis is merely a  concomitant of the best syntactic parse. We  demonstrate hat a robust, broad-coverage parser  can be implemented irectly on a word lattice  input and can be used to resolve word-breaking  ambiguities effectively without adverse  performance effects. A similar model of  word-breaking is reported for the problem of  Chinese word segmentation (Wu and Jiang 1998),  but the amount of ambiguity that exists in the word  lattice is nmch larger in Japanese, which requires a different treatment. In the l'ollowing, we first  describe the word-breaker and the parser in more  detail (Section 2); we then report the results of  segmentation accuracy (Section 3) and the results  of related experinaents a sessing the effects of the  segmentation ambiguities in the word lattice to  parsing (Section 4). In Conclusion, we discuss  implications for future research.
878	Explains Non-Technical Concepts	We submitted translations for English-German and German-English for the Shared Translation Task. In the following we present the experiments we conducted for both translation directions ap- plying the aforementioned models and extensions to the baseline systems. The performance of each individual system configuration was measured ap- plying the BLEU metric. All BLEU scores are cal- culated on the lower-cased translation hypotheses. The individual systems that were used to create the submission are indicated in bold.
713	Explains Non-Technical Concepts	In this paper we focus on the Combinatory Cat- egorial Grammar (CCG) parser of Clark and Cur- ran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical cate- gories, or CCG supertags, with the words in a sen- tence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard max- imum entropy tagger to assign small sets of su- pertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efficient parser, given the rich struc- tural output, operating at tens of newspaper sen- tences per second.
260	Explains Technical Concepts	Powell's algorithm (PA), which is at the core  of MERT, has good convergence when features  are mostly independent and do not depart much  from a simple coordinate search; it can run into  problems when there are many correlated fea- tures (as with multiple translation and language  models). Figure 1 shows the kind of case where  PA works well. The contours of the function be- ing optimized are relatively smooth, facilitating  learning of new search directions from gradients.  Figure 2 shows a more difficult case: there is  a single optimum, but noise dominates and PA  has difficulty finding new directions. Search of- ten iterates over the original co-ordinates, miss- ing optima that are nearby but in directions not  discoverable from local gradients. Probes in ran- dom directions can do better than iteration over  the same directions (this is similar to the method  proposed for N-best MERT by Cer et al, 2008).  Each 1-dimensional MERT optimization is exact,  so if our probe stabs a region with better scores,  it will be discovered. Figures 1 and 2 only hint  at the problem: in reality, 2-dimensional search  isn?t a problem. The difficulties occur as the di- mension grows: in high dimensions, it is more  important to get good directions and they are  harder to find.
155	Explains Non-Technical Concepts	1. Introduction  The statistical approach to speech recognition requires that  we compare the incoming speech signal to our model of  speech and choose as our recognized sentence that word  string that has the highest probability, given our acoustic  models of speech and our statistical models of language.  The required computation is fairly large. When we realized  that we needed to include a model of understanding, our  estimate of the computational requirement was increased,  because we assumed that it was necessary for all of the  knowledge sources in the speech recognition search to be  tightly coupled.
771	Assumes Prior Knowledge	EXPERIMENTAL CONDIT IONS  Four corpora have been used to carry out the experiments  reported in this paper: BDSONS\[2\] and BREF\[15, 8\] for  French; and TIMIT\[4\] and WSJ0122\] for English. From the  BDSONS corpus only the phonetically equilibrated sentence  sub-corpus (CDROM 6) has been used for testing, whereas  depending on experiment, the 3 other corpora have been used  for training and testing.
863	Assumes Prior Knowledge	It is a bit surprising to see that results using the two  segmenters are very similar. It appears that better  segmentation may not mean better retrieval. It is  possible that these two segmenters are not sufficiently  different o reflect any significant changes in results. A  very high quality segmenter of 95% or higher  accuracy may tell a different story.
730	Explains Technical Concepts	With these future views in mind, this paper presents a uni-  fied approach for identifying non-linguistic speech features,  such as the language being spoken, and the identity or sex  of the speaker, using phone-based acoustic likelihoods. The  basic idea is similar to that of using sex-dependent models for  recognition, but instead of the output being the recognized  string, the output is the characteristic associated with the  model set having the highest likelihood. This approach as  been evaluated for French/English language identification,  and speaker and sex identification i both languages.
763	Explains Technical Concepts	2.2.5 The Sub-String Matching Algorithm When the dialogue manager is expecting a certain type of input (examples : product names, yes/no responses) from the user, the user response is processed by the context-based parser. Since the type of input is known, the context-based parser uses a sub-string matching algorithm that uses character-based unigram and bigram counts to match the user input with the expectation of the dialogue manager. Therefore, the sub-string matching module takes as input a user utterance string along with a list of expected responses, and it ranks the list of expected responses based upon the user response. Listed below are the details of the algorithm : 1) The algorithm first concatenates the words of the user utterance into one long string. This is needed because the speech recognition system often breaks up the utterance into words even though a single word is being said. For example, the product name AMXl l0  is often broken up into the string 'Amex 110'. 2) Next, the algorithm goes through the string formed in (1) and compares this character by character with the list of expected responses. It assigns one point for every common character. Therefore, the expected response 'AMX3' gets three points for the utterance 'Amex110'. 3) The algorithm then compares the user utterance with the list of expected responses using 2 characters (bigrams) at a time. It assigns 2 points for each bigram match. For the example shown in (2), there are two bigram matches: the first is that the utterance starts with an 'A' (the previous character is this case is the null character), and the second is the bigram 'AM'. 4) The algorithm now compares the length of the user utterance string and the expected response. If the length of the two strings is the same, then it assigns 2 points to the expected response. 5) Finally, the algorithm calculates the number of unique characters in the expected response, and the user utterance string. If these characters are the same, then it assigns 4 points to the expected response. The expected response which has the highest number of points is the most likely one. If two or more expected responses have the same number of points, then the system asks the user to confh'm the correct one. While we have not evaluated this sub- string matching algorithm independently, a brief evaluation in the context of the system resulted in about 90% accuracy.
243	Explains Non-Technical Concepts	2.1 Morphological Analysis  2.1.1 Morphological characterist ics of Japanese  language  A Japanese sentence is composed of a string of  Bunsetsu, and each Bunsetsu is a string of morphemes. In  a Bunsetsu the relationship between the preceeding  morpheme and succeeding morpheme is strongly  regulated by grammar. The grammatical connectability  between morphemes can be easily determined by using a  grammatical table in morphological nalysis \[2\]. On the  other hand, on the morphological level there is little if any  grammatical restriction between the last morpheme in a  Bunsetsu and the first morpheme in the following  Bunsetsu. In this sence a compound word is also a series  of Bunsetsu, each of which contains an independent word.  There is no limit to the length of a compound word, and  there are no restrictions in the way words can be  combined. Therefore, since there are a tremendous  number of compound words, it is almost impossible to  compile a dictionary of these words.
994	Explains Non-Technical Concepts	No similarity between German and Chinese was  obtained by checking the nulnbcr of retraced  words in Chinese, because the majority of "the  retraced parts" in Chinese are word fragments.  But it is clearly shown in Table 2 that Gennan  words and Chinese characters play a similar role  in the production of speech repairs. Whether it  has to do with the syllabic weighting in both  languages or the semantic ontcnt of characters  in Chinese necds fnrther linguistic investigation.
619	Assumes Prior Knowledge	Abstract  This paper focuses on the Web-based  English-Chinese OOV term translation  pattern, and emphasizes particularly on  the translation selection strategy based  on the fusion of multiple features and  the ranking mechanism based on Rank- ing Support Vector Machine (Ranking  SVM). By utilizing the CoNLL2003  corpus for the English Named Entity  Recognition (NER) task and selected  new terms, the experiments based on  different data sources show the consis- tent results. Our OOV term translation  model can ?filter? the most possible  translation candidates with better abili- ty. From the experimental results for  combining our OOV term translation  model with English-Chinese Cross- Language Information Retrieval (CLIR)  on the data sets of Text Retrieval Eval- uation Conference (TREC), it can be  found that the obvious performance  improvement for both query translation  and retrieval can also be obtained.
200	Explains Technical Concepts	Like many systems, we do a form of word ex-  pansion in attempting to relate the query to the  document. However, the fact that we restrict ex-  pansion to proper nouns and verbs and their nom-  inalizations is notable. We found this limited set  of expansions restricts the relations between the  text and the query well and also fits within the  framework of part-whole relations in coreference.  We did not consider part-whole relations for com-  mon nouns, because in practice we have not had  very good results limiting over-generation i  that  domain.
876	Explains Non-Technical Concepts	1 Introduction contain strings of keywords. Typical queries are, as in most Web search applications, two to three words in length. At this point, all of the captions are in English. eMotion hosts a large database of images for sale and for licensing, PictureQuest. At least 10% of PictureQuest's user base is outside the United States. The tests were performed on the PictureQuest database of approximately 400,000 images.
107	Explains Technical Concepts	Karttunen \[1983\] and Karttlmen and Wittenburg \[1983\] have  some suggestions for what a proper syntactic component for  a morphological analyzer might contain. They mention using  context-free rules and some sort of feature-handling system as  possible extensions of both their and Koskenniemi's systems. In  short, it has been acknowledged that any such system really  ought to have some of the tools that have been used in syntax  proper.
343	Explains Non-Technical Concepts	1 Introduction  In recent years, opinion mining or sentiment  analysis (Liu, 2010; Pang and Lee, 2008) has  been an active research area in NLP. One task is  to extract people?s opinions expressed on  features of entities (Hu and Liu, 2004). For  example, the sentence, ?The picture of this  camera is amazing?, expresses a positive  opinion on the picture of the camera. ?picture? is the feature. How to extract features from a  corpus is an important problem. There are  several studies on feature extraction (e.g., Hu  and Liu, 2004, Popescu and Etzioni, 2005,  Kobayashi et al, 2007, Scaffidi et al, 2007,  Stoyanov and Cardie. 2008, Wong et al, 2008,  Qiu et al, 2009). However, this problem is far  from being solved.
803	Explains Technical Concepts	2.1. T ime-Synchronous  Search   In the time-synchronous Viterbi beam search, all the states  of the model are updated in lock step frame-by-frame as the speech is processed. The computation required for this  simple method is proportional to the number of states in the  model and the number of frames in the input. If we discard  any state whose score is far below the highest score in that  frame we can reduce the computation by a large factor.  There are two important advantages of a time-synchronous  search. First, it is necessary that the search be time-  synchronous in order for the computation to be finished at  the same time that the speech is finished. Second, since  all of the hypotheses are of exactly the same length, it is  possible to compare the scores of different hypotheses in or-  der to discard most hypotheses. This technique is called the  beam search. Even though the beam search is not theoreti-  cally admissible, it is very easy to make it arbitrarily close  to optimal simply by increasing the size of the beam. The  computational properties are fairly well-behaved with minor  differences in speech quality.
908	Explains Non-Technical Concepts	3.3 Parallel Text Al ignment Before the mined documents can be aligned into par- allel sentences, the raw texts have to undergo a se- ries of some preprocessing, which, to some extent, is language dependent. For example, the major opera- tions on the Chinese-English corpus include encod- ing scheme transformation (for Chinese), sentence level segmentation, parallel text alignment, Chinese word segmentation (Nie et al, 1999) and English expression extraction.
573	Explains Non-Technical Concepts	Although we have made dramatic progress there remains a  large gap between commercial applications and laboratory  systems. One problem is the large number of out of vocabu-  lary (OOV) words in real dictation applications. Even for a  20000-word ictation system, on average more than 25% of  the utterances in a test set contain OOV words. Even if we  exclude those utterance containing OOV words, the error rate  is still more than 9% for the 20000-word task due to the lim-  itations of current echnology. Other problems are illustrated  by the November 1992 DARPA stress test evaluation, where  testing data comprises both spontaneous speech with many  OOV words but also speech recorded using several different  microphones. Even though we augmented our system with  more than 20,000 utterances in the training set and a noise  normalization component \[1\], our augmented system only re-  duced the error rate of our 20000-word baseline result from  12.8% to 12.4%, and the error rate for the stress test was even  worse 'when compared with the baseline (18.0% vs. 12.4%).  To summarize, our current word error rates under different  testing conditions are listed in Table 1.
813	Explains Technical Concepts	3.2 Exper imenta l  Resu l ts   The features used in our experiment are listed in Ta-  bles 3 and 4. Each feature consists of a type and  a value. The features consist basically of some at-  tributes of the bunsetsu itself, and syntactic and con-  textual information. We call the features listed in  Tables 3 'basic features.' We selected them man-  ually so that they reflect the basic conditions gov-  erning word order that were sunmmrized by Saeki  (Saeki, 1998). The features in Table 4 are combina-  tions of basic features ('combined features') and were  also selected manually. They are represented by the  nmne of the target bunsetsu plus the feature type of  the basic features. The total number of features was  about 190,000, and 51,590 of them were observed in  the training cortms three or more times. These were  the ones we used in our experiment.
759	Explains Non-Technical Concepts	Towards the automatic detection of text genre,  various types of style markers (i.e., countable  linguistic features) have been proposed so far.  Karlgren and Cutting (1994) use a combination  of structural markers (e.g., noun count), lexical  markers (e.g., "it" count), and token-level  markers (e.g., words per sentence average,  type/token ratio, etc.). Kessler et al (1997)  avoid structural markers since they require  tagged or parsed text and replace them with  character-level markers (e.g., punctuation mark  counts) and derivative markers, i.e., ratios and  variation measures derived from measures of  lexical and character-level markers.  Furthermore, some interesting stylometric  approaches have been followed in authorship  attribution studies. Specifically, various  functions that attempt to represent the  vocabulary richness have been proposed  (Honore 1979; Sichel, 1975). The combination  of the best vocabulary richness functions in a  lnultivariate model can then be used tbr  capturing the characteristics of a stylistic  category (llohnes, 1992). However, recent  studies have shown that the m~tjority of these  liinctions depend heavily on text-length  (Tweedie and Baaycn, 1998). Additionally,  Stamatatos el al. (1999) attempted to take  advantage ot' ah'eady existing text processing  tools by proposing the analysis-level markers  taking into account the methodology of the  particular tool that has been used to analyze the  text. This approach requires the availability era   robust text processing tool and the time and/or  conaputational cost for the calculation of the  style markers is proportional to the  corresponding cost of the analysis of the text by  this tool.
581	Explains Technical Concepts	Introduction  Spontaneous speech analysis has recently been  playing a crucial role in providing empirical  evidence for applications in both theoretical nd  applied fields of computational linguistics. For  the purpose of constructing more salient and  robust dialogue systems, recent analyses on  speech repairs, or more generally speaking, on  speech disfluencies in spoken dialogues have  tried to explore the distributional characteristics  of irregular sequences in order to develop  annotation systems to cope with speech repairs  (Heeman and Allen 1999, Nakatani and  Hirschberg 1994). This new research direction,  nevertheless, has until recently merely focused  on the surface structure of speech repairs on the  one hand. On the other hand, except for very few  ilwestigations tarting to deal with speech  repairs across several languages (Eklund and  Shribcrg 1998), most of the studies on speech  repairs have investigated only single languages.  In addition, studies have shown that syntactic  and prosodic features of spontaneous speech  data provide empirical evidence with regard to  reflecting the speaking habits of speakers, and  also help to develop better parsing strategies and  natural language processing systems (Heeman  and Allen 1999, Hindle 1983). These systems  should understand and react o the language use  of human users (Lickley and Bard 1998, Tseng  1998).
49	Explains Technical Concepts	3 Mean ing  Representat ion   The meaning representation language is based on a case sys-  tem (Bruce 1975) inspired by Filhnore's notion of deep cases.  Basically, a text is represented by a list of propositions, each  consisting of a proposition type corresponding to a predicate  name, and a list of cases corresponding to the arguments of  the predicate. Contradictory to Fillmore's notion, proposition  types are not verbs, but abstract concepts defined in the case  frames of a case grammar. Furthermore, cases show seman-  tic relationships between proposition types and abstract con-  cepts. The case system (set of cases) is chosen in a somewhat  adhoc way. The cases, which are supposed to be necessary  in order to describe the domain of thermodynamic exercises,  are included. The cases and their use are explained below :  object object being affected by an action or event,  or being described.
718	Explains Technical Concepts	Apart from these five voting methods we have  also processed the output streams with two clas-  sifters: MBL and IG%'ee. This approach is  called classifier stacking. Like Van Halteren et  al. (1998), we have used diff'erent intmt ver-  sions: olle containing only the classitier Otltl)ut  and another containing both classifier outlmt  and a compressed representation of the data  item tamer consideration. \]?or the latter lmr-  pose we have used the part-of-speech tag of the  carrent word.
386	Explains Technical Concepts	3.1 Features  of  BusTUC For the Norwegian systems, the figures give an in- dication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc. There are 1300 grammar ules ( 810 for English) although alf of the rules are very low level. The semantic net described below contains about 4000 entries.
73	Mathematically-Oriented Paragraph	3.1 Preliminaries Conditional random fields (CRFs) (Lafferty et al, 2001) are undirected graphical models trained to maximize the conditional probability of the de- sired outputs given the corresponding inputs. Let G be a factor graph (Kschischang et al, 2001) defining a probability distribution over a set of output variables o conditioned on observation se- quences x. C = {?c(oc, xc)} is a set of factors in G, then the probability distribution over G can be written as P (o|x) = 1Z(x) ? c?C ?c(oc, xc) (2) where ?c is a potential function and Z(x) =? o ? c?C ?c(oc, xc) is a normalization factor. We assume the potentials factorize according to a set of features {fk(oc, xc)} as ?c(oc, xc) = exp( ? k ?kfk(oc, xc)) so that the family of dis- tributions is an exponential family. The model parameters are a set of real-valued weights ? = {?k}, one weight for each feature. Practical mod- els rely extensively on parameter tying to use the same parameters for several factors.
125	Explains Technical Concepts	Because of the desire to benchmark multiple algorithms  under several conditions in this evaluation combined with  limited resources and the severe time constraints imposed  by the evaluation protocol, this evaluation was performed  using a version of SPHINX-II that was slightly reduced in  performance, but that could process the test data more rap-  idly than the system described in \[12\]. Specifically, the  selection of phonetic models (across genders) was per-  formed by minimizing mean VQ distortion of the cepstral  vectors before recognition was attempted, rather than on the  basis of a posteriori probability after classification. In addi-  tion, neither the unified stochastic engine (USE) described  in \[12\] nor the cepstral mean normalization algorithms were  applied. Finally, the CDCN evaluations were conducted  without making use of the CART decision tree or alternate  pronunciations in the recognition dictionary. The effect of  these computational shortcuts was to increase the baseline  error rate for the 5000-word task from 6.9% as reported in  \[12\] to 8.1% for the MFCDCN evaluation, and to 8.4% for  the CDCN evaluation.
945	Assumes Prior Knowledge	2.4 Combinat ion techniques  At two points in our noun phrase recognition  process we will use system combination. We will  start with system-internal combination: apply  the same learning algorithm to variants of the  task and combine the results. The approach  we have chosen here is the same as in Tjong  Kim Sang (2000): generate different variants  of the task by using different representations  of the output (IOB1, IOB2, IOE1, IOE2 and  O+C). The five outputs will converted to the  open bracket representation (O) and the close  bracket; representation (C) and M'ter this, tile  most frequent of the five analyses of each word  will chosen (inajority voting, see below). We  expect the systems which use this combination  phase to perform better than their individuM  members (Tjong Kim Sang, 2000).
600	Explains Non-Technical Concepts	Since the identification of non-linguistic speech features  is based ,on phone recognition, some phone recognition re-  sults for the above corpora are given here. The speaker-  independent (SI) phone recognizers use sets of context-  dependent (CD) models which were automatically selected  based on their frequencies in the training data. There are  428 sex-dependent CD models for BREF, 1619 for WSJ and  459 for TIMIT. Phone errors rates are given in Table 1. For  BREF and WSJ phone errors are reported after removing  silences, whereas for TIMIT silences are included as tran-  scribed. Scoring without the sentence initial/final silence  increases the phone error by about 1.5%. The phone er-  ror for BREF is 21.3%, WSJ (Feb-92 5knvp) is 25.7% and  TIMIT (complete testset) is 27.6% scored using the 39 phone  set proposed by\[18\]. These results are provided to calibrate  the recognizers used in the experiments in this paper, and  observe differences in the corpora. It appears that the BREF  data is easiest to recognize at the phone level, and that TIMIT  is more difficult than WSJ.
362	Explains Technical Concepts	1. INTRODUCTION  Although both continuous peech recognition and key-  word-spotting tasks use the very similar underlying technology,  there are typically significant differences in the way in which the  technology is developed and used for the two applications (e.g.  acoustic model training, model topology and language modeling,  filler models, search, and scoring). A number of HMM-based  systems have previously been developed for keyword-spotting  \[1-5\]. One of the most significant differences between these key-  word-spotting systems and a CSR system is the type of non-key-  word model that is used. It is generally thought that very simple  non-keyword models (such as a single 10-state model \[2\], or the  set of monophone models \[1\]) can perform as well as more com-  plicated non-keyword models which include words or triphones.  We describe how we have applied CSR techniques to the  keyword-spotting task by using a speech recognition system to  generate a transcription of the incoming spontaneous speech  which is searched for the keywords. For this task we have used  SR.I's DECIPI-IER TM system, a state-of-the-art large-vocabulary  speaker-independent continuous-speech recognition system \[6-  10\]. The method is evaluated on two domains: (1) the Air Travel  Information System (ATIS) domain \[13\], and (2) the "credit card  topic" subset of the Switchboard Corpus \[11\], a telephone speech  corpus consisting of spontaneous conversation on a number of  different opics.
597	Explains Non-Technical Concepts	2 MINDS - Mul t i -L ingual  Interact ive  Document  Summar izat ion   2.1 Background  The need for summarization tools is especially  strong if the source text is in a language different  from the one(s) in which the reader is most fluent.  Interactive summarization of multilingual docu-  ments is a very promising approach to improving  productivity and reducing costs in large-scale doc-  ument processing. This addresses the scenario  where an analyst is trying to filter through a large  set of documents to decide quickly which docu-  ments deserve further processing. This task is more  difficult and expensive when the documents are in  a foreign language in which the analyst may not be  as fluent as he or she is in English. The task is even  more difficult when the documents are in several  different languages. For example, the analyst's task  may be to filter through newspaper a ticles in many  different languages published on a particular day to  generate a report on different nations' reactions to a  current international event, such as a nuclear test  on the previous day. This last task is currently  infeasible for a single analyst, unless he or she  understands each one of those languages, since  machine translation (MT) of entire documents can-  not yet meet he requirements of such a task. Multi-  lingual summarization (MLS) introduces the  possibility of translating a summary rather than the  entire document to the language of the summary  (i.e., English). We hope that MLS and MT can  mutually benefit from one another since summari-  zation offers MT the benefit of not having to trans-  late entire texts and also spares a user from having  to read through an entire document produced by an  MT system.
267	Explains Technical Concepts	Comparison of the first three experimental  results and the results reported at NEWS 2009  shows that we achieve comparable perfor- mance with their best-reported systems at the  same conditions of using single system and  orthographic features only. This indicates that  our baseline represents the state-of-the-art per- formance. In addition, we find that the back- transliteration (line 4-6) consistently performs  worse than its corresponding forward- transliteration (line 1-3). This observation is  consistent with what reported at previous work  (Li et al, 2004; Zhang et al, 2004). The main  reason is because English has much more  transliteration units than foreign C/J/K lan- guages. This makes the transliteration from  English to C/J/K a many-to-few mapping issue  and back-transliteration a few-to-many map- ping issue. Therefore back-transliteration has  more ambiguities and thus is more difficult.  Overall, the lower six experiments (line 7- 12) shows worse performance than the upper  six experiments which has English involved.  This is mainly due to the less available training  data for the language pairs without English  involved. This observation motivates our study  using pivot language for machine translitera- tion.
922	Assumes Prior Knowledge	The use of a constrained segmentation greatly simpli-  fies parameter estimation in the tied mixture case, since  there is only one unobserved component, the mixture  mode. In this case, the parameter estimation step of the  iterative segmentation/estimation algorithm involves the  standard iterative expectation-maximization (EM) ap-  proach to estimating the parameters of a mixture distri-  bution \[17\]. In contrast, the full EM algorithm for tied  mixtures in an HMM handles both the unobserved state  in the Markov chain and the unobserved mixture mode  \[21.
599	Assumes Prior Knowledge	DP ratings were significantly correlated with the  ratings provided by the two human raters who  agreed well (_r = 0.32 and 0.31), resulting in  agreement of ratings in 63% and 66% of the  questions. In other words, the agreement of  ratings provided by the system and by two human  raters is comparable to the highest agreement rate  achieved between the human raters.  Some of the human ratings diverged  substantially. Therefore, we computed two  restrictive measures based on the ratings to  evaluate the performance of DP. Both scores are  Boolean. The first score is "lenient"; it reports a  presupposition only if at least two raters report a  presupposition for the question (rating of 3 or 4).  We call this measure P~j, a majority-based  presupposition count. The second score is strict.  It reports a presupposition only if all three raters  report a presupposition. This measure is called  Pcomp, a presupposition count based on complete  agreement. It results in fewer detected  presuppositions overall: Pcomp reports  presuppositions for 29 of the questions (33%),  whereas P~j reports 57 (64%).
405	Explains Non-Technical Concepts	In short, without a more sophisticated control strategy, the  grammar would contain a fair amount of redundancy if one really  attempted to handle English morphology in its entirety. How-  ever, on a more positive note, the rules do allow one to deal  effectively and elegantly with a sufficient range of phenomena to  make it quite acceptable as, for instance, an interface between a parser and its lexicon.
570	Explains Non-Technical Concepts	1 Introduction  Summarization is the problem of presenting the  most important information contained in one or  more documents. The research described here  focuses on multi-lingual summarization (MLS).  Summaries of documents are produced in Spanish,  Japanese, English and Russian using the same  basic summarization engine.
957	Explains Technical Concepts	5 Word Lattice Input We generalized the bottom-up parsing algorithm that generates the translation hypergraph so that it supports translation of word lattices instead of just sentences. Our implementation?s runtime and memory overhead is proportional to the size of the lattice, rather than the number of paths in the lat- tice (Dyer et al, 2008). Accepting lattice-based input allows the decoder to explore a distribution over input sentences, allowing it to select the best translation from among all of them. This is es- pecially useful when Joshua is used to translate the output of statistical preprocessing components, such as speech recognizers or Chinese word seg- menters, which can encode their alternative analy- ses as confusion networks or lattices.
489	Assumes Prior Knowledge	2. Translation and localization 2.1 A pivot language Localization of products and their documentation is a great problem for any company, which wants to strengthen its position on foreign language market, especially for companies producing various kinds of  software. The amounts of texts being localized are huge and the localization costs are huge as well.
233	Explains Technical Concepts	The use of proper names allows the summaries  to be weighted towards sections of the documents  discussing specific individuals or organizations  rather than more general topics. In terms of pro-  duction of informative summaries, rather than  indicative summaries, this may be an important  capability. This technique was used to produce  summaries evaluated using a "question and  answer" methodology at the Tipster evaluation and  produced ahigh performance here.
487	Explains Non-Technical Concepts	1 In t roduct ion Parallel texts have been used in a number of studies in computational linguistics. Brown et al (1993) defined a series of probabilistic translation models for MT purposes. While people may question the effectiveness of using these models for a full-blown MT system, the models are certainly valuable for de- veloping translation assistance tools. For example, we can use such a translation model to help com- plete target ext being drafted by a human transla- tor (Langlais et al, 2000).
197	Explains Technical Concepts	4 Semiring Parsing In Joshua, we use a hypergraph (or packed forest) to compactly represent the exponentially many derivation trees generated by the decoder for an input sentence. Given a hypergraph, we may per- form many atomic inference operations, such as finding one-best or k-best translations, or com- puting expectations over the hypergraph. For each such operation, we could implement a ded- icated dynamic programming algorithm. How- ever, a more general framework to specify these algorithms is semiring-weighted parsing (Good- man, 1999). We have implemented the in- side algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eis- ner (2009), plut the first-order expectation semir- ing (Eisner, 2002) and its second-order version (Li and Eisner, 2009). All of these use our newly im- plemented semiring framework.
147	Explains Technical Concepts	Moreover, it has been shown that the  fi'equencies of occurrence of the most fi'equent  punctuation marks can considerably enhance the  peM'ormance of the proposed model and increase  the reliability of the classification results  especially when training data of limited size are  available.
231	Explains Technical Concepts	6.3 Metrics A popular metric used in sequence evaluation  is Kendall?s ? (Lapata, 2006), which measures  ordering differences in terms of the number of  adjacent sentence inversions necessary to  convert a test ordering to the reference ordering. ? = 4m/(n(n ? 1))             (Eq. 16) where m is the number of inversions described  above and n is the total number of sentences. The second metric we use is the Average  Continuity (AC) developed by Bollegala et al (2006), which captures the intuition that the  ordering quality can be estimated by the number  of correctly arranged continuous sentences. ?? = exp( ? ??? ? log( ?? + ?)????                (Eq. 17) where k is the maximum number of continuous  sentences, ? is a small value in case Pn = 1. Pn, the proportion of continuous sentences of length  n in an ordering, is defined as m/(N ? n + 1)  where m is the number of continuous sentences  of length n in both the test and reference  orderings and N is the total number of sentences.  We set k = 4 and ? = 0.01.
611	Assumes Prior Knowledge	 1.2 Classifying presuppositions  Different types of presuppositions can be  distinguished based on particular indicators.  Examples for presupposition types, such as  events or possessions, were mentioned above.  Table 2 presents an exhaustive overview of  presupposition types identified in our analysis.  Note that some indicators can point to more than  one type of presupposition.  Table 2 : Classification of presupposition based on  indicators. In the right column, expressions in  parentheses identify the presupposed unit.
37	Mathematically-Oriented Paragraph	2.2. Cepstral Filtering Techniques  In this :section we describe two extremely simple tech-  niques, RASTA and cepstral mean normalization, which  can achieve a considerable amount of environmental  robusmess atalmost negligible cost.  RASTA. In RASTA filtering \[10\], a high-pass filter is  applied to a log-spectral representation f speech such as  the cepstral coefficients. The SRI DECIPHER TM system,  for example, uses the highpass filter described by the differ-  ence equation  y\[n\] = x\[n\] -x \ [n -  1\] +0.97y\ [n-  1\]  where x \[n\] and y \[n\] are the time-varying cepstral vectors  of the utterance before and after RASTA filtering, and the  index n refers to the analysis frames \[11\].  Cepstral mean normalization. Cepstral mean normaliza-  tion (CMN) is an alternate way to high-pass filter cepstral  coefficients. In cepstral mean normalization the mean of the  cepstral vectors is subtracted from the cepstral coefficients  of that utterance on a sentence-by-sentence basis:  y \[n\]  N  1  = x In\] - ~ 2 . ,  x \[n\]  1  n=l   where N is the total number frames in an utterance.  Figure 2 shows the low-frequency portions of the transfer  functions of the RASTA and CMN filters. Both curves  exhibit a deep notch at zero frequency. The shape of the  CMN curve depends on the duration of the utterance, and is  plotted in Figure 2 for the average duration in the DARPA  Wall Street Journal task, 7 seconds. The Nyquist frequency  for the time-varying cepstral vectors is 50 frames per sec-  ond.
678	Explains Technical Concepts	4.2 Phrase  Leve l  Ev idence   Investigators in IR are aware of the simplistic and  inadequate representation of document content based  on a bag of single word stems or some 2-word  adjacency phrases. To a certain extent his is dictated  by the requirements hat text retrieval systems have to  support 'large scale environments as well as  unpredictable, diverse needs. Many previous attempts,  including Tipster contractors (e.g. \[5\]), have been  made to include more sophisticated phrasal  representation i order to improve retrieval results.  They have not worked as well as content terms or  generally been inconclusive.
769	Assumes Prior Knowledge	 Although our processing is limited to the  verb metaphor examples and hasn?t considered  other instances like ?tasty tidbits of informa- tion?, it points out promising directions for fi- gurative language processing. After our inten- tion to improve the performance of affect sens- ing from individual turn-taking input, we focus  on improvement of the performance using con- text profiles. In future work, we intend to use a  metaphor ontology to recognize metaphors.   4 Affect Sensing from Context Profiles  Our previous affect detection (Zhang et al  2008a) has been performed solely based on in- dividual turn-taking input. Thus the context in- formation has been ignored. However, the con- textual and character profiles may influence the  affect implied in the current input. In this sec- tion, we will discuss relationships between cha- racters, linguistic contextual indicators, cogni- tive emotion simulation from a communication  context and our approach developed based on  these features to interpret affect from context.
955	Assumes Prior Knowledge	3 Resul ts   We want to find out whether system combi-  nation could improve performmlce of baseNP  recognition and, if this is the fact, we want to  seJect the best confl)ination technique. For this  lmrpose we have pertbrmed an experiment with  sections 15-18 of the WSJ part of the Prom %'ee-  bank as training data (211727 tokens) and sec-  tion 21 as test data (40039 tokens). Like the  data used by Ramshaw and Marcus (1995), this  data was retagged by the Brill tagger in order  to obtain realistic part-of  speech (POS) tags 5.  The data was seglnente.d into baseNP parts and  non-lmseNP t)arts ill a similar fitshion as the  data used 1)y Ramshaw and Marcus (1995). Of  the training data, only 90% was used for train-  ing. The remaining 10% was used as laming  data for determining the weights of the combi-  nation techniques.
52	Explains Technical Concepts	A test criterion for the understanding capacity is that after a set of definitions in a Naturally Read- able Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent. Every man that lives loves Mary. John is a man. John lives. Who loves Mary? ==> John NRL is defined in a closed context. Thus in- terfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
710	Explains Non-Technical Concepts	Abstract  This paper presents two pivot strategies  for statistical machine transliteration,  namely system-based pivot strategy  and model-based pivot strategy. Given  two independent source-pivot and pi- vot-target name pair corpora, the mod- el-based strategy learns a direct source- target transliteration model while the  system-based strategy learns a source- pivot model and a pivot-target model,  respectively. Experimental results on  benchmark data show that the system- based pivot strategy is effective in re- ducing the high resource requirement  of training corpus for low-density lan- guage pairs while the model-based pi- vot strategy performs worse than the  system-based one.
606	Assumes Prior Knowledge	The success of the system CESILKO has encouraged the investigation of the possibility to use the same method for other pairs of Slavic languages, namely for Czech-to-Polish translation. Although these languages are not so similar as Czech and Slovak, we hope that an addition of a simple partial noun phrase parsing might provide results with the quality comparable to the full- fledged syntactic analysis based system RUSLAN (this is of course true also for the Czechoto-Slovak translation). The first results of Czech-to Polish translation are quite encouraging in this respect, even though we could not perform as rigorous testing as we did for Slovak.
309	Explains Technical Concepts	4. The Word Grammar  The internal structure of words is handled by a  unification feature grammar with rules of the form:  mother -~ daughter 1 daughter 2 ...  where 'mother', 'daughtcrl', etc. are categories. A rule  which adds the plural morpheme to a noun might be  given as shown below:  ((BAR 0) (V -) (N +) (PLU +) (INFL -)) =>  ((BAR 0) (V -) (N +) (INFL +))  ((BAR -1) (V -) (N 4-) (PLU 4-) (INFL -))  The system provides two methods of writing rules in a  more general form; variables and feature-passing conven-  tions.  In our grammar, the category and inflectabllity of a  suffixed word are determined by the category and  lnflectablllty of the suffix; in the rule below, ALPHA,  BETA, and GAMMA are variables ranging over the set  of values {+, -}:  ((V ALPHA)(N BETA)(INFL GAMMA)(BAR 0)) =>  ((BAR 0))  ((V ALPHA)(N BETA)(INFL GAMMA)(BAR -1))  Since variables are interpreted consistently throughout a  rule, the mother category and suffix will be identical In  their specifications for N, V and INFL.    The Word-Daughter Convention:  (a) If any WDaughter features exist on the Right-  daughter then the WDaughter features on the  Mother are the unification of the pre-lnstantlaUon  WDaughter features on the Mother with the pre-  lnstantlatlon WDaughter featm-es on the Right-.  daughter.  (b) If no WDaughter features exist on the Right-  daughter then the WDaughter features on the  Mother are the unification of the pre-lnstantiatlon  WDaughter features on the Mother with the pre-  lnstantlation WDaughter features on the Left-  daughter.
717	Explains Technical Concepts	The Viterbi approximation is efficient, but it ig- nores most of the derivations in the hypergraph. We implemented variational decoding (Li et al, 2009b), which works as follows. First, given a for- eign string (or lattice), the MT system produces a hypergraph, which encodes a probability distribu- tion p over possible output strings and their deriva- tions. Second, a distribution q is selected that ap- proximates p as well as possible but comes from a family of distributions Q in which inference is tractable. Third, the best string according to q (instead of p) is found. In our implementation, the q distribution is parameterized by an n-gram model, under which the second and third steps can be performed efficiently and exactly via dynamic programming. In this way, variational decoding considers all derivations in the hypergraph but still allows tractable decoding.
613	Explains Technical Concepts	As an example from the current analysis of how the  system can operate to produce well-formed words, con-  sider the familiar fact of English morphology that no  word may contain more than one imqection. The word  grammar must permit both walked and walking, but not  walkinged. This is achiev~xi by restricting the distribu-  tion of inflectional suffixes so that they attach to non-  Inflected stems only. A general statement of this type  of restriction is made in terms of a feature INFL: stems  specified as (INFL +) may take an lnflecUonal sulfix,  while those specified as (INFL ~) may not. The STEM  feature described in section 4 provides one means of  enforcing correct stem-affix combinations; if the suffixes  ed and ing are specified with (STEM ((INFL +))), they  will attach only to categories which Include the  specification (INFL +). Walk, as a regular verb, is so  specified; wallced and waltcing are therefore accepted. Ed,  ing, other tnfectlonal suffixes, and irregular (i.e.  unlnflectable) words, however, are specified as (INFL -).  Our grammar assigns a binary structure to the words in  question. In order for this method to prevent e.g. walk-  inged, the stem walking must also bear the (INFL -)  specification. This it does, since we regard sutfixes as  being the head of a word, and as contributing to the  categorial content of the word as a whole. If the INFL  specification of the suf~x is copied into the mother  category, the STEM specification of a further suffix will  not be satisfied. See section 4 for more discussion of  these matters.
131	Explains Technical Concepts	The main bilingual dictionary contains data necessary for the translation of  both lemmas and tags. The translation of tags (from the Czech into the Slovak morphological system) is necessary, because due to the morphological differences both systems use close, but slightly different tagsets. Currently the system handles the 1:1 translation of tags (and 2:2, 3:3, etc.). Different ratio of translation is very rare between Czech and Siovak, but nevertheless an advanced system of dictionary items is under construction (for the translation 1:2, 2:1 etc.). It is quite interesting that the lexically homonymous words often preserve their homonymy even after the translation, so no special treatment of homonyms is deemed necessary.
701	Mathematically-Oriented Paragraph	While the focus has been on improving the performance  of the speech recognizers, it is also of interest o be able  to identify what we refer to as some of the "non-linguistic"  speech features present in the acoustic signal. For example,  it is possible to envision applications where the spoken query  is to be recognized without prior knowledge of the language  being spoken. This is the case for information centers in  public places, such as train stations and airports, where the  language may change from one user to the next. The ability  to automatically identify the language being spoken, and to  respond appropriately, is possible.
537	Assumes Prior Knowledge	1 Introduction  Many technical terms and proper names, such  as personal, location and organization names,  are translated from one language into another  language with approximate phonetic equiva- lents. This phonetic translation using computer  is referred to as machine transliteration. With  the rapid growth of the Internet data and the  dramatic changes in the user demographics  especially among the non-English speaking  parts of the world, machine transliteration play  a crucial role in  most multilingual NLP, MT  and CLIR applications (Hermjakob et al,  2008; Mandl and Womser-Hacker, 2004). This  is because proper names account for the major- ity of OOV issues and translation lexicons  (even derived from large parallel corpora)  usually fail to provide good coverage over di- verse, dynamically increasing names across  languages.
95	Explains Non-Technical Concepts	Abstract  In this paper we present a method for  detecting the text genre quickly and easily  following an approach originally proposed  in authorship attribution studies which uses  as style markers the frequencies of  occurrence of the most frequent words in a  training corpus (Burrows, 1992). In contrast  to this approach we use the frequencies of  occurrence of the most frequent words of the  entire written language. Using as testing  ground a part of the Wall Street Journal  corpus, we show that the most frequent  words of the British National Corpus,  representing the most frequent words of the  written English language, are more reliable  discriminators oftext genre in comparison to  the most frequent words of the training  corpus. Moreover, the fi'equencies of  occurrence of the most common punctuation  marks play an important role in terms of  accurate text categorization aswell as when  dealing with training data of limited size.
809	Explains Technical Concepts	However, only certain translation directions are cov- ered by ECMT, and its maintenance is quite compli- cated and requires quite a lot of dedicated and special- ized human resources. In the light of these facts and with the addition of the languages of (prospective) new member states, statistical approaches to machine trans- lation seem to offer a viable alternative. First of all, SMT is data-driven, i.e. it exploits par- allel corpora of which there are plenty at the EU in- stitutions in the form of translation memories. Trans- lation memories have two main advantages over other parallel corpora. First of all, they contain almost ex- clusively perfectly aligned segments, as each segment is stored together with its translation, and secondly, they contain cleaner data since their content is regu- larly maintained by linguists and database administra- tors. SMT systems are quicker to develop and easier to maintain than rule-based systems. The availability of free, open-source software like Moses2 (Koehn et al., 2007), GIZA++3 (Och and Ney, 2003) and the like constitutes a further argument in their favor. Early experiments with Moses were started by mem- bers of DGT?s Portuguese Language Department as early as summer 2008 (Leal Fontes and Machado, 2009), then turned into a wider interinstitutional project with the codename Exodus, currently combining re- sources from European Commission?s DGT and Euro- pean Parliament?s DGTRAD. Exodus is the first joint project of the interinstitutional Language Technology Watch group where a number of EU institutions join forces in the field of language technology.
82	Explains Technical Concepts	1 In t roduct ion It has been known for quite some time now, that the language used when interacting with a comput- er is different from the one used in dialogues between people, (c.f. JSnsson and Dahlb~ick (1988)). Given that we know that the language will be different, but not how it will be different, we need to base our development of natural language dialogue sys- tems on a relevant set of dialogue corpora. It is our belief that we need to clarify a number of different issues regarding the collection and use of corpora in the development of speech-only and multimodal dia- logue systems. Exchanging experiences and develop- ing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the develop- ment of computational models of speech, language, and dialogue/discourse. It is interesting to note the difference in the state of art in the field of natu- ral language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representative- ness in corpus design and other have been discussed for quite some time (e.g. (Garside t al., 1997; Atkins et al, 1992; Crowdy, 1993; Biber, 1993)). Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further.
889	Explains Technical Concepts	Fully stochastic language models (e.g. Nagata  1994), on the other hand, do not allow such manual  cost manipulation and precisely for that reason,  improvements in segmentation accuracy are harder  to achieve. Attaining a high accuracy using fully  stochastic methods is particularly difficult for  Japanese due to the prevalence of orthographic  variants (a word can be spelled in many different  ways by combining different character sets), which  exacerbates the sparse data problem. As a result,  the performance of stochastic models is usually not  as good as the heuristics-based language models.  The best accuracy reported for statistical methods  to date is around 95% (e.g. Nagata 1994).
659	Explains Technical Concepts	Words that have prominence in a phrase or  utterance m:e accented (sentence level stress).  Unlike lexical stress which is usually fixed, sen-  tence level stress is variable. When a word  carries sentence level stress, a pitch accent is  associated with the syllable of primary stress.  Pitch accents are denoted by *. The most com-  mon pitch accent is an H*, which is usually  realised as a pitch peak near tim vowel in the  primary stressed syllable, it is also possible to  have pitch accents which are a combination of a  pitch movement towards and including a peak  or trough. One sudl bitonal accent is L+H*,  which moves from a low in pitch towards a high.  Intermediate and intonational phrases carry  edge tones. Intermediate phrases carry phrase  tones, indicated by - .  The phrase tone L- is low  pitch following the final pitch accent of a phrase.
910	Assumes Prior Knowledge	DP currently works like the other modules of  QUA\]D: it reports potential problems, but leaves  it to the survey methodologist to decide whether  to act upon the feedback. As such, DP is a  substantial addition to QUA\]D. A future  challenge is to turn DP into a DIP (detector of  incorrect presuppositions), that is, to reduce the  number of reported presuppositions to those  likely to be incorrect. DP currently evaluates all  questions independent of context, resulting in  frequent detections. For example, 20 questions  about "this person" may follow one question that  establishes the referent. High-frequency  repetitive presupposition reports could easily get  annoying.
144	Explains Non-Technical Concepts	4 Fur ther  Work   There are still some things that are not as straightforward asone  would like. In particular, consider the following example. Let  us suppose as a first approximation that one wanted to analyze  the \[un\] prefix in English as combining with adjectives to yield  new ones, e.g., unfair, unclear, unsafe. Suppose also that one  wanted to be able to build past participles of transitive verbs  (passives) into adjectives, so that they could combine with \[tin\],  a.~ in uncovered, unbuilt, unseen.
1001	Explains Technical Concepts	4 Conclusions The TUC approach as as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming. The implemented system has proved its worth, and is interesting if for no other reason. There is also an increasing interest from other bus compa- nies and route information companies alike to get a similar system for their customers. Further work remains to make the parser really efficient, and much work remains to make the lan- guage coverage complete within reasonable imits. It is an open question whether the system of this kind will be a preferred way of offering information to the public. If it is, it is a fair amount of work to make it a portable system that can be implemented lsewhere, also connecting various travelling agencies. If not, it will remain a curiosity. But anyway, a system like this will be a contribution to the devel- opment of intelligent systems.
835	Explains Technical Concepts	2.2 Use of Automatic Translations and Comparable corpora Available human translated bitexts such as the UN corpus seem to be out-of domain for this task. We used two types of automatically extracted re- sources to adapt our system to the task domain. First, we generated automatic translations of the French News corpus provided (231M words), and selected the sentences with a normalised transla- tion cost (returned by the decoder) inferior to a threshold. The resulting bitext has no new words in the English side, since all words of the transla- tion output come from the translation model, but it contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008).
549	Assumes Prior Knowledge	 3 Multi-Document  Summarization   3.1 Spreading activation  A set of GDA-tagged documents is regarded as  a network in which nodes roughly correspond to  GI)A elements and links represent he syntac-  tic and semantic relations among them. This  network is the tree of GI)A elements plus cross-  reference (via eq, eq.ab,  sub, sup, and so on)  links among them. Cross-reference \]inks nlay  encompass different documents. Figure 2 shows  a schematic, graphical representation f the net-  work.
4	Explains Non-Technical Concepts	Analysis of the Data  There were 35 different John Smiths mentioned in  the articles. Of these, 24 of them only had one ar-  ticle which mentioned them. The other 173 articles  were regarding the 11 remaining John Smiths. The  background of these John Smiths , and the num-  ber of articles pertaining to each, varied greatly.  Descriptions of a few of the John Smiths are:  Chairman and CEO of General Motors, assistant  track coach at UCLA, the legendary explorer, and  the main character in Disney's Pocahontas, former  president of the Labor Party of Britain.
575	Explains Technical Concepts	3.2 Statistical Translation Model Many approaches in computational linguistics try to extract ranslation knowledge from previous trans- lation examples. Most work of this kind establishes probabilistic models from parallel corpora. Based on one of the statistical models proposed by Brown et al (1993), the basic principle of our translation model is the following: given a corpus of aligned sen- tences, if two words often co-occur in the source and target sentences, there is a good likelihood that they are translations of each other. In the simplest case (model 1), the model earns the probability, p(tls), of having a word t in the translation of a sentence con- taining a word s. For an input sentence, the model then calculates a sequence of words that are most probable to appear in its translation. Using a sim- ilar statistical model, Wu (1995) extracted a large- scale English-Chinese l xicon from the HKUST cor-
1000	Assumes Prior Knowledge	2.2 Agreement among the raters  We evaluated the agreement among the raters with  three measures: correlations, Cohen's kappa, and  percent agreement. Correlations were significant  only between two raters (r = 0. 41); the  correlations of these two with the third rater  produced non-significant correlations, indicating  that the third rater may have used a different  strategy. The kappa scores, similarly, were  significant only for two raters (_k_ = 0.36). In terms  of percent agreement, he raters with correlated  ratings agreed in 67% of the cases. The  percentages of agreement with rater 3 were 57%  and 56%, respectively.
143	Explains Technical Concepts	The results of MMR reranking are shown in Table  1. In this Reuters document collection, article 1403 is  a duplicate of 1388. MMR reranking performs as  expected, for decreasing values of 1, the ranking of  1403 drops. Also as predicted, novel but still  relevant information as evidenced by document 69  starts to increase in ranking. Relevant, but similar to  the highest ranked documents, such as document  1713 drop in ranked ordering. Document 2149 's  position varies depending on its similarity to  previously seen information.
271	Explains Technical Concepts	Each utterance is decoded with each gender-dependent model.  For each utterance, the N-best list with the highest op-1 hypoth-  esis score is chosen. The top choice in the final list constitutes the  speech recognition results reported below. This N-best strategy  \[3, 4\] permits the use of otherwise computationally prohibitive  models by greatly reducing the search space to a few (N=20-100)  word sequences. It has enabled us to use cross-word-bonndary  triphone models and trigram language models with ease.  During most of the development of the system we used the  1000-Word RM cospus \[8\] for testing. More recently, the system  has been used for recognizing spontaneous speech from the ATIS  corpus, which contains many spontaneous speech effects, such as  partial words, nonspeech sounds, extraneous.noises, false starts,  etc. The vocabulary of the ATIS domain was about twice that  of the RM corpus. So there were no significant new problems  having to do with memory and computation.
881	Explains Technical Concepts	The allowed-type rules in tile top set are those that license  consonant doubling. The disallowed-type rules in the second set  constrain the doubling so it does not occur in words like \[eat+ing\]  ?:==> \[eating\] and \[hear+ing\] ?====~ \[hearing I. The disallowed-type  rulcs say that a morpheme boundary \[+\] may not ever correspond  to a consonant when tile \[+\] is followed by a vowel and preceded  by that same consonant and then two more vowels.
727	Explains Technical Concepts	Several other factors make the PictureQuest application a particularly good application for machine translation technology. Unlike document ranslation, there is no need to match every word in the description; useful images may be retrieved even if a word or two is lost. There are no discourse issues at all: searches never use anaphora, and no one cares if the translated query sounds good or not.
647	Explains Non-Technical Concepts	1 Introduction  Although it is said tha~ word order is free in  Japanese, linguistic research shows that there art  certain word order tendencies - -  adverbs of time, for  example, tend to t)recede subjects, mM bunsetsus in  a sentence that are modified by a long modifier tend  to precede other bunsetsus in the sentence. Knowl-  edge of these word order tendencies would be useful  in analyzing and generating sentences.
486	Assumes Prior Knowledge	In this way, by considering the linguistic con- textual indicators, the potential emotional con- text one character was in, relationships with  others and recent emotional profiles of other  characters, our affect detection component has  been able to inference emotion based on context  to mark up the rest of the above test example  transcript (e.g. Mayid being ?angry? for the 10th  input). However our processing could be fooled  easily by various diverse ways for affective ex- pressions and creative improvisation (test emo- tional patterns not shown in the training and  generated sets). We intend to adopt better emo- tion simulation tools, more linguistic hints, psy- chological (context-based) emotional theories  for further improvements. Also, our processing  currently only focused on the school bullying  scenario. We are on our way to extend the con- text-based affect sensing to the Crohn?s disease  scenario to further evaluate its efficiency.
744	Explains Non-Technical Concepts	3.2.3 Frequency  It is obvious that the iterative occurrences of  words must be higher than those of non-word  strings. String frequency is also useful  information for our task. Because the string  frequency depends on the size of corpus, we  normalize the count of occurrences by dividing by  the size of corpus and multiplying by the average  value of Thai word length:
617	Explains Non-Technical Concepts	In fact, Smeaton and Quigley (1996) conclude that WordNet is indeed helpful in image retrieval, in particular because image captions are too short for statistical analysis to be useful. This insight is what led us to develop a proprietary image retrieval engine in the first place: fine-grained linguistic analysis is more useful that a statistical approach in a caption averaging some thirty words. (Our typical captions are longer than those reported in Smeaton and Quigley 1996).
951	Explains Technical Concepts	1. INTRODUCTION  At Carnegie Mellon, we have made significant progress  in large-vocabulary speaker-independent continuous peech  recognition during the past years \[16, 15, 3, 18, 14\]. In com-  parison with the SPHINX system \[23\], SPHINX-II offers not  only significantly fewer recognition errors but also the capa-  bility to handle amuch larger vocabulary size. For 5,000-word  speaker-independent speech recognition, the recognition error  rate has been reduced to 5%. This system achieved the lowest  error rate among all of the systems tested in the November  1992 DARPA evaluations, where the testing set has 330 utter-  ances collected from 8 new speakers. Currently we are refin-  ing and extending these and related technologies to develop  practical unlimited-vocabulary dictation systems, and spoken  language systems for general application domains with larger  vocabularies and reduced linguistic onstraints.    2. FEATURE EXTRACTION  The extraction of reliable features is one of the most impor-  tant issues in speech recognition and as a result the training  data plays a key role in this research. However the curse of  dimensionality reminds us that the amount of training data  will always be limited. Therefore incorporation of additional  features may not lead to any measurable error reduction. This  does not necessarily mean that the additional features are  poor ones, but rather that we may have insufficient data to  reliably model those features. Many systems that incorporate  environmentally-robust \[1\] and speaker-robust \[11\] models  face similar constraints.
13	Explains Technical Concepts	3.5. 3-Way Gender Selection  It has become a standard technique to model the speech of male  and female speakers eparately, since the speech of males and  females is so different. This typically results in a 10% reduction  in error relative to using a single speaker-independent model.  However, we have found that there are occassional speakers who  do not match one model much better than the other. In fact,  there are some very rare sentences in which the model of the  wrong gender is chosen. Therefore we experimented with using  a third "gender" model that is the simple gender-independent  model, derived by averaging the male and the female models.  During recognition, we find the answer independently using each  of these models and then we choose the answer that has the  highest overall score. We find that about one out of 10 speakers  will typically score better using the gender-independent model  than the model for the correct gender. In addition, with this third  model, we no longer ever see sentences that are misclassitied as  belonging to the wrong gender. The reduction error associated  with using a third gender model was 0.4%.
706	Explains Technical Concepts	To be able to integrate the approach easily into a standard phrase-based SMT system, a token in the bilingual language model is defined to consist of a target word and all source words it is aligned to. The tokens are ordered according to the target lan- guage word order. Then the additional tokens can be introduced into the decoder as an additional tar- get factor. Consequently, no additional implemen- tation work is needed to integrate this feature. If we have the German sentence Ich bin nach Hause gegangen with the English translation I went home, the resulting bilingual text would look like this: I Ich went bin gegangen home Hause. As shown in the example, one problem with this approach is that unaligned source words are ig- nored in the model. One solution could be to have a second bilingual text ordered according to the source side. But since the target sentence and not the source sentence is generated from left to right during decoding, the integration of a source side language model is more complex. Therefore, as a first approach we only used a language model based on the target word order.
684	Explains Non-Technical Concepts	We used the summarizer, via the QET inter-  face, to build effective search queries for an informa-  tion retrieval system. This has been demonstrated  to produce dramatic performance improvements in  TREC evaluations. We believe that this query ex-  pansion approach will also prove useful in searching  very large databases where obtaining a full index  may be impractical or impossible, and accurate sam-  pling will become critical.
890	Explains Non-Technical Concepts	The evaluation of results of RUSLAN showed that roughly 40% of input sentences were translated correctly, about 40% with minor errors correctable by a human post-editor and about 20% of the input required substantial editing or re-translation. There were two main factors that caused a deterioration of the translation. The first factor was the incompleteness of the main dictionary of the system. Even though the system contained a set of so-called fail-soft rules, whose task was to handle such situations, an unknown word typically caused a failure of the module of syntactic analysis, because the dictionary entries contained - besides the translation equivalents and morphological information - very important syntactic information.
568	Explains Non-Technical Concepts	This paper deals with search algorithms for real-time speech recog-  nition. We argue that software-only speech recognition has several  critical advantages over using special or parallel hardware. We  present a history of several advances in search algorithms, which  together, have made it possible to implement real-time recogni-  tion of large vocabularies on a single workstation without he need  for any hardware accelerators. We discuss the Forward-Backward  Search algorithm in detail, as this is the key algorithm that has  made possible recognition of very large vocabularies in real-time.  The result is that we can recognize continuous speech with a vocab-  ulary of 20,000 words strictly in real-time ntirely in software on  a high-end workstation with large memory. We demonstrate hat  the computation needed grows as the cube root of the vocabulary  size.
121	Explains Non-Technical Concepts	A natural anguage understanding system for a restricted  domain of discourse - thermodynamic exercises at an intro-  ductory level - is presented. The system transforms texts  into a formal meaning representation language based on cases.  The semantical interpretation f sentences and phrases is con-  trolled by case frames formulated around verbs and surface  grammatical roles in noun phrases. During the semantical  interpretation of a text, semantic constraints may be im-  posed on elements of the text. Each sentence is analysed  with respect o context making the system capable of solving  anaphoric references uch as definite descriptions, pronouns  and elliptic constructions.  The system has been implemented and succesfully tested  on a selection of exercises.
602	Assumes Prior Knowledge	2.5.3 HTML Structure and Alignment In the STRAND system (Resnik, 1998), the candi- date pairs are evaluated by aligning them according to their HTML structures and computing confidence values. Pairs are assumed to be wrong if they have too many mismatching markups or low confidence values.
874	Explains Technical Concepts	The BREF Corpus: BREF is a large read-speech cor-  pus, containing over 100 hours of speech material, from 120  speakers (55m/65f)\[15\]. The text materials were selected  verbatim from the French newspaper Le Monde, so as to  provide a large vocabulary (over 20,000 words) and a wide  range of phonetic environments\[8\]. Containing 1115 distinct  diphones and over 17,500 triphones, BREF can be used to  train vocabulary-independenet phonetic models. The text  material was read without verbalized punctuation.
907	Explains Technical Concepts	The induction algorithm proceeds by  evaluating content of a series of attributes and  iteratively building a tree fiom the attribute values  with the leaves of the decision tree being the value  of the goal attribute. At each step of learning  procedure, the evolving tree is branched on the  attribute that pal-titions tile data items with the  highest information gain. Branches will be added  until all items in the training set arc classified. To  reduce the effect of overfitting, c4.5 prunes the  entire decision tree constructed. It recursively  examines each subtree to determine whether  replacing it with a leaf or brauch woukt reduce  expected error rate. This pruning makes the  decision tree better in dealing with tile data  different froul tile training data.
985	Explains Technical Concepts	6 Variational Decoding Statistical models in machine translation exhibit spurious ambiguity. That is, the probability of an output string is split among many distinct deriva- tions (e.g., trees or segmentations) that have the same yield. In principle, the goodness of a string is measured by the total probability of its many derivations. However, finding the best string dur- ing decoding is then NP-hard. The first version of Joshua implemented the Viterbi approximation, which measures the goodness of a translation us- ing only its most probable derivation.
947	Explains Technical Concepts	Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog pro- gram. This is done by a production system called Pragma, which acts like an advanced rewriting sys- tem with 580 rules. In addition, there is another rule base for actually generating the natural language answers (120 rules).
544	Explains Non-Technical Concepts	The system is written in FRANZ LISP (opus 42.15)  running under Berkeley 4.2 Unix. Future developments  will concentrate on improving its efficiency, in particular  by restructuring the code. We also hope to produce an  implementation in C, which should offer a faster  response time.
391	Explains Technical Concepts	5.3. Microphone Selection  In the problem we were trying to solve the test microphone is  not known, and is not even included in any data that we might  have seen before. In this case, how can we estimate a codebook  transformation like the one described above? One technique is to  estimate a transformation for many different types of microphones  and then use one of those transformations.   We had available stereo training data from several microphones  that were not used in the test. We grouped the alternate micro-  phones in the training into six broad categories, such as lapel,  telephone, omni-directional, directional microphones, and two  specific desk-mounted microphones. Then, we estimated a trans-  formed codebook for each of the microphones using stereo data  from that microphone and the Sennheiser, being sure that the  adaptation data included both male and female speakers.  To select which microphone transformation to use, we tried  simply using each of the transformed codebooks in turn, recog-  nizing the utterance with each, and then choosing the answer with  the highest score. Unfortunately, we found that this method did  not work well, because data that really came from the Sennheiser  microphone was often misclassified as belonging to another mi-  crophone. We believe this was due to the radically different  nature of the Gaussians for the Sennheiser and the alternate mi-  crophones. The alternate microphone Gaussians overlapped much  more.
393	Assumes Prior Knowledge	3.1 Results  Table 4 lists the performance measures for the  updated DP. Hit and recall rate increased, but so  did the false alarm rate, resulting in a lower  precision score. The d' score of the updated  system with respect o Pcomp (1.3) is substantially  better. The recall rate for this setting is perfect,  i.e., DP did not miss any presuppositions. Since  survey methodologists will decide whether the  presupposition is really a problem, a higher false  alarm rate is preferable to missing out  presupposition cases. Thus, the updated DP is an  improvement over the first version.
884	Explains Technical Concepts	4.1 Word Alignment In most phrase-based SMT systems the heuristic grow-diag-final-and is used to combine the align- ments generated by GIZA++ from both direc- tions. Then these alignments are used to extract the phrase pairs.
311	Assumes Prior Knowledge	ABSTRACT:  This paper 1 develops a method for combining query-  relevance with information-novelty in the context of  text retrieval and summarization. The Maximal Marginal  Relevance (MMR) criterion strives to reduce  redundancy while maintaining query relevance in re-  ranking retrieved documents and in selecting  appropriate passages for text summarization.  Preliminary results indicate some benefits for MMR  diversity ranking in ad-hoc query and in single  document summarization. The latter are borne out by  the trial-run (unofficial) TREC-style evaluation of  summarization systems. However, the clearest  advantage is demonstrated in the automated construction  of large document and non-redundant multi-document  summaries, where MMR results are clearly superior to  non-MMR passage selection. This paper also discusses  our preliminary evaluation of summarization methods  for single documents.
381	Explains Non-Technical Concepts	2. PartslD: A System for Identification of Parts for Medical Systems Initially, we were approached by the medical systems business of our company for help in reducing the number of calls handled by human operators at their call center. An analysis of the types of customer service provided by their call center showed that a large volume of calls handled by their operators were placed by field engineers requesting identification umbers of parts for various medical systems. The ID numbers were most often used for ordering the corresponding parts using an automated IVR system. Therefore, the system we have built helps automate some percentage of these calls by allowing the engineer to describe a part using natural language. The rest of this section describes our system in detail.
571	Explains Non-Technical Concepts	5 D iscuss ion In general, extra-linguistic information is hard to obtain. However, some extra-linguistic infor- mation can be easily obtained: (1) One piece of information is the participant's social role, which can be obtained from the in- terface such as the microphone used. It was proven that a clerk and customer as the social roles of participants are useful for translation into Japanese. However, more research is re- quired on another participant's social role. (2) Another piece of information is the par- ticipant's gender, which can be obtained by a speech recognizer with high accuracy (Takezawa and others, 1998; Naito and others, 1998). We have considered how expressions can be useful by using the hearer's gender for Japanese-to- English translation.
360	Explains Technical Concepts	6.2 Peer Orderings We evaluate the role played by various key  elements in our approach, including event, topic  continuity, time penalty, and LSA-style  dimensionality reduction. In addition, we  produce a random ordering and a baseline  ordering according to chronological and textual  order only. Table 2 lists the 9 peer orderings to  be evaluated, with their codes. A Random B Baseline (time order + textual order) C Entity only (no LSA) D Event only (no LSA) E Entity + Event ? topic continuity (no LSA) F Entity + Event ? time penalty (no LSA) G Entity + Event (no LSA) H Entity + Event (event clustering LSA) I Entity + Event (event + sentence clustering LSA) Table 2. Peer Orderings
58	Explains Non-Technical Concepts	2 Previous Works   Reviewing the previous works on Thai word  extraction, we found only the work of  Sornlertlamvanich and Tanaka (1996). They  employed the fiequency of the sorted character n-  grams to extract Thai open compounds; the strings  that experienced a significant change of  occurrences when their lengths are extended. This  algorithm reports about 90% accuracy of Thai  open compound extraction. However, the  algorithm emphasizes on open compotmd  extraction and has to limit tile range of n-gram to  4-20 grams for the computational reason. This  causes limitation in the size of corpora and  efficiency in the extraction.
799	Explains Technical Concepts	The most common and simplest approach to performing compound NLP tasks is the 1-best pipeline architecture, which only takes the 1-best hypothesis of each stage and pass it to the next one. Although it is comparatively easy to build and efficient to run, this pipeline approach is highly ineffective and suffers from serious prob- lems such as error propagation (Finkel et al, 2006; Yu, 2007; Yu et al, 2008). It is not sur- prising that, the end-to-end performance will be restricted and upper-bounded.
219	Assumes Prior Knowledge	We further notice that it is difficult to align sen- tences 0002. The sentence in the Chinese page is much longer than its counterpart in the English page because some additional information (font) is added. The length-based method thus tends to take sen- tence 0002, 0003, and 0004 in the English page as the translation of sentence 0002 in the Chinese page (Fig. 2), which is wrong. This in turn provocated the three following incorrect alignments. As we can see in Fig. 3, the cognate method did not make the same mistake because of the noise in sentence 0002. Despite their large length difference, the two 0002 sentences are still aligned as a 1-1 pair, because the sentences in the following 4 alignments (0003 - 0003; 0004 - 0004, 0005; 0005 - 0006; 0006 - 0007) have rather similar HTML markups and are taken by the program to be the most likely alignments. Beside HTML markups, other criteria may also be incorporated. For example, it would be helpful to consider strong correspondence b tween certain English and Chinese words, as in (Wu, 1994). We hope to implement such correspondences in our fu- ture research.
708	Explains Non-Technical Concepts	INTRODUCTION  As speech recognition technology advances, so do the  aims of system designers, and the prospects of potential ap-  plications. One of the main efforts underway in the com-  munity is the development of speaker-independent, task-  independent large vocabulary speech recognizers that can  easily be adapted to new tasks. It is becoming apparent  that many of the portability issues may depend more on the  specification of the task, and the ergonomy, than on the per-  formance of the speech recognition component itself. The  acceptance of speech technology in the world at large will  depend on how well the technology can be integrated in sys-  tems which simplify the life of the users. This in turns means  that the service provided by such a system must be easy to  use, and as fast as other providers of the service (i.e., such as  using a human operator).
640	Explains Technical Concepts	2.4. N-best Paradigm  The N-best Paradigm was introduced in 1989 as a way to  integrate speech recognition with natural anguage process-  ing. Since then, we have found it to be useful for applying  the more expensive speech knowledge sources as well, such  as cross-word models, tied-mixture densities, and trigrarn  language models. We also use it for parameter and weight  optimization. The N-best Paradigm is a type of fast match  at the sentence level. This reduces the search space to a  short list of likely whole-sentence hypotheses.  The Exact N-best Algorithm \[1\] has the side benefit hat it  is also the only algorithm that guarantees finding the most  likely sequence of words. Theoretically, the computation  required for this algorithm cannot be proven to be less than  exponential with the length of the utterance. However, this  case only exists when all the models of all of the phonemes  and words are identical (which would present a more seri-  ous problem than large computation). In practice, we find  that the computation required can be made proportional to  the number of hypotheses desired, by the use of techniques  similar to the beam search.
169	Assumes Prior Knowledge	Included in Table 7 as the 'bi.c' column is the result  of adding characters to bigram indexing, just like  adding characters to short-words. Compared to Table  6, it is seen that this is also useful in 3 out of 4 cases,  varying from -0.7% (.454 vs.0.457) to +13% (0.489  vs. .432) changes in AvPre for bigram results.  Characters are highly ambiguous as indexing terms but  there are also Chinese words that are truly single  character, and using bigrams only would not lead to  correct erm matching.
241	Explains Technical Concepts	The system uses no special features for delimiting the  scope of referred objects. When a reference is to be solved,  the objects and events specified in the leftmost context are  examined. An object or event, which fullfills the constraints  specified in the case frame and which matches possible syn-  tactic features (gender and number), is claimed to be the  token referred to. The resolution of synonymous references  (for instance of gas in : "A container contains llelium, and  the gas ... ") uses the is-a hierarchy.
208	Explains Non-Technical Concepts	2.5 Filtering Next, we further examine the contents of the paired files to determine if they are really parallel according to various external and internal features. This may further improve the pairing precision. The following methods have been implemented in our system.
916	Assumes Prior Knowledge	In addition to the work described above, there are re-  lated methods that have informed the research concern-  ing tied mixtures. First, mixture modeling does not re-  quire the use of Gaussian distributions. Good results  have also been obtained using mixtures of Laplacian dis-  tributions \[9, 10\], and presumably other component den-  sities would perform well too. Ney \[11\] has found strong  similarities between radial basis functions and mixture  densities using Gaussians with diagonal covariances. Re-  cent work at BBN has explored the use of elliptical basis  functions which share many properties with tied mix-  tures of full-covariance Gaussians \[12\]. Second, the posi-  tive results achieved by several researchers u ing non-tied  mixture systems \[13\] raise the question of whether tied-  mixtures have significant performance advantages over  untied mixtures when there is adequate training data.  It is possible to strike a compromise and use limited ty-  ing: for instance the context models of a phone can all  use the same tied distributions (e.g. \[14, 15\]).  Of course, the best choice of model depends on the na-  ture of the observation vectors and the amount of train-  ing data. In addition, it is likely that the amount of  tying in a system can be adjusted across a continuum to  fit the particular task and amount of training data. flow-  ever, an assessment of modeling trade-offs for speaker-  independent recognition is useful for providing insight  into the various choices, and also because the various  results in the literature are difficult to compare due to  differing experimental paradigms.
751	Mathematically-Oriented Paragraph	Here p(vi) is the probability that vi occurs in a doc-  ument and p(vi, vj) is the probability that vi and  vj occur in the same document. These probabili-  ties are based on frequencies gathered from approx-  imately 45,000 Wall Street Journal articles. Crite-  rion 1 is a measure of mutual information between  two verbs. Criterion 2 is used to rule out frequently  occurring verbs such as "be" and "make". Crite-  rion 3 allows for verbs which are ruled out by cri-  terion 2 to be associated when additional context  is available. This is important since some queries  only contain verbal forms which are ruled out by  criterion 2.
978	Explains Non-Technical Concepts	A characteristic syntactic expression in Consen- sical Grammar  may define an incomplete construct in terms of a "difference " between complete con- structs. When possible, the parser will use the sub- tracted part in stead of reading from the input, after a gap if necessary. The effect is the same as for Ex- traposition grammars, but the this format is more intuitive.
811	Explains Technical Concepts	Therefore, a Markov chain is used to learn  from the emotional context shown in the rec- orded transcripts for each sub-theme and for  each character, and generate other possible rea- sonable unseen emotional context similar to the  training data for each character. Markov chains  are usually used for word generation. In our ap- plication, they are used to record the frequencies  of several emotions showing up after one par- ticular emotion. A matrix has been constructed  dynamically for neutral and the 12 most com- monly used emotions in our application (caring,  arguing, disapproving, approving, grateful, hap- py, sad, threatening, embarrassed, angry/rude,  scared and sympathetic) with each row  representing the previous emotion followed by  the subsequent emotions in columns. The Mar- kov chains employ roulette wheel selection to  ensure to produce a greater probability to select  emotions with higher frequencies than emotions  with lower occurrences. This will allow the  generation of emotional context to probabilisti- cally follow the training data, which may reflect  the creative nature of the improvisation.   Then a dynamic algorithm is used to find the  most resembling emotional context for any giv- en new situation from the Markov chain?s train- ing and generated emotional contexts. I.e. by  using the algorithm, a particular series of emo- tions for a particular story sub-theme has been  regarded as the most resembling context to the  test emotional situation and an emotional state  is recommended as the most probable emotion  for the current user input. Since the most recent  affect histories of other characters and relation- ships between characters may also have an im- pact on the affect conveyed by the speaking  character, the recommended affect will be fur- ther evaluated (e.g. a most recent ?insulting?  input from Mayid could make Lisa ?angry?).
437	Explains Technical Concepts	In this paper we investigate two types of chart pruning: a standard beam search, similar to that used in the Collins parser (Collins, 1999), and a more aggressive strategy in which complete cells are pruned, following Roark and Hollingshead (2009). Roark and Hollingshead use a finite-state tagger to decide which words in a sentence can end or begin constituents, from which whole cells in the chart can be removed. We develop a novel extension to this approach, in which a tagger is trained to infer the maximum length constituent that can begin or end at a particular word. These lengths can then be used in a more agressive prun- ing strategy which we show to be significantly more effective than the basic approach. Both beam search and cell pruning are highly effective, with the resulting CCG parser able to process almost 100 sentences per second using a single CPU, for both newspaper and Wikipedia data, with little or no loss in accuracy.
956	Explains Technical Concepts	This problem can be avoided by keeping the lattice of all  sentence hypotheses generated by the algorithm, rather than  enumerating independent sentence hypotheses. Then the lat-  tice is treated as a grammar and used to rescore all the hy-  potheses with the more powerful knowledge sources \[10\].  2.5. Forward-Backward Search Paradigm  The Forward-Backward Search algorithm is a general  paradigm in which we use some inexpensive approximate  time-synchronous search in the forward direction to speed up  a more complex search in the backwards direction. This al-  gorithm generally results in tw o orders of magnitude speedup  for the backward pass. Since it was the key mechanism that  made it possible to perform recognition with a 20,000-word  vocabulary in real time, we discuss it in more detail in the  next section.
34	Explains Non-Technical Concepts	Most "new" documents merely repeat information  already contained in previously offered ones, and the  user could have tired long before reaching the first non-  TWA-800 air disaster document. Perfect precision,  therefore, may prove insufficient in meeting user needs.  A better document ranking method for this user is one  where each document in the ranked list is selected  according to a combined criterion of query relevance  and novelty of information. The latter measures the  degree of dissimilarity between the document being  considered and previously selected ones already in the  ranked list. Of course, some users may prefer to drill  down on a narrow topic, and others a panoramic  sampling bearing relevance to the query. Best is a user-  tunable method that focuses the search from a narrow  beam to a floodlight. Maximal Marginal Relevance  (MMR) provides precisely such functionality, as  discussed below.
635	Explains Technical Concepts	P rob lem Statement   Given the relative immaturity of summarization  technologies and their evaluation, it is worthwhile  to describe our approach in detail and the prob-  lems it is intended to solve. An important aspect  of our technique is that we produce sentence xtrac-  tion summaries which are constructed by selecting  sentences from the source document. In addition,  our summaries are focused on providing relevant  information about a query. We feel that the cur-  rent state-of-the-art techniques are better equipped  to produce high quality query-sensitive summaries  than generic summaries. Our goal is to produce  'indicative' summaries \[5\] which allow a user to de-  termine whether the document is relevant o his or  her query. The summary is not intended to replace  the document or provide answers to questions di-  rectly but may have this effect.  Casting our technology in terms of a product,  we see the application as an intermediate step be-  tween viewing entire documents and the output of  an information retrieval engine. Instead of looking  at either headlines or an entire document, the user  would look at the summaries of the documents and  then decide whether the document merited further  reading.
722	Explains Technical Concepts	In Figure 11, we examine the effect of compression  on normalized recall and precision and in Figure 12,  we show a plot of normalized F1. This F1 graph  indicates that the normalized F1 score is helped by  having the pseudo-relevance fe dback and title in the  query thereby extracting relevant sentences that  would otherwise be missed. As the number of  sentences that are allowed in the summary grows, the  difficulty of finding relevant sentences grows and  thus the added prf sentence and title to the query help  to find relevant sentences for their particular  document. We need to do more studying on the  effects of query expansion and compression on  summarization, as well as see how our preliminary  results hold for additional data sets.  If we calculate the normalized F1 score for the first  sentence retrieved in the summary, we obtain a score  of .80 for 110 Set standard query, .67 for 110 Set  short query and .79 for the Model Summaries. This  indicates that even for the short query we obtain a  relevant sentence two thirds of the time. However,  ideally this first sentence retrieval score would be 1.0  and we will explore methods to increase this score as  well as select a "highly relevant" first retrieved  sentence for the document.
421	Explains Technical Concepts	This fact has led us to an experiment with automatic translation between Czech and Slovak. It was clear that application of a similar method to that one used in the system RUSLAN would lead to similar results. Due to the closeness of both languages we have decided to apply a simpler method. Our new system, (~ESILKO, aims at a maximal exploitation of the similarity of both languages. The system uses the method of direct word-for-word translation, justified by the similarity of syntactic constructions of both languages.
525	Explains Non-Technical Concepts	1 Introduction A natural anguage interface to a computer database provides users with the capability of obtaining in- formation stored in the database by querying the system in a natural language (NL). With a natural language as a means of communication with a com- puter system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from hav- ing to know how the computer stores or processes the information.
285	Assumes Prior Knowledge	Figure 2: DSyntS (Graphical nd ASCII Notation)  The ConcSs correspond to the standard frame-  like structures used in knowledge representation,  with labeled arcs corresponding to slots. We  have used them only for a very limited  meteorological domain (in MeteoCogent), and  we imagine that they will typically be defined in  a domain-specific manner.
661	Explains Technical Concepts	Note that this discriminant ranking model pre- dicts the possibility of a discriminant being man- ually disambiguated. It is not modeling the spe- cific decision that the human annotator makes on the discriminant. Including the decision outcome in the model can potentially damage the annota- tion quality if annotators develop a habit of over- trusting the model prediction, making the whole manual annotation pointless. A discriminant rank- ing model, however, only suggestively re-orders the discriminants on the presentation level, which are much safer when the annotation quality is con- cerned.
385	Assumes Prior Knowledge	5.4 Discussion Many previous dialogue-based ITSs have been implemented with finite-state machines, either simple or augmented. In the most common finite state mode\[, each time the human user issues an utterance, the processor educes it to one of a small number of categories. These categories represent the possible transitions between states. Thus history can be stored, and context considered, only by expanding the number of states. This approach puts an arbitrary restriction on the amount of context or depth of conversational nesting that can be considered. More importantly, it misses the significant generalization that these types of dialogues are hierarchical: larger units contain repeated instances of the same smaller units in different sequences and instantiated with different values. Furthermore, the finite-state machine approach does not allow the author to drop one line of attack and replace it by another without hard- coding every possible transition. It is also clear that the dialogue-based approach has many benefits over the hint-sequence approach. In addition to providing a multi-step teaching methods with new content, it can respond flexibly to a variety of student answers at each step and take context into account when generating a reply.
394	Explains Technical Concepts	Cross-document coreference also differs in sub-  stantial ways from within-document coreference.  Within a document here is a certain amount of  consistency which cannot be expected across docu-  ments. In addition, the problems encountered dur-  ing within document coreference are compounded  when looking for coreferences across documents be-  cause the underlying principles of linguistics and  discourse context no longer apply across docu-  ments. Because the underlying assumptions in  cross-document coreference are so distinct, they re-  quire novel approaches.
7	Explains Technical Concepts	With both speed and quality in mind, a good treebank annotation method should acknowledge the complexity of the decision-making process; for instance, the same tree can be disambiguated by different sets of individual decisions which are mutually dependent. The annotation method should also strive to create a distraction-free en- vironment for annotators who can then focus on making the judgments. To this effect, we present a simple statistical model that learns from the anno- tation history, and offers a ranking of disambigua- tion decisions from the most to the least relevant ones, which enables well-trained human annota- tors to speed-up treebanking without compromis- ing on the quality of the linguistic decisions guid- ing the annotation task.
912	Assumes Prior Knowledge	The other notable new feature of our TREC-5 sys-  tem is the stream architecture. It is a system of par-  allel indexes built for a given collection, with each  index reflecting a different ext representation strat-  egy. These indexes are called streams because they  represents different streams of data derived from the  underlying text archive. A retrieval process earches  all or some of the streams, and the final ranking  is obtained by merging individual stream search re-  sults. This allows for an effective combination of  alternative document representation and retrieval  strategies, in particular various NLP and non-NLP  methods. The resulting meta-search system can be  optimized by maximizing the contribution of each  stream. It is also a convenient vehicle for an objec-  tive evaluation of streams against one another.
812	Assumes Prior Knowledge	We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fer- tilities generated by the PGIZA++2 Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using maximum likeli- hood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008).
56	Explains Non-Technical Concepts	3.2 Syntactic and Semantic Analysis  3.2.1 A meaning system  A detailed general purpose meaning system is  necessary for Kana-Kanji translation. The meaning  system adopted was basically a thesaurus of 32,600 words  classified into 798 categories of 5 levels \[6\]. The system  was enhanced by adding 11 higher level meaning codes  called macro codes, such as "human", "thing" and "place".  Each macro code was made by gathering related meaning  codes in the system. In the original system, these codes  appeared in different categories. The word dictionary  developed for the new system contains 160,000 words.  Each word is given a meaning code according to the new  meaning system.
412	Explains Technical Concepts	3 Word Reordering Model Reordering was applied on the source side prior to decoding through the generation of lattices en- coding possible reorderings of each source sen- tence that better match the word sequence in the target language. These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target lan- guage words in the corpus. For short-range re- orderings, continuous reordering rules were ap- plied to the test sentences (Rottmann and Vogel, 2007). To model the long-range reorderings be- tween German and English, different types of non- continuous reordering rules were applied depend- ing on the translation direction. (Niehues and Kolss, 2009). When translating from English to German, most of the changes in word order con- sist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction.
183	Explains Technical Concepts	 To estimate these model parameters, we gather the annotation logs from our treebank annotators on the completed datasets with detailed informa- tion about each discriminant. Apart from the necessary information to reconstruct the discrim- inants from the forest, the log also contains the status information of i) whether the discriminant takes value 0 or 1 on the gold tree; ii) whether the human annotator has said yes or no to the dis- criminant. Note that the human annotator does not need to manually decide on the value of each dis- criminant. Whenever a new decision is made, the forest will be pruned to the subset of trees compat- ible with the decision. And all remaining discrim- inants are checked for effectiveness on the pruned forest. Discriminants which become ineffective from previous decisions are said to have received inferred values.
336	Explains Non-Technical Concepts	A potential advantage of self-training is the availability of large amounts of training data. However, our results are somewhat negative in this regard, in that we find training the tagger on more than 40,000 parsed sentences (the size of CCGbank) did not improve the self-training re- sults. We did see the usual speed improvements from using the self-trained taggers, however, over the baseline parser with no pruning.
872	Explains Technical Concepts	   Before illustrating how HITS can be applied  to our scenario, let us first give a brief  introduction to HITS. Given a broad search  query q, HITS sends the query to a search  engine system, and then collects k (k = 200 in  the original paper) highest ranked pages, which  are assumed to be highly relevant to the search  query. This set is called the root set R; then it  grows R by including any page pointed to a  page in R, then forms a base set S. HITS then  works on the pages in S. It assigns every page in  S an authority score and a hub score. Let the  number of pages to be studied be n. We use G =  (V, E) to denote the (directed) link graph of S. V is the set of pages (or nodes) and E is the set of  directed edges (or links). We use L to denote the  adjacency matrix of the graph.
356	Explains Technical Concepts	2.4 Graphical User Interface (GUI)  For some applications such as database  population, the user may want to validate the  system output. REES is provided with a Java-  based Graphical User Interface that allows the  user to run REES and display, delete, or  modify the system output. As illustrated in  Figure 4, the tool displays the templates on the  bottom half of the screen, and the user can  choose which template to display. The top half  of the screen displays the input document with  extracted phrases in different colors. The user  can select any slot value, and the tool will  highlight the portion of the input text  responsible for the slot value. This feature is  very useful in efficiently verifying system  output. Once the system's output has been  verified, the resulting templates can be saved  and used to populate adatabase.
100	Mathematically-Oriented Paragraph	2. MAXIMAL MARGINAL  RELEVANCE  Most modern IR search engines produce a ranked list  of retrieved ocuments ordered by declining relevance  to the user's query \[1, 18, 21, 26\]. In contrast, we  motivated the need for '"relevant novelty" as a  potentially superior criterion. However, there is no  known way to directly measure new-and-relevant  information, especially given traditional bag-of-words  methods uch as the vector-space model \[19, 21\]. A  first approximation to measuring relevant novelty is to  measure relevance and novelty independently and  provide a linear combination as the metric. We call the  linear combination "marginal relevance" -- i.e. a  document has high marginal relevance if it is both  relevant to the query and contains minimal similarity to  previously selected ocuments. We strive to maximize  marginal relevance in retrieval and summarization,  hence we label our method "maximal marginal  relevance" (MMR).  The Maximal Marginal Relevance (MMR) metric is  defined as follows:  Let C = document collection (or document stream)  Let Q = ad-hoc query (or analyst-profile or  topic/category specification)  Let R = IR (C, Q, q) - i.e. the ranked list of  documents retrieved by an IR system, given C and Q  and a relevance threshold theta, below which it will not  retrieve documents. (q can be degree of match, or  number of documents).  Let S = subset of documents in R already provided to  the user. (Note that in an IR system without MMR and  dynamic reranking, S is typically a proper prefix of list  R.) R~ is the set difference, i.e. the set of documents in R, not yet offered to the user.  def  MMR(C,Q,R,S)=Argmax\[X*Sim 1 (Di,Q)-(1-X)Max(Sim2(Di,Dj))\]  Di ~R\S Dj eS
59	Explains Technical Concepts	Applying the HITS algorithm: Based on the  key idea of HITS algorithm and feature indica- tors, we can apply the HITS algorithm to obtain  the feature relevance ranking. Features act as  authorities and feature indicators act as hubs.  Different from the general HITS algorithm, fea- tures only have authority scores and feature in- dicators only have hub scores in our case. They  form a directed bipartite graph, which is illu- strated in Figure 2. We can run the HITS algo- rithm on this bipartite graph. The basic idea is  that if a feature candidate has a high authority  score, it must be a highly-relevant feature. If a  feature indicator has a high hub score, it must be  a good feature indicator.
473	Explains Technical Concepts	5.1 Word  Segmentat ion   A major difference of Chinese writing from English  is that a Chinese sentence (which can usually be  recognized by a punctuation ending) consists of a  continuous tring of characters and there is no white-  space to delimit words. Words can be one, two or more  characters long. At the time, we believed that word  segmentation is important for effective Chinese IR.  Since efficient word segmentation software for large  collections were not available, we relied on an  approximate short-word segmenter that was developed  by ourselves in house (Queens segmenter \[9\]).
896	Explains Technical Concepts	The rules given above suffer from the same problem as the  previous rules, namely, over generation. Although they produce  all the right answers and allow nmltiple forms for words like  \[travel+er\] ~ (\[traveller\] or \[traveler\]), which is certainly a  positive result, they also allow multiple forms for words which do  not allow them. For instance they generate both \[referred\] and  \[refered\]. As mentioned earlier, this problem will be tolerated for  the time being.
146	Explains Non-Technical Concepts	This operator library used to generate this text contained 1l 1 plan operators, divided as follows: Tutoring schemata Switching between schemata API and GUI handling Answer handling Domain-dep. lex. insertion Domain-indep. lex. insertion TOTAL 4 4% 5 4% 33 30% 35 31% 24 22% 10 9% 111 100% We are currently working on components hat will allow us to increase the number of physics concepts covered without a corresponding increase in the number of operators. The schema switching operators prevent the tutor from repeating itself during a physics problem. They could be reduced or eliminated by a general discourse history component that tutoring schema operators could refer to. Domain-dependent lexical insertion refers to the choice of lexical items such as car and east in the sample dialogue, while domain-independent iexical insertion refers to items such as OK and exactly. Both categories could be eliminated, or at least severely reduced, through the use of a text realization package. Together that would provide a one-third reduction in the number of operators needed. As the set of API and GUI handling operators is fixed, that would reduce by half the number of application operators needed.
768	Explains Technical Concepts	2.1 Search for candidate Sites We take advantage of the huge number of Web sites indexed by existing search engines in determining candidate sites. This is done by submitting some particular equests to the search engines. The re- quests are determined according to the following ob- servations. In the sites where parallel text exists, there are normally some pages in one language con- taining links to the parallel version in the other lan- guage. These are usually indicated by those links' anchor texts 1. For example, on some English page there may be a link to its Chinese version with the anchor text "Chinese Version" or "in Chinese". 1An anchor text  is a piece of text on a Web page which, when clicked on, will take you to another linked page. To be helpful, it usual ly  contains the key information about the l inked page.
409	Explains Technical Concepts	Topic Expansion Experiments  In this section we discuss a semi-interactive ap-  proach to information retrieval which consists of two  tasks performed in a sequence. First, the system as-  sists the searcher in building a comprehensive state-  ment of information eed, using automatically gen-  erated topical summaries of sample documents. Sec-  ond, the detailed statement of information eed is  automatically processed by a series of natural lan-  guage processing routines in order to derive an op-  timal search query for a statistical information re-  trieval system. We investigate the role of automated  document summarization i  building effective search  statements.
284	Explains Technical Concepts	1.2 Most  F requent  Words   In order to extract he most fi'equent words of  the acquired corpora we used equally-sized text  samples (approximately 640k) from each  category providing a genre-corpus of 2560k for  the four categories. The genre-corpus was  divided into 160 text samples of 16k (i.e.,  approximately 2,000 words) each, including 40  text samples from each genre. Hall' of the text  samples from each category were used as  training corpus and the rest as test corpus.  For the extraction of the most frequent words of  the entire English language we used the British  National Corpus (BNC). This corpus consists of  100M tokens covering both written and spoken  language.
858	Explains Non-Technical Concepts	The distillation took about three hours for all 39 dialogues, i.e. it is reasonably fast. The distilled dialogues are on the average 27% shorter. However, this varies between the dialogues, at most 73% was removed but there were also seven dialogues that were not changed at all.
176	Assumes Prior Knowledge	DP reports when a presupposition is present, and  it also indicates the type of presupposition that is  made (e.g., a common ground presupposition or the presupposition f a habit) in order to point the  question designer to the potential presupposition  error. DP uses the expressions in the right  column in Table 2, selected in accordance with  the indicators, and fills them into the brackets in  its output (see Figure 1). For example, given the  question "How old is your child?", DP would  detect the possessive pronoun "your", and  accordingly respond: "It looks like you are  presupposing a possession (child). Make sure that  the presupposition is correct by consulting the  previous questions."
180	Explains Technical Concepts	As expected, our system did not rank among the top competitors in the WMT10 shared task. This is mainly due to the data we trained on, which is of a very spe- cific domain (common legislative procedures of Eu- ropean Institutions) and relatively small in size when compared to what others used for this language combi- nation. In addition, we more or less used Moses out-of- the-box with no sophisticated add-ons or optimization. In the internal evaluation, our system beat neither Google Translate nor ECMT overall but it did show a similar performance. This is all the more encourag- ing as Exodus has been built within less than a month, while ECMT has been developed and maintained in ex- cess of 30 years, and while Google Translate is based on manpower and computing resources that a public administration body usually cannot provide. Finally, the successful trials of SMT software at the EC?s Portuguese department seem to indicate that such a system holds enormous potential, especially when a serious adaptation to specific language combinations and domains is taken into consideration.
532	Explains Technical Concepts	After the HSQL Project was finished, an inter- nal reseach project TUC (the Understanding Com- puter) was initiated at NTNU to carry on the results from HSQL. The project goals differed from those of HSQL in a number of ways, and would not be con- cerned with multimedia interfaces . On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications. The research goals could be sum- marised as to ? Give computers an operational understanding of natural language. ? Build intelligent systems with natural language capabilities. ? Study common sense reasoning in natural an- guage.
306	Explains Technical Concepts	If we used the same rules as for reordering the test sets, the lattice would be so big that the num- ber of extracted phrase pairs would be still too high. As mentioned before, the word reordering is mainly a problem at the phrase extraction stage if one word is aligned to two words which are far away from each other in the sentence. There- fore, the short-range reordering rules do not help much in this case. So, only the long-range reorder- ing rules were used to generate the lattices for the training corpus.
19	Explains Technical Concepts	This concern also proved unfounded in practice. In  another experiment, we evaluated the processing  speed of the system by measuring the time it takes  per character in the input sentence (in  milliseconds) relative to the sentence length. The  results are given in Figure 5. This figure shows  that the processing time per-character grows  moderately as the sentence grows longer, due to  the increased number of intermediate analyses  created during the parsing. But the increase is  linear, and we interpret these results as indicating  that our approach is fully viable and realistic in  terms of processing speed, and robust against input  sentence length. The current average parsing time  for our 15,000-sentence corpus (with average  sentence length of 49.02 characters) is 23.09  sentences per second on a Dell 550MHz Pentium  III machine with 512MB of RAM.
651	Explains Technical Concepts	Implementation  The summarization technique was developed within  the CAMP NLP framework. This system provides  an integrated environment in which to access many  levels of linguistic information as well as world  knowledge. Its main components include: named  entity recognition, tokenization, sentence detec-  tion, part-of-speech tagging, morphological analy-  sis, parsing, argument detection, and coreference  resolution. Many of the techniques used for these  tasks perform at or near the state of the art and are  described in more depth in \[16, 12, 11, 9, 6, 2, 3\].  The system produces coreference annotated ocu-  ments which serve as the input to the summariza-  tion algorithm.
68	Assumes Prior Knowledge	In the target syntax model, the target gaps and the entire target phrase must map to constituents in the parse tree. This restriction may be relaxed by adding constituent labels such as DET+ADJ or NP\DET to group neighboring constituents or indi- cate constituents that lack an initial child, respec- tively (Zollmann and Venugopal, 2006).
834	Assumes Prior Knowledge	At first glance, our results suggest hat allowing  query-specific fusion functions substantially  improves retrieval. For instance, by using query-  specific static fusion functions, we achieved precision  at 5 of .5960, compared to .3840 when applying the  same static fusion function to all queries. However,  this comparison is overly optimistic, since it allows  query-specific fusion functions to be trained and  evaluated on exactly the same data, while forcing the  overall fusion function to be trained on a large set of  data, but then evaluated on a small subset of that  data. To provide a more pessimistic omparison, we  partitioned the data for our 50 queries into equally-  sized training and test sets. We trained each query-  specific fusion function on the query's training data,  and evaluated it on the test data. (Although our goal  is to improve retrospective r trieval, this arrangement  resembles the TREC routing scenario.) Table 3  shows a considerably weaker, but still appreciable  improvement due to using query-specific fusion  functions. For instance, we achieved precision at 5 of  .4160 when allowing each query its own static fusion  function, compared to .3400 when forcing all queries  to use the same function.
478	Explains Technical Concepts	2. EFFICIENT CEPSTRUM-BASED  COMPENSATION TECHNIQUES  In this section we describe several of the cepstral normal-  ization techniques we have developed to compensate  simultaneously for additive noise and linear filtering. Most  of these algorithms are completely data-driven, as the com-  pensation parameters are determined by comparisons  between the testing environment and simultaneously-  recorded speech samples using the DARPA standard clos-  etalking Sennheiser HMD-414 microphone (referred to as  the CLSTLK microphone in this paper). The remaining  algorithm, codeword-dependent cepstral normalization  (CDCN), is model-based, asthe speech that is input to the  recognition system is characterized as speech from the  CLSTLK microphone that undergoes unknown linear filter-  ing and corruption by unknown additive noise.  In addition, we discuss two other procedures, the RASTA  method, and cepstral mean normalization, that may be  referred to as cepstral-filtedng techniques. These proce-  dures do not provide as much improvement as CDCN,  MFCDCN and related algorithms, but they can be imple-  mented with virtually no computational cost.
450	Explains Non-Technical Concepts	In the remainder of this paper we briefly  review the prior work on responding to ill-  formedness, describe our proposal and its imple-  mentation as part of a small question-answering  system, and relate our initial experiences with this  system
266	Explains Non-Technical Concepts	7 Conclus ion  The system described in this paper transforms thermodynam-  ical exercises expressed in Danish into a formal meaning rep-  resentation language. In order to accomplish this, morphol-  ogy, syntax and semantics are considered. Most important  is the application of the case grammar formalism, in which  semantic onstraints can be imposed on phrases, causing am-  biguities in a text to be removed. The case grammar have  a clear, well-defined structure and is easy to extend, also to  other domains.
53	Assumes Prior Knowledge	Indicator  "how often" ...VP  "how" aux NP VP  "while"... VP  "where"... VP  "why"... VP  Presupposition type: The  question presupposes...  an action (V)  "usually"... VP  "how often",  "frequently", etc.  a habit (V)  "how many" NP  "where is" NP  Indexicals:  "this" / "that" NP  "these" / "those" NP  "such a(n)" NP  an entity: object, state, or  person (NP)  a shared referent orcommon  ground (NP)  "how much" NP ...  "how much does" NP  "know"  "how many" NP ...  Possessive pronouns  Apostrophe 's': NP's  a possession (NP);  exception list: NP's that can be  presupposed (name, age, etc.)  "why" S a state of affairs, fact, or  assertion (S)  VP infinitive an intention / a goal (infinitive /  "why" VP NP NP VP)  "who" VP  "When" VP  ..."when" NP VP  an a~ent (A person who VP)  an event (VP)
418	Explains Non-Technical Concepts	6 Conc lus ion We have proposed a method of translation us- ing information on dialogue participants, which is easily obtained from outside the translation component, and applied it to a dialogue trans- lation system for travel arrangement. This method can select a polite expression for an utterance according to the "participant's social role," which is easily determined by the inter- face (such as the wires to the microphones). For example, if the microphone is for the clerk (the speaker is a clerk), then the dialogue translation system can select a more polite expression. In an English-to-Japanese translation system, we added additional transfer ules and transfer dictionary entries for the clerk to be more po- lite than the customer. Then, we conducted an experiment with 23 unseen dialogues (344 ut- terances). We evaluated the translation results to see whether the impressions of the results im- proved or not. Our method achieved a recall of 65% and a precision of 86%. These rates could easily be improved to 86% and 96%, respec- tively. Therefore, we can say that our method is effective for smooth conversation with a dia- logue translation system.
982	Explains Technical Concepts	Our seven learners will generate different clas-  sifications of tile training data and we need to  find out which combination techniques are most  appropriate. For the system-external combi-  nation experiment, we have evaluated itfi;rent  voting lllechanisms~ effectively the voting meth-  ods as described in Van Halteren et al (1998).  In the first method each classification receives  the same weight and the most frequent classifi-  cation is chosen (Majority). The second nmthod  regards as tile weight of each individual clas-  sification algorithm its accuracy on solne part  of the data, tile tuning data (TotPrecision).  The third voting method computes the preci-  sion of each assigned tag per classifer and uses  this value as a weight for tile classifier in those  cases that it chooses the tag (TagPrecision).  The fourth method uses both the precision of  each assigned tag and tile recall of the com-  peting tags (Precision-Recall). Finally, tile fifth  lnethod uses not only a weight for tile current  classification but it also computes weights tbr  other possible classifications. The other classi-  fications are deternfined by exalnining the tun-  ing data and registering the correct wflues for  (;very pair of classitier esults (pair-wise voting,  see Van Halteren et al (1998) tbr an elaborate  explanation).
429	Assumes Prior Knowledge	Each tone group contains an element which  carries the nuclear stress, called Tonic. In the  default case, the Tonic is placed on the last lex-  (a) / /1 Margaret's / looking for you / /   (b) / /2 A is there / any more / news of the / Iq-ench  e/ leet ions/ /   (c) / /4  A in the/ far corner of that/f ield the / /  1 foot-  path goes / over a / sq;i\]e_//  Figure 3: Examples of SFG labelling  ical elenmnt in tile tone group (unmarked nu-  clear stress). In marked cases, the Tonic can  be placed on other elements in the tone group.  For an example of the tbrmer see (b) in Fig. 3  (Tonic denoted by underlining); an example of  the latter is (a) in Fig. 3.
300	Explains Non-Technical Concepts	5 Conc lus ion   A morphological analyzer has been presented that is capable of  interpreting both orthographic and syntactic rules. This rep-  resents a substantial improvement over the method of incorpo-  rating morphological facts directly into the code of an analyzer.  The use of these rules leads to a powerful, flexible morphological  analyzer.
23	Explains Non-Technical Concepts	Abstract  In this paper we address the problem of  discovering word semantic similarities via  statistical processing of text corpora. We  propose a knowledge-poor method that  exploits the sentencial context of words for  extracting similarity relations between them  as well as semantic in nature word clusters.  The approach aims at full portability across  domains and languages and therefore is  based on minimal resources.
338	Assumes Prior Knowledge	3. DETAILED MODEL ING THROUGH  PARAMETER SHARING  We need to model a wide range of acoustic-phonetic phenom-  ena, but this requires a large amount of training data. Since  the amount of available training data will always be finite one  of the central issues becomes that of how to achieve the most  detailed modeling possible by means of parameter sharing.  Our successful examples include SCHMMs and senones.
891	Explains Non-Technical Concepts	Abstract This paper describes a system that provides customer service by allowing users to retrieve identification umbers of parts for medical systems using spoken natural language dialogue. The paper also presents an evaluation of the system which shows that the system successfully retrieves the identification numbers of approximately 80% of the parts.
534	Explains Technical Concepts	5 Effects of the Original Source Language of Articles on Translation During our experiments, we found that translation quality is highly variable depending on the origi- nal source language of the news sentences. This phenomenon is correlated to the previous work of Kurokawa et al (2009) that showed that whether or not a piece of text is an original or a trans- lation has an impact on translation performance. The main reason that explains our observations is probably that the topics and the vocabulary of news originally expressed in languages other than French and English tend to differ more from those of the training materials used to train PBM mod- els for these two languages. In order to take into account this phenomenon, MERT tuning was re- peated for each original source language, using the same PBM models trained on all parallel corpora and incorporating an IR score.
206	Explains Technical Concepts	   Following the initial work in (Hu and Liu  2004), several researchers have further explored  the idea of using opinion words in product fea- ture mining. A dependency based method was  proposed in (Zhuang et al, 2006) for a movie  review analysis application. Qiu et al (2009)  proposed a double propagation method, which  exploits certain syntactic relations of opinion  words and features, and propagates through  both opinion words and features iteratively. The  extraction rules are designed based on different  relations between opinion words and features,  and among opinion words and features them- selves. Dependency grammar was adopted to  describe these relations. In (Wang and Wang,  2008), another bootstrapping method was pro- posed. In (Kobayashi et al 2007), a pattern min- ing method was used. The patterns are relations  between feature and opinion pairs (they call as- pect-evaluation pairs). The patterns are mined  from a large corpus using pattern mining. Statis- tics from the corpus are used to determine the  confidence scores of the extraction.
113	Explains Technical Concepts	2.1.3 Algorithm of morphological nalysis  Figure 1 shows the algorithm for the morphological  analysis. All candidates for dependent words are first  picked up from input Kana sentences by using a string-  matching operation and by examining the grammatical  connectability between a preceding word and its  successor. This process is executed from right to left,  resulting in the generation of subtrees of dependent  words.
168	Explains Technical Concepts	3 Reference Time Dynamic-choosing Mechanism  3.1 Referential feature in Implicit Time  In this paper, we define the Implicit Time con- sists of the modifier and the temporal noun  which is modified by modifiers. And here we  extend the modifier based on the TIMEX2 Stan- dard, which include verb, conjunction, adverb  and preposition that quantify or modify temporal  nouns. For example, ?ten days? is a temporal  noun, but ?ten days ago? is modified after add- ing the modifier ?ago?.
990	Explains Non-Technical Concepts	4.2 Temporal Expression Defuzzification  In general, the defuzzification for fuzzy times  faces two problems: deciding granularity and  choosing offset. Here we introduce some know- ledge on the human cognitive psychology and  the empirical method to figure out these two is- sues respectively. Based on the scenario-time  shifting model referred in related works, we get  the conclusion that once the scenario is shifting,  the time is shifting. More specifically, the time  shifting is reflected in the temporal granularity  between two different scenarios. So referring to  writers, they will choose a few temporal expres- sions own the same granularity to render the co- herent temporal dimensionality in one scenario  in order to avoid generating improper scenario  shifting for readers. Figure 4 describes the varia- tion process of the temporal granularity between  two different scenarios through scenario-time  shifting.    5.2 Results  Results on Implicit Times classification: We  firstly choose some temporal expressions classi- fied in advance by crafted, and manually extend  them in expressing patterns as the original train- ing samples. For example, ?last month? will ex- tend to ?this month? and ?next month?, which  all belong to Global Times. Actually there are  only 16,104 temporal expressions in our experi- ment because integrated temporal expressions in  corpus are segmented into several parts, and we  combine them together again before operating.  Using classifier trained by training data, we get  2,264 Global Times and 998 Local Times from  testing collections, where there are 1,705 Global  Times and 804 Local Times are correct respec- tively by manual statistics. Table 3 gives the de- tails of classification.
467	Explains Technical Concepts	Hierarchical decomposition asserts that each goal can be achieved via a series of subgoals instead of relying on means-end reasoning. Hierarchical decomposition is more appropriate to dialogue generation for a number of reasons. First, decomposition is better suited to the type of large- scale dialogue planning required in a real-world tutoring system, as it is easier to establish what a human speaker will say in a given situation than to be able to understand why in sufficient detail and generality to do means-end planning. Second, Hierarchical decomposition minimizes search time. Third, our dialogues are task-oriented and have a hierarchical structure (Grosz and Sidner 1986). In such a case, matching the structure of the domain simplifies operator development because they can often be derived from transcripts of human tutoring sessions. The hierarchy information is also useful in determining appropriate referring expressions. Fourth, inter- leaved planning and execution is important for dialogue generation because we cannot predict he human user's future utterances. In an HTN-based system, it is straightforward to implement interleaved planning and execution because one only needs to expand the portion of the plan that is about to be executed. Finally, the conversation is in a certain sense the trace of the plan. In other words, we care much more about the actions generated by the planner than the states involved, whether implicitly or explicitly specified. Hierarchical decomposition provides this trace naturally.
754	Explains Non-Technical Concepts	5. CONCLUSION  A new disambiguation approach to Kana-kanji  translation for non-segmented input sentences has been  described. Ambiguitiy is resolved using syntactic and  semantic analysis based on a case grammar. To avoid a  combinatorial explosion of the ambiguity, some heuristics  are introduced. Large dictionaries were also developed for  the experimental system and both the limit and  substantial performance of the system were evaluated.  The experimental results show that an accuracy of 90.5%  is obtainable using this approach, and that the accuracy  can be improved to about 95% by optimizing the  dictionaries. Further improvement can be achieved by  introducing context analysis and plagmatic analysis.
941	Explains Technical Concepts	When we say \[+\] corresponds to \[el between an lxl and an N,  we mean between a Icxical I x\] corresponding to a surface lxl and  a lexical Is\] corrcsponding to a surface \[s\]. If we wantcd to say  that it does not matter what the lexieal \[x\] corresponds to on the  surface, we would use \[x/=\] instead of just ix\].  The rules given above get tile facts right for the words that  do not end in \[o\]. For those that do, however, Rule 1 misses  on \[do+s\] ~-~ \[docs\], \[potato+s\[ ?=~ \[potatoes\]; Rule 2 misses  on \[piano+s\] ~ \[pianos\], \[solo+s\] ~:~ \[solos\[. Furthermore,  neither ule allows for the possibility of more than one acceptable  form, as in \[banjo+s\] ~ (\[banjoes\] or \[banjos\]), \[cargo+s\]  (\[cargoes\] or \[cargos\]).
280	Explains Technical Concepts	1 Introduction  The task of Information Extraction (IE) as under-  stood in this paper is the selective xtraction of mean-  ing from free natural language text. 1 This kind of  text analysis is distinguished from others in Natu-  ral Language Processing in that "meaning" is under-  stood in a narrow sense - in terms of a fixed set of se-  mantic objects, namely, entities, relationships among  these entities, and events in which these entities par-  ticipate. These objects belong to a small number of  types, all having fixed regular structure, within a fixed  and closely circumscribed subject domain, which per-  mits the objects to be stored in a database (e.g., a  relational data base). These characteristics make the  IE task both simpler and more tractable than the  more ambitious problem of general text understand-  ing. They allow us to define the notion of a "correct  answer", and, therefore, to conduct quantitative val-  uation of performance ofan IE system. A series of for-  mal evaluations - -  the Message Understanding Con-  1For a review of a range of extraction systems, the reader  is referred to \[9\].  ferences (MUCs), 2 conducted over the last decade - -   are described in \[8, 6\].
25	Explains Non-Technical Concepts	4.3 Further  Problems The Chinese-English translation model has a fax lower CLIR performance than that of the English- French model established using the same method (Nie et al, 1999). The principal reason for this is the fact that English and Chinese are much more differ- ent than English and French. This problem surfaced in many phases of this work, from text alignment to query translation. Below, we list some further fac- tors affecting CLIR precision.
216	Explains Technical Concepts	This attempt has a two-fbld motiw~tion.  First, it is motivated by computational ppli-  cation in concept-to-si)eech systems, in which  text in spoken mode is automatically generated  from an underlying abstract lneaning represen-  tation, it is widely acknowledged that in order  for spoken language technology to gain wider  acceptance, it has to improve on the quality of  output considerably. Itere, appropriate intona-  tion is one of the major factors (ct'. Cole et  al. (1995)). The concrete goal we are pursu-  ing is to connect an oil-the-shelf speech syn-  thesizer for English (FESTIVAL; (Black et al,  1998)) with an automatic text generation sys-  tem tbr English based on SFO (Matthiessen & Bateman, 19911. Since in the SFO approach, in-  tonation is accounted for as part of grammar  rather than as an independent component, it is  straightforward to extend the grammatical re-  sources of a systemically based text generation  system with an account of intonation (cf Teich  et al (1.997) iml)lenmnting such all approach for  German concet/t-to-speech generation). Con-  necting such a system to a speech synthesizer  requires mapping the OUtl)ut of the generator  to the input requirements of the st)eech synth(>  sizer. In the FESTIVAL systei11, the intonation of  the text to be synthesized can be manipulated  1)y ~mnotation with TOBI labels. Therefore, a mapl)ing betweeIl the SFC and the ToBI anno-  tation systems is required.
687	Assumes Prior Knowledge	For the word alignments, we chose MGIZA (Gao and Vogel, 2008), using seven threads per MGIZA in- stance, with the parallel option, i.e. one MGIZA in- stance per pair direction running in parallel. The target language model is a 7-gram, binarized IRSTLM (Fed- erico et al, 2008). The weights of the distortion, trans- lation and language models were optimized with re- spect to BLEU scores (Papineni et al, 2002) on a given held-out set of sentences with Minimum Error Rate Training (MERT; (Och, 2003)) in 15 iterations. After the actual translation with Moses, an additional recasing ?translation? model was applied in the same manner. Finally, the translation output underwent min- imal automatic postprocessing based on regular expres- sion replacements. This was mainly undertaken in or- der to fix the distribution of whitespace and some re- maining capitalization issues.
11	Explains Technical Concepts	5.1. Cepstrum Preprocessing  The RASTA algorithm \[11\] smoothes the cepstral vector with a  five-frame averaging window, and also removes the effect of a  slowly varying multiplicative filter, by subtracting an estimate  of the average cepstrum. This average is estimated with an ex-  ponential filter with a constant of 0.97, which results in a time  constant of about one third of a second. The blind deeonvolution  algorithm estimates the simple mean of each cepstral value over  the utterance, and then subtracts this mean from the value in each  frame. In both cases, speech frames are not distinguished from  noise frames. The processing is applied to all frames equally. In  addition, there was no dependence on estimates of SNR.  Every test utterance was recorded simultaneously on the same  microphone used in the training (a high-quality noise-cancelling  Sennheiser microphone) and on some other microphone which  was not known, but which ranged from an omni-directional  boom-mounted microphone or table-mounted microphone, a lapel  microphone, or a speaker-phone.
590	Assumes Prior Knowledge	Abstract  This paper presents a disambiguation approach for  translating non-segmented-Kana i to Kanji. The method  consists of two steps. In the first step, an input sentence is  analyzed morphologically and ambiguous morphemes are  stored in a network form. In the second step, the best  path, which is a string of morphemes, is selected by  syntactic and semantic analysis based on case grammar.  In order to avoid the combinatorial explosion of possible  paths, the following heuristic search method is adopted.  First, a path that contains the smallest number of  weighted-morphemes is chosen as the quasi-best path by a  best-first-search technique. Next, the restricted range of  morphemes near the quasi-best path is extracted from the  morpheme network to construct preferential paths.  An experimental system incorporating large  dictionaries has been developed and evaluated. A  translation accracy of 90.5% was obtained. This can be  improved to about 95% by optimizing the dictionaries.
419	Explains Non-Technical Concepts	9 Visualization We created tools for visualizing two of the main data structures used in Joshua (Weese and Callison-Burch, 2010). The first visualizer dis- plays hypergraphs. The user can choose from a set of input sentences, then call the decoder to build the hypergraph. The second visualizer dis- plays derivation trees. Setting a flag in the con- figuration file causes the decoder to output parse trees instead of strings, where each nonterminal is annotated with its source-side span. The visual- izer can read in multiple n-best lists in this format, then display the resulting derivation trees side-by- side. We have found that visually inspecting these derivation trees is useful for debugging grammars. We would like to add visualization tools for more parts of the pipeline. For example, a chart visualizer would make it easier for researchers to tell where search errors were happening during decoding, and why. An alignment visualizer for aligned parallel corpora might help to determine how grammar extraction could be improved.
969	Explains Technical Concepts	2.2. N-Best Recognition Techniques  N-best techniques\[2\] are also similar to progressive  search in that a coarse match is used to constrain a more  computationaUy costly technique. In this case. the coarse  mateher is a complete (simple) speech recognition system. The  output of the N-best system is a list of the top N most likely  sentence hypotheses, which can then be evaluated with the  slower but more accurate techniques.  Progressive search is a generalization of N-best--the  earlier-pass technique produces a graph, instead of a list of N-  best sentences. This generalization is crucial because N-best is  only eomputationally effective for N in the order of tens or  hundreds. A progressive search word graph can effectively  account for orders of magnitude more sentence hypotheses. By  limiting the advanced techniques to just searching the few top N  sentences, N-best is destined to limit the effectiveness of the  advanced techniques and, consequently, the overall system's  accuracy. Furthermore, it does not make much sense to use N-  best in an iterative fashion as it does with progressive s arches.
323	Explains Non-Technical Concepts	The complete corpus contained solutions to five physics problems by 41 students each. We analyzed every tutoring episode dealing with the acceleration vector during deceleration, totaling 29 examples divided among 20 students and both tutors. The tutors had very different styles. Tutor A tended to provide encouragement rather than content, making those transcripts less useful for deriving an information-based approach. Tutor B used an information-based approach, but after one wrong answer tended to complete the solution as a monologue. Largely following tutor B's approach to sequence and content, we isolated six ways of teaching the student about direction of acceleration.
656	Assumes Prior Knowledge	Abstract  This paper describes a morphological nalyzer which, when pars-  ing a word, uses two sets of rules: rnles describing the syntax of  words, and rules describing facts about orthography.
244	Explains Technical Concepts	The pivot-based strategies at both system  and model levels have been explored in ma- chine translation. Bertoldi et al (2008) studies  two pivot approaches for phrase-based statis- tical machine translation. One is at system lev- el and one is to re-construct source-target data  and alignments through pivot data. Cohn and  Lapata (2007) explores how to utilize multilin- gual parallel data (rather than pivot data) to  improve translation performance. Wu and  Wang (2007, 2009) extensively studies the  model-level pivot approach and also explores  how to leverage on rule-based translation re- sults in pivot language to improve translation  performance. Utiyama and Isahara (2007)  compares different pivot approaches for  phrase-based statistical machine translation.  All of the previous work on machine transla- tion works on phrase-based statistical machine  translation. Therefore, their translation model  is to calculate phrase-based conditional proba- bilities at unigram level (        ) while our  transliteration model is to calculate joint trans- literation unit-based conditional probabilities  at bigram level (                   ).
350	Explains Non-Technical Concepts	In this paper we present a variation of tiffs  approach. Instead of extracting the most fi'equent  word list of the training corpus, we use as style  markers the fi'equencies of occurrence of the  most fi'equent words of the entire written  language. For English, the most frequent words  of the written language component of the Brilisq7  National Co,7ms are considered. We show that  our approach performs better than the Burrows'  original method without aking into account any  of the above restrictions. Moreover, we show  that the frequencies of occurrence of the most  fi'equent punctuation marks contain very useful  stylistic information that can enhance the  performance ofan automatic text genre detector.  The paper is organized as follows. The next  section describes both the corpora nsed in tiffs  study and the procedure of extracting the most  li'equent word lists. Section 2 includes the text  genre detection experiments while section 3  contains experiments dealing with the role of  punctuation marks. Finally, in the last section  some conclusions are drawn and future work  directions are given.
77	Explains Technical Concepts	2.4 Pair Scan After collecting file names for each candidate site, the next task is to determine the parallel pairs. Again, we try to use some heuristic rules to guess which files may be parallel texts before downloading them. The rules are based on external features of the documents. By external feature, we mean those features which may be known without analyzing the contents of the file, such as its URL, size, and date. This is in contrast with the internal features, such as language, character set, and HTML structure, which cannot be known until we have downloaded the page and analyzed its contents.
299	Explains Technical Concepts	2.5.2 Language and  Character Set It is also obvious that the two files of a pair have to be in the two languages of interest. By auto- matically identifying language and character set, we can filter out the pairs that do not satisfy this basic criterion. Some Web pages explicitly indicate the language and the character set. More often such information is omitted by authors. We need some language identification tool for this task. SILC is a language and encoding identification system developed by the RALI laboratory at the University of Montreal. It employs a probabilistic model estimated on tri-grams. Using these mod- els, the system is able to determine the most proba- ble language and encoding of a text (Isabelle et al, 1997).
699	Explains Non-Technical Concepts	3 Generated  Corpus  and Trans la t ion Mode l  Tra in ing In this section, we describe the results of our parallel text mining and translation model training. 3.1 The Corpus Using the above approach for Chinese-English, 185 candidate sites were searched from the domain hk. We limited the mining domain to hk because Hong Kong is a bilingual English-Chinese city where high quality parallel Web sites exist. Because of the small number of candidate sites, the host crawler was used to thoroughly explore each site. The resulting cor- pus contains 14820 pairs of texts including 117.2Mb Chinese texts and 136.5Mb English texts. The entire mining process lasted about a week. Using length comparison and language identification, we refined the precision of the corpus to about 90%. The preci- sion is estimated by examining 367 randomly picked pairs.
974	Explains Non-Technical Concepts	Other more advanced techniques of phrase ex-  traction, including extended N-grams and syn-  tactic parsing, attempt to uncover "concepts",  which would capture underlying semantic uniformity  across various surface forms of expression. Syntac-  tic phrases, for example, appear reasonable indi-  cators of content, arguably better than proximity-  based phrases, since they can adequately deal with  word order changes and other structural variations  (e.g., "college junior" vs. "junior in college" vs. "ju-  nior college"). A subsequent regularization process,  1in a vector-space model term weights are represented  as coordinate values; in a probabilistic model estimates  of prior probabilities are used.
434	Explains Non-Technical Concepts	3.2.2 Case grammar and case frames  Case grammar \[7\] is widely used in natural  language processing systems. It is also useful in Kana-  Kanji translation because it can be applied to homonym  selection as well as to syntactic analysis. When used for  this purpose, the case frame must have a high resolving  power so that it can distinguish a correct sentence from  among many ambiguous sentences. The way in which the  new approach achieves high resolving power in case  frames can be summerized as follows:  (1) Detailed meaning description in case frames.  Each slot in a case frame has a list of meaning codes  that fit for each case. The meaning codes are written  in the lowest level of the meaning system except when  higher meaning codes are preferable. In special cases,  such as when an ideomatic expression is required for a  case slot, a word itself is written instead of the  meaning code.  (2) Rank specification of cases.  Cases are classified into either obligatory or optional  cases.  (3) Multi-case frames for each verb.  A case frame is provided corresponding to each usage  of a verb.  A case frame dictionary of 4,600 verbs was  developed for this system.
67	Assumes Prior Knowledge	In addition, the fact that the objects being retrieved were images greatly simplified the endeavor. Under normal circumstances, developing a user-friendly interface is a major challenge. Users with only limited (or nonexistent) reading knowledge of the language of the documents need a way to determine, first, which ones are useful, and second, what they say. In the PictureQuest application, however, the retrieved assets are images. Users can instantly assess which images meet heir needs.
89	Assumes Prior Knowledge	However, while for language models we have a effective method to find the interpolation weights (optimizing perplexity on a development set), we do not have such a method for the translation model. Thus, we simply recycle the weights we obtained from language model interpolation (ex- cluding the weighting for monolingual corpora).
541	Explains Non-Technical Concepts	4 An  Exper iment The TDMT system for English-to-Japanese at the time Of the experiment had about 1,500 transfer ules and 8,000 transfer dictionary en- tries. In other words, this TDMT system was capable of translating 8,000 English words into Japanese words. About 300 transfer ules and 40 transfer dictionary entries were modified to improve the level of "politeness."
154	Explains Non-Technical Concepts	3. TRAIN ING ALGORITHMS  In this section we first review properties of the SSM  and then describe the training algorithm used for tied  mixtures with the SSM. Next, we describe an effi-  cient method for training context-dependent models,  and lastly we describe a parallel implementation f the  trainer that greatly reduces experimentation time.
674	Explains Technical Concepts	2.2 System Architecture The architecture of the system is shown in Figure 1. The system was designed in a manner such that it could be easily ported from one application to another with minimal effort other than providing the domain-specific knowledge regarding the new application. Therefore, we decided to abstract away the domain-specific information into self-contained modules while keeping the other modules completely independent. The domain-specific modules are shown in the dark shaded boxes in Figure I.
498	Explains Technical Concepts	2.1 Bilingual data Our system was developed in two stages. First, a baseline system was built to generate automatic translations of some of the monolingual data avail- able. These automatic translations may be used directly with the source texts to build additional bitexts, or as queries of an Information Retrieval (IR) system to extract new bitexts from compara- ble corpora. In a second stage, these additional bilingual data were incorporated to the system (see Section 4 and Tables 1 and 2).
484	Mathematically-Oriented Paragraph	The probability of a level tag having value x increases as x increases from 1 to N . We set a probability threshold Q and choose the smallest level tag value x with probability P (tl = x) ? Q as the level tag for a word. If P (tl = N) < Q, we set the level tag to 0 and do not prune the column or diagonal. The threshold value determines a bal- ance between pruning power and accuracy, with a higher value pruning more cells but increasing the risk of incorrectly pruning a cell. During devel- opment we arrived at a threshold value of 0.8 as providing a suitable compromise between pruning power and accuracy.
952	Explains Non-Technical Concepts	2.3 Host Crawling A host crawler is slightly different from a Web crawler. Web crawlers go through innumerable pages and hosts on the Web. A host crawler is a Web crawler that crawls through documents on a given host only. A breadth-first crawling algorithm is applied in PTMiner as host crawler. The principle is that when a link to an unexplored ocument on the same site is found in a document, it is added to a list that will be explored later. In this way, most file names from the candidate sites are obtained.
692	Explains Technical Concepts	5.2. Known Microphone Adaptation  We decided to attack the problem of accomodating an unknown  microphone by considering another problem that seemed simpler  and more generally useful. It would be very useful to be able  to adapt a system trained on one microphone so that it works  well on another particular microphone. The microphone would  not have been known at the time the HMM training data was  collected, but it is known before it is to be used. In this case,  we can collect a small sample of stereo data with the microphone  used for training and the new microphone simultaneously. Then  using the stereo data we can adapt the system to work well on  the new microphone.
199	Explains Technical Concepts	This problem can be solved by introducing the  concept of a statistical model of a morpheme chain.  Statistical research in this area \[3\] indicates that  compound words have some distinct morphological  characteristics:  (1) Part of speech: about 90% of morphemes in compound  words are nouns or their prefixes or suffixes.  (2) Word category: about 7'7% of all morphemes are words  of foreign origin (Chinese).  (3) Length: About 93% of compound words are 3 to 5 Kanji  in length.
254	Explains Technical Concepts	Makino and Kizawa \[1\] proposed an automatic  Kana-Kanji translation system in which these two types  of ambiguity are treated separately in different ways: The  segmentation of input sentences is carried out  heuristically by the longest string-matching method of  two "Bunsetsu". A Bunsetsu is a Japanese syntactic unit  which usually consists of an independent word followed by  a sequence of dependent words. After determining the  segmentation of a sentence, suitable words are selected  from the homonym set based on a syntactic and semantic  analysis. In their approach, the ambiguity of the  segmentation is treated without using syntactic and  semantic analysis.
39	Explains Technical Concepts	Our representation f the lexicon at the lexical  level (as opposed to conceptual) is similar to the  one found in RealPro. Figure 4 shows a  specification for the lexeme SELL. This lexeme  is defined as a verb of regular morphology with  two lexical-structural mappings, the first one  introducing the preposition TO for its 3 r? actant,  and the preposition FOR for its 4 th actant: (a  seller) X1 sells (merchandise) X2 to (a buyer)  X3 for (a price) X4. What is important is that  each mapping specifies a transformation  between structures at different levels of  representation but that are represented in one  and the same representation formalism (DSyntS  and SSyntS in this case). As we will see  below, grammar ules are also expressed in a  similar way.  LEX~ME: SELL  CATEGORY:  verb   FEATURES:  \[ \]  GOV-PATTERN: \ [   DSYNT-RULE:   SELL ( I I I  $X3 )  <- ->  SELL  ( complet ive2  TO  ( p repos i t iona l  $X3 ) )  DSYNT-RULE :  SELL ( IV $X4 )  <- ->  SELL  ( complet ive3  FOR  ( p repos i t iona l  $X4 )  \]  MORPHOLOGY:  \[  ( \[ tense :past  \] so ld  \[ inv  ( \[ mood:past -par t  \] so ld  \[ inv  ( \[ \] sel l  \[ reg  \]
207	Assumes Prior Knowledge	Because the segmenter may not be sufficiently  accurate, we-actually use characters in addition to  short-words for both query and document  representation. Earlier work has used word  segmentation on queries only and rely on character  representation for documents with operators to  combine characters for matching query words \[8\].  The blind Chinese retrieval results in both TREC 5  and 6 showed that our short-word plus character  indexing method works very well, since we have  returned the best automatic retrieval evaluations for  both years \[10,11\]. It also demonstrates that the  PIRCS retrieval model can handle both English and  Chinese languages equally good. After the blind  TREC5 experiment, we further optimize parameters in PIRCS such as sub-document size, number of  documents and number of terms to use for 2 nd stage  retrieval to obtain better esults \[12\] as shown in Table  3.
384	Explains Non-Technical Concepts	We describe its different components in Sec- tion 2. Section 3 reports our experiments to sub- sample the available out-of-domain corpora in or- der to adapt the translation models to the news domain. Section 4, dedicated to post-processing, presents how N-best lists are reranked and how the French 1-best output is corrected by a grammatical checker. Section 5 studies how the original source language of news acts upon translation quality. We conclude in Section 6.
815	Explains Technical Concepts	We selected features that, according to linguistic  studies, as mudl  as possible reflect the basic condi-  tions governing word order. The rightmost column  in Tables 3 and 4 shows the extent o which each con-  dition contributes to improving the agreement rate.  However, each category of features might be rougher  than that which is linguistically interesting. For ex-  ample, all case markers uch as "wa" and "wo" were  classified into the same category, and were deleted  together in the experiment when single categories  were removed. An experiment that considers each  of these markers eparately would help us verify the  importance of these markers separately. If we find  new features in future linguistic research on word or-  der, the experiments lacking each feature separately  would help us verify their importance in the same  manner.
886	Explains Technical Concepts	3.3 Comparison with Previous Work   Almost all previous work on machine translite- ration focuses on direct transliteration or trans- literation system combination. There is only  one recent work (Khapra et al, 2010) touching  this issue. They work on system-based strategy  together with CRF. Compared with their work,  this paper gives more formal definitions and  derivations of system-based strategy from  modeling and decoding viewpoints based on  the JSCM model.
767	Assumes Prior Knowledge	1. INTRODUCTION  With the continuing rowth of online information, it  has become increasingly important to provide improved  mechanisms to find information quickly. Conventional  IR systems rank and assimilate documents based on  maximizing relevance to the user query \[1, 8, 6, 12, 13\].  In cases where relevant documents are few, or cases  where very-high recall is necessary, pure relevance  ranking is very appropriate. But in cases where there is  a vast sea of potentially relevant documents, highly  redundant with each other or (in the extreme) containing  partially or fully duplicative information we must utilize  means beyond pure relevance for document ranking.  In order to better illustrate the need to combine  relevance and anti-redundancy, consider a reporter or a  This research was performed as part of Carnegie  Group Inc.'s Tipster III Summarization Project under  the direction of Mark Borger and Alex Kott.  student, using a newswire archive collection to research  accounts of airline disasters. He composes a well-  though-out query including "airline crash", "FAA  investigation", "passenger deaths", "fire", "airplane  accidents", and so on. The IR engine returns a ranked  list of the top 100 documents (more if requested), and  the user examines the top-ranked ocument. It's about  the suspicious TWA-800 crash near Long Island. Very  relevant and useful. The next document is also about  "TWA-800", so is the next, and so are the following 30  documents. Relevant? Yes. Useful? Decreasingly so.
852	Explains Non-Technical Concepts	Conc lus ion   In this paper we presented a methodology for  detecting automatically the text genre of  unrestricted text. We followed the main idea of a  stylometric approach originally proposed for  attributing authorship, which uses as style  markers the fi'equencies of occurrence of the  most fi'equent words of a certain training corpus.  I n  order to improve the accuracy of this model  various additional restrictions have been  proposed (see the introduction), which in general  complicate the computational processing ot' the  texts.
331	Explains Technical Concepts	The data are represented in EMU (Cassidy &;  Harrington, 1996), a database system for stor-  ing speech data that provides for a nmltiple-  tier analysis of acoustic (e.g., F0 contour and  speech wavetbrm) and phonological (segmental  and suprasegmental) features. We present he  major differences and commonalities between  ToBI and SFO (See. 2). On the basis of the  corpus analysis, we identify matches between  the tunes assmned by Halliday and unique se-  quences of To\]~I tones (See. 4). We conclude  with a smmnary and a sketch of future work.
503	Assumes Prior Knowledge	   To deal with the problem, we propose a novel  method to mine features, which consists of two  steps: feature extraction and feature ranking.  For feature extraction, we still adopt the double  propagation idea to populate feature candidates.  But two improvements based on part-whole re- lation patterns and a ?no? pattern are made to  find features which double propagation cannot  find. They can solve part of the recall problem.  For feature ranking, we rank feature candidates  by feature importance.
357	Explains Technical Concepts	Compared to its predecessors (Fog, LFS,  JOYCE), our approach as obvious advantages  in uniformity, declarativity and portability. The  framework has been used in a wider variety of  domains, for more languages, and for more  applications (NLG as well as MT). The  framework uses the same engine for all the  transformations at all levels because all the  syntactic and conceptual structures are  represented asdependency tree structures.  In contrast, the predecessor systems were not  designed to be rapidly portable. These systems  used programming languages or scripts for the  implementation f the transformation rules, and  used different ypes of processing at different  levels of representation. For instance, in LFS  conceptual structures were represented as  graphs, whereas syntactic structures were  represented as trees which required different  types of processing at these two levels.  Our approach also has some disadvantages  compared with the systems mentioned above.  Our lexico-structural transformations are far  less powerful than those expressible using an  arbitrary programming language. In practice,  the formalism that we are using for expressing  the transformations is inadequate for long-range  phenomena (inter-sentential or intra-sentential),  including syntactic phenomena such as long-  distance wh-movement and discourse  phenomena such as anaphora nd ellipsis. The  formalism could be extended to handle intra-  sentential syntactic effects, but inter-sentential  discourse phenomena probably require  procedural rules in order to access lexemes in  other sentences. In fact, LFS and JOYCE  include a specific module for elliptical structure  processing.
944	Explains Non-Technical Concepts	3.2.3 Pars ing algorithm  Syntactic and semantic analysis is performed  concurrently. Moreover, the homonym selection is made  simultaneously. The process is basically a pattern-  matching of paths with the case frame dictionary and is  performed as follows. A path is scanned from left to right.  Every noun Bunsetsu which depends on a verb in the path  is pushed down to a stack. Whenever a verb is  encountered during scanning, case frame matching is  carried out. Every combination of noun Bunsetsu and  case slots of the verb are tried and evaluated. The best  combination is determined using the following conditions:  (1) Coincidence of postpositions.  The postposition ofthe noun Bunsetsu must be equal to  the one for the case slot.  (2) Coincidence of meaning code.  The meaning code of the noun must be equal to the one  for the case slot. If the noun has homonyms in ABL, a  coincident homonym is selected.  (3) Occupation of obligatory case slots.  Higher occupation ofobligatory case slots is preferable.  (4) Total occupation of case slots.  To addition to the condition (3), higher total occupation  of case slots is preferable.
479	Explains Non-Technical Concepts	In addition to the identity relation, phrases in a  text which refer to parts of an entity or concept  mentioned in the query will likely provide useful  information, and therefore should be included in a  summary. Finding these relations in in general is  beyond the scope of this paper, however, our ap-  proximation of a subclass of these relations proved  helpful for a number of queries.
885	Explains Technical Concepts	QUAID performs reliably on the first five problem  categories. In comparison to these five problems,  presupposition detection is even more challenging.  For unfamiliar technical terms, for example,  QUAID reports words with frequencies below a  certain threshold. Such an elegant solution is  impossible for presuppositions. Their forms vary  widely across presupposition types. Therefore,  their detection requires a complex set of rules,  carefully tuned to identify a variety of  presupposition problems. DP prints out the  presuppositions of a question, and relies on the  survey methodologist to make the final decision  whether the presuppositions are valid.
276	Assumes Prior Knowledge	The user can define the sets WHead and WDaughter  as he wishes, or, by leaving them undefined, avoid their  effects altogether. The feature STEM is built in, and  need not be defined. The effects of the Word-Sister  Convention can be modified by changing the STEM  specifications ill the lexlcal entries, and avoided by  omitting them.
808	Assumes Prior Knowledge	Figure 5: Specification ofConcept #TEMPERATURE  Note that since each lexicon entry can have  more than one lexical-structural mapping rule,  the list of these rules represents a small grammar  specific to this lexeme or concept.  Realization grammar ules of the main grammar  include generic mapping rules (which are not  lexeme-specific) such as the DSyntS-rule  illustrated in Figure 6, for inserting a determiner.  DSYNT-RULE:   $X  \[ c lass :noun ar t i c le :de f  \]  $X  ( determinat ive  THE )
556	Explains Non-Technical Concepts	Below we compare the recognition error rate between SI and  SD recognition. The SI models were trained with 7,200 sen-  tences, while the SD were trained with only 600 sentences, each.  Two different sets of test speakers were used for the SI model,  while for the SD case, the test and training speakers were the  same, but we compare two different test sets from these same  speakers. These experiments were performed using the 5K-word  N'VP test sets, using the standard bigram language models and  also rescofing using a trigram language model.
756	Assumes Prior Knowledge	Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table. ( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story). It was decided that a text based information sys- tem should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
451	Assumes Prior Knowledge	4.1 Modifying system utterances The problem of modifying 'system' utterances can be divided into two parts: how to change and when to change. They are in some respects intertwined, but as the how-part affects the when-part more we will take this as a starting point. The 'system' provides as much relevant infor- mation as possible at once. This depends on the capabilities of the systems output modal- ities. If we have a screen or similar output device we present as much as possible which normally is all relevant information. If we, on the other hand, only have spoken output the amount of information that the hearer can inter- pret in one utterance must be considered when 1The bus time table dialogues are collected at LinkSping University and are available (in Swedish) on http://www.ida.l iu.se/~arnjo/kfb/dialoger.html distilling. The system might in such cases pro- vide less information. The principle of provid- ing all relevant information is based on the as- sumption that a computer system often has ac- cess to all relevant information when querying the background system and can also present it more conveniently, especially in a multimodal system (Ahrenberg et al, 1996). A typical ex- ample is the dialogue fragment in figure 1. In this fragment he system provides information on what train to take and how to change to a bus. The result of distilling this fragment pro- vides the revised fragment of figure 2. As seen in the fragment of figure 2 we also remove a num- ber of utterances typical for human interaction, as discussed below.
295	Explains Technical Concepts	4.2 Noun-based Event Extraction  REES currently handles only verb-based  events. Noun-based event extraction adds  more complexity because:  Nouns are often used in a generic, non-  referential manner (e.g., "We see a merger  as being in the consumer's interest"), and  When referential, nouns often refer to  verb-based events, thus requiring noun-  verb co-reference resolution ("An F-14  crashed shortly after takeoff... The crash").  However, noun-based events are crucial  because they often introduce additional key  information, as the underlined phrases below  indicate:  While Bush's meetings with prominent anti-  apartheid leaders uch as Archbishop  Desmond Tutu and Albertina Sisulu are  important...  We plan to develop a generic set of patterns for  noun-based event extraction to complement the  set of generic verb-based extraction patterns.
515	Assumes Prior Knowledge	5.3 Sample output and evaluation Figure 5 shows an example of text that can be generated by the Atlas-Andes ystem, showing an analogy-based approach to teaching this content. The operator library used to generate this text could generate a combinatorially arge number of versions of this dialogue as well as selected examples of other ways of teaching about direction of acceleration.
109	Explains Non-Technical Concepts	On the other hand, though, the creation of de- tailed syntactic structures turns out to be an ex- tremely challenging task. From the choice of the appropriate linguistic framework and the de- sign of the annotation scheme to the choice of the text source and the working protocols on the syn- cronization of the parallel development, as well as the quality assurance, none of these steps in the entire annotation procedure is considered a solved issue. Given the vast design choices, very few of the treebanking projects have made it through all these difficult annotation stages. Even the most outstanding projects have not been com- pleted without receiving criticisms.
148	Explains Technical Concepts	4.2. Discussion  When tit is extended we are considering all possible nd times  t for th.w and all possible xtensions w. When extending th  with w to obtain th' we are only interested in the value for  th'.t which gives the best value for th'.h + th'.g. For any t  and w, th'.h is easily determined via table lookup from the  right/left lattice. Furthermore the value of th'.g is given by  th.g + w_score (w, th.t+l, t). The function w_score(w,b,e)  computes the score for the word w with begin time b and end  time e.
99	Explains Technical Concepts	Due to the large size of the database, we did not attempt to clean the data. However, we did build several data structures based on the database which were used by the system. The primary data structures built were two inverted hash tables corresponding to the product, and the part description fields in the database. The inverted hash tables were built as follows: 1) Each product and part description field was split into words. 2) Stop-words (words containing no information like: a, the, an, etc.) were filtered. 3) Each remaining word was inserted as the index of the appropriate hash table with the identification number of the part being the value corresponding to the index. Therefore, for each non-stop-word word used in describing a part, the hash table contains a list of all the parts whose descriptions contained that word. Similarly, the products hash table contains a list of all parts corresponding to each product word.
262	Explains Technical Concepts	The baseline language models are trained on the target language part of the Europarl and News Commentary corpora. Additional, bigger lan- guage models were trained on monolingual cor- pora. For both systems the News corpus was used while an English language model was also trained on the even bigger Gigaword corpus.
269	Explains Technical Concepts	2.6 Output Process  At this point in the summarization process we  have a version of the document's ummary in the  original language and a version in English, both  encoded using UNICODE and in plain text format.  The Output Process stage takes these two versions  of the summary and converts the one written in the  original language to the original encoding of the  document (identified by the Language Recognition  module), then it converts the version in English  from UNICODE to "8859_1" (ISO Latin-l).  After the summaries are in the proper output  encoding, the system generates the summary in one  of the following formats: SGML, HTML, E-mail  or Plain text according to the user's specification or  to system parametrization, for example, if the sum-  marization system is being used for web delivery,  then the output format will be HTML by default.
134	Explains Technical Concepts	3 A knowledge-poor approach  In order to achieve portability we approach the  issue from a knowledge-poor perspective. Syntax-  based methods employ partial parsers which  require highly language-dependent resources  (morphological/grammatical analysis), and/or  properly tagged training corpus in order to detect  syntactic relations between sentence constituents.  On the other hand, n-gram methods operate on  large corpora and, in order to reduce  computational resources, consider as context  words only the immediately adjacent ones.  Medium-distance word context is not exploited.  Since large corpora are available only for few  domains we aimed at developing a method for  processing small or medium sized corpora  exploiting the most of contextual information, that  is, the tifll sentential context of words. Our  approach was driven by the observation that in  domain-constrained corpora, unlike fiction or  general journalese, the vocabulary is limited, the  syntactic structures are not complex and that  medium-distance lexical patterns are frequently  used to express imilar facts.
658	Explains Non-Technical Concepts	1. Int roduct ion and Overv iew  This paper describes the current state of a three-year  project aimed at the development of software for use in  handling large quantities of dictionary information within  natural language processing systems. 1 The project was  accepted for funding by SERC/Alvey commencing ill  June 1984, and is being carried out by Graeme Ritchie  and Alan Black at the University of Edinburgh and  Steve Puhnan and Graham Russell at the University of  Cambridge. It is one of three closely related projects  funded under the Alvey IKBS Programme (Natural  Language Tlleme); a parser is under development at  Edinburgh by Henry Thompson and John Phillips, and a  sentence grammar is being devised by Ted Briscoe and  Clare Grover at Lancaster and Bran Boguraev and John  Carroll at Cambridge. It is intended that the software  and rules produced by all three projects will be directly  compatible and capable of functioning in an integrated  system.
246	Assumes Prior Knowledge	Abstract  We describe a method of word segmentation i Japanese in which a broad-coverage parser selects  the best word sequence while producing a syntactic  analysis. This technique is substantially different  from traditional statistics- or heuristics-based  models which attempt o select the best word  sequence before handing it to the syntactic  component. By breaking up the task of finding the  best word sequence into the identification of words  (in the word-breaking component) and the selection  of the best sequence (a by-product of parsing), we  have been able to simplify the task of each  component and achieve high accuracy over a wide  varicty of data. Word-breaking accuracy of our  system is currently around 97-98%.
135	Explains Technical Concepts	Because cluster profiles would be important for a  query to pick the groups correctly, we have  implemented a clustering algorithm that emphasizes on  profile forming rather than the more common  similarity-matrix based methods uch as the single-link  or average-link. It is based on the iterative clustering  approach of \[6,7\]. Each sub-document of a (100) top-  ranked retrieval list, if not too long or too short, is used  as a seed to form a cluster by picking highly similar  documents that are not yet clustered. The profile from  the resulting group is further iterated until there is no  or little change in the profile. Each unclustered sub-  document is tested as a seed to form a group, but many  failed because fairly stringent conditions need to be  satisfied. After the process, there often would be left  with sub-documents that belong to no clusters. They  are lumped together as 'miscellaneous' and has its  profile formed. In a number of queries, this  'miscellaneous' cluster actually contain the most  relevant documents. This is the case because there is  not sufficient relevant documents to satisfy the group  forming criteria, or that their usage of terms are too  diverse and non-overlapping.
850	Explains Technical Concepts	2.1 ToBI   There are two tiers to the ToBI analysis, the  tonal analysis and the analysis of the strength  of the word boundaries, which is referred to as  the "break index". The Tom tones are either  high (H) or low (L). The break index gives the  strength of a word's association with the tbl-  lowing word, where 0 is the strongest perceived  conjoining and 4 is the most disjoint (Beckman  gc Ayers, 19971. In our analysis (See. 3), we  only consider the tonal part of TOBI.
158	Explains Non-Technical Concepts	5 Conclusions  A big challenge to statistical-based machine  transliteration is the lack of the training data,  esp. to those language pairs without English  involved. To address this issue, inspired by the  research in the SMT research community, we  study two pivot transliteration methods. One is  at system level while another one is at model  level. We conduct extensive experiments using  NEW 2009 benchmarking data. Experimental  results show that system-based method is very  effective in capturing the phonetic information  of source language. It not only avoids success- fully the error propagation issue, but also fur- ther boosts the transliteration performance by  generating more alternative pivot results as the  inputs of the second stage. In contrast, the  model-based method in its current form fails to  convey enough phonetic information from  source language to target language.
401	Explains Technical Concepts	4.2 Exper imenta l  Results  Both the upper l imit and the substantial level of the  accuracy of the new Kana-Kanji translation system was  determined experimentally. The upper limit of  translation accuracy was determined using a set of  benchmark texts consisting of 9 typical Japanese texts  including official documents, scientific papers and legal  documents. The total number of input characters in the  benchmark text was about ten thousand. Program errors  and data errors in dictionaries were corrected to as great  an extent as possible. The accuracy of the system using  the benchmark texts was 94.9%. Another set of Japanese  texts was prepared with twenty thousand chracters and  the translation exreriment was repeated. This time no  correction of data errors was made during the experiment.  The average accuracy was 90.5%, which is the current  level of performance ofour system.
371	Explains Technical Concepts	   For our scenario, we have three strong clues  for features in a corpus: opinion words, part- whole patterns, and the ?no? pattern. Although  all these three clues are not hard rules, there  exist mutual enforcement relations between  them. If an adjective modify many features, it is  highly likely to be a good opinion word. If a  feature candidate is modified by many opinion  words, it is likely to be a genuine feature. The  same goes with part-whole patterns, the ?no?  pattern, or the combination for these three clues.  This kind of mutual enforcement relation can be  naturally modeled in the HITS framework.
921	Assumes Prior Knowledge	It can be seen that standard 2-stage strategy  performs about 9% to 15% better than initial retrieval  using the AvPre measure as reference (TREC5.161 vs.  .140, TREC6 .240 vs..220). The other techniques  successively bring further improvements, accumulating  to about 20 to 40% over the standard 2nd stage retrieval  results (TREC5.239 vs.. 161, TREC6.289 vs..240).  It is found that collection enrichment also works for  long queries. It is an attractive technique since  searchable t xts are increasingly available nowadays.
919	Assumes Prior Knowledge	Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized to- wards BLEU using Minimum Error Rate Training as proposed in Venugopal et al (2005). 2.1 Training, Development and Test Data We used the data provided for the WMT for train- ing, optimizing and testing our systems: Our training corpus consists of Europarl and News Commentary data, for optimization we use new- stest2008 as development set and newstest2009 as test set.
960	Assumes Prior Knowledge	In the most recent chat history, there is only  one input from Lisa after the director?s interven- tion, which implied ?anger?. Since Lisa and  Mayid have a negative relationship (pre-defined  by character profiles), then we predict Mayid  currently experiences negative emotion. Since  capitalizations have been used in Mayid?s input,  we conclude that the affect implied in the input  could be ?angry?. However, EMMA could be  fooled if the affect histories of other characters  fail to provide any useful indication for predic- tion (e.g. if Lisa implied ?neutral? in the most  recent input, the interpretation of the affect con- veyed by Mayid would be still ?neutral?).   EMMA also detected affect for the 3rd, 4th,  and 5th user input in the above example (based  on individual turn-taking) until it detected ?neu- tral? again from the 6th input ?go on den (go on  then)? from Mayid. Since it is an imperative  mood sentence (a linguistic contextual indica- tor), the input may imply a potential (emotional)  response to the previous speaking character.  Since we couldn?t obtain the affect embedded in  the imperative purely based on the analysis of  the input itself, the contextual processing is re- quired. Thus the emotional context profile for  Mayid is retrieved, i.e. [angry (the 2nd input)  and angry (the 3rd input)]. The Markov chain is  used to produce the possible emotional context  based on the training data for each sub-theme  for Mayid.
377	Explains Non-Technical Concepts	Abstract  Results presented in this paper strongly  support the notion that similarities as well as  differences in language systems can be  empirically investigated by looking into the  linguistic patterns of speech repairs in real  speech data. A total of 500 Gemmn and 325  Mandarin Chinese overt immediate speech  repairs were analysed with regard to their  internal phrasal structures, with particular  focus on the syntactic and morphological  characteristics. Computational models in the  form of finite state automata (FSA) also  illustrate the describable regularity of  German and Mandarin Chinese speech  repairs in a formal way.
239	Explains Technical Concepts	Foster et al (2006) applied ideas from language model smoothing to the translation model. Good Turing smoothing (Good, 1953) uses counts of counts statistics to assess how likely we will see a word (or, in our case, a phrase) again, if we have seen it n times in the training corpus. Instead of using the raw counts, adapted (lower) counts are used in the estimation of the conditional probabil- ity distribution.
580	Explains Technical Concepts	Background  The main thrust of this project has been to demon-  strate that robust if relatively shallow NLP can help  to derive better representation of text documents  for indexing and search purposes than any simple  word and string-based methods commonly used in  statistical full-text retrieval. This was based on the  premise that linguistic processing can uncover cer-  tain critical semantic aspects of document content,  something that simple word counting cannot do,  thus leading to more accurate representation. The  project's progress has been rigorously evaluated in a  series of five Text Retrieval Conferences (TREC's)  organized by the U.S. Government under the guid-  ance of NIST and DARPA. Since 1995, the project  scope widened substantially to include several paral-  lel efforts at GE, Rutgers, Lockheed Martin Corpo-  ration, New York University, University of Helsinki,  and Swedish Institute for Computer Science (SICS).  We have also collaborated with SRI International  during TREC-6. At TREC we demonstrated that  NLP can be done efficiently on a very large scale,  and that it can have a significant impact on IR. At  the same time, it became clear that exploiting the  full potential of linguistic processing is harder than  originally anticipated.
447	Assumes Prior Knowledge	The TQL expressions consist of predicates, func- tions, constants and variables. The textual words of nouns and verbs are translated to generic predi- cates using the selected interpretation. The follow- ing question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below. Typically, the Norwegian equivalent Vet du om bussen gaar til Nidar paa soendag ? 5 gives exactly the same code. test:: % isa(real,program,tuc), % isa(real,bus,A), % isa(real,saturday,B), % isa(real,place,nidar), % event(real,D), % Type of question tuc is a program A is a real bus B isa saturday Nidar is a place D is an event know(whether,tuc,C,D), Y. C was known at D event (C , E) , Y. E is an event in C action(go,E), Y. the action of E is Go actor(A,E), Y. the actor of E is A srel(to,place,nidar,E),Y. E is to nidar srel(on,time,B,E), y, E is on the saturday B The event parameter plays an important role in the semantics. It is used for various purposes. The most salient role is to identify a subset of time and space in which an action or event occured. Both the actual time and space coordinates are connected to the actions through the event parameter.
989	Explains Non-Technical Concepts	3 Background: the Andes physics tutor Andes (Gertner, Conati and VanLehn 1998) is an intelligent tutoring system in the domain of first- year college physics. Andes teaches via coached problem solving (VanLehn 1996). In coached problem solving, the tutoring system tracks the student as the latter attempts to solve a problem. If the student gets stuck or deviates too far from a correct solution path, the tutoring system provides hints and other assistance. A sample Andes problem is shown in mid- solution in Figure 1. A physics problem is given in the upper-left corner with a picture below it. Next to the picture the student has begun to sketch the vectors involved using the GUI buttons along the left-hand edge of the screen. As the student draws vectors, Andes and the student cooperatively fill in the variable definitions in the upper-right corner. Later the student will use the space below to write equations connecting the variables.
358	Assumes Prior Knowledge	It can be seen that two-stage retrieval is good for  both English and Chinese, leading to improvements in  AvPre of some 15% to 31% (.452 vs..392 and .384 vs.  .293) over initial 1 st stage retrieval. Moreover, long  queries perform better than short ones as in English,  between 17% and 22% (.452 vs. .384 and .603 vs.  .476). These Chinese queries return surprisingly good  results even though the segmentation is approximate. It  is not clear if the language characteristics it elf may be  a factor contributing to this.
634	Assumes Prior Knowledge	4 Resu l ts   The first l)art of the study estaMished that there  is a basic eorresl)onden(:e l) tween the SFG tones  mid particular sequences of ToBI lal)els tbr the  simplest possible utterances, i.e., those consist-  ing of a tonic segment only. As can be seen from  ~l~,l)le 1, tone 1 usually corresponds to H'L-L%,  tone 2 to L 'H-H% and tone 4 to II*L-H%. a  These siml)le milts usually have one pitch a('-  cent and (;oincide with one intonation t)hrase  (:(resisting of one internmdiate 1)hrase.  In a second step, we looked at the more com-  plicated utterances, i.e., those with a pretonic  segment, and those consisting of a sequence of  tone groups. In these cases there is usually more  2The phonetieimt was aware of the Sl.'(' analysis. How-  ever, the ToBI analysis was done listening to the audio  files and looking at the pitch plots.
942	Explains Non-Technical Concepts	Information is increasingly global, and the need to access it crosses language barriers. The topic of this paper, cross-language information retrieval, concerns the automatic retrieval of text in one language via a query in a different language. A considerable body of literature has grown up around cross-language information retrieval (e.g. Grefenstette 1998, TREC-7 1999). There are two basic approaches. Either the query can be translated, or each entire document can be translated into the same language as the query. The accuracy of retrieval across languages, however, is generally not good. One of the weaknesses that plagues cross- language retrieval is that we do not have a good sense of who the users are, or how best to interact with them.
50	Explains Non-Technical Concepts	2.2 Overview  The MINDS system is a multilingual domain  independent summarization system, which is able  to summarize documents written in English, Japa-  nese, Russian and Spanish. The system is intended  to be rapidly adaptable to new language and genres  by adjusting a set of parameters. A summarization  system for Turkish has just been added to the sys-  tem. This required about one programmer day of  effort, mostly spent in preprocessing the language  resources used by the system. The types of summa-  rization information used are also intended to be  adjustable by a user "on the fly", to allow the tun-  ing of the summarizers output based on length of  summary needed, type of document structure, topic  focus.
642	Explains Technical Concepts	2.5.1 Text Length Parallel files often have similar file lengths. One sim- ple way to filter out incorrect pairs is to compare the lengths of the two files. The only problem is to set a reasonable threshold that will not discard too many good pairs, i.e. balance recall and precision. The usual difference ratio depends on the language pairs we are dealing with. For example, Chinese- English parallel texts usually have a larger differ- ence ratio than English-French parallel texts. The filtering threshold had to be determined empirically, from the actual observations. For Chinese-English, a difference up to 50% is tolerated.
186	Explains Non-Technical Concepts	Abstract This paper describes an application of APE (the Atlas Planning Engine), an integrated planning and execution system at the heart of the Atlas dialogue management system. APE controls a mixed- initiative dialogue between a human user and a host system, where turns in the 'conversation' may include graphical actions and/or written text. APE has full unification and can handle arbitrarily nested discourse constructs, making it more powerful than dialogue managers based on finite- state machines. We illustrate this work by describing Atlas-Andes, an intelligent tutoring system built using APE with the Andes physics tutor as the host.
864	Explains Non-Technical Concepts	At the test stage, our affect detection compo- nent, EMMA, is integrated with an AI agent and  detects affect for each user input solely based on  the analysis of individual turn-taking input it- self. The above algorithms for context-based  affect sensing will be activated when the affect  detection component recognizes ?neutral? from  the current input during the emotionally charged  proper improvisation after all the characters  have known each other and went on the virtual  drama stage. First of all, the linguistic indicators  are used to identify if the input with ?neutral?  implication is a contextual-based input. E.g. we  mainly focus on the checking of the five contex- tual implications we mentioned previously, in- cluding imperatives, prepositional phrases, con- junctions, simplified question sentences, charac- ter names, and other commonly used contextual  indicators (e.g. ?yeah?, ?I think so?). If any of  the above contextual indicators exists, then we  further analyze the affect embedded in the input  with contextual emotion modeling reported  here.
976	Assumes Prior Knowledge	This is a critical difference. In the traditional word-lattice  approach, many segmentations of the input speech which could  not be generated (or scored well) by the earlier-pass algorithms  will be eliminated for consideration before the advanced  algorithms are used. With progressive-search techniques, these  segmentations are implicit in the grammar and can be recovered  by the advanced techniques in subsequent recognition passes.
530	Assumes Prior Knowledge	TUC is a prototypical natural language proces- sor for English written in Prolog. It is designed to be a general purpose easily adaptable natural lan- guage processor. It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual informa- tion sources.
184	Assumes Prior Knowledge	1. INTRODUCTION  During the past year, the DARPA program has graduated from  medium vocabulary recognition problems like Resource Manage-  ment and ATIS into the large vocabulary dictation of Wall Street  Joumal (WSJ) texts. With this move comes some changes in com-  putational requirements and the possibility that the algorithms that  worked best on smaller vocabularies would not be the same ones  that work best on larger vocabularies. We found that, while the  required computation certainly increased, the programs that we  had developed on the smaller problems still worked efficiently  enough on the larger problems. However, while the BYBLOS  system achieved the lowest word error rate obtained by any site  for recognition of ATIS speech, the error rates for the WSJ tests  were the second lowest of the six sites that tested their systems on  this corpus. The reader will find more details on the evaluation  results in \[1\].
877	Explains Non-Technical Concepts	We also demonstrated that standard beam search is highly effective in increasing the speed of the CCG parser, despite the fact that the su- pertagger has already had a significant pruning effect. In future work we plan to investigate the gains that can be achieved from combining the two pruning methods, as well as other pruning methods such as the self-training technique de- scribed in Kummerfeld et al (2010) which re- duces the number of lexical categories assigned by the supertagger (leading to a speed increase). Since these methods are largely orthogonal, we expect to achieve further gains, leading to a re- markably fast wide-coverage parser outputting complex linguistic representations.
853	Explains Non-Technical Concepts	In our case, we assume that the planned dialogue system has the ability to reason on various aspects of dialogue and properties of the application. In our current work, and in the examples used for illustra- tion in this paper, we assume a dialogue model that can handle any relevant dialogue phenomenon and also an interpreter and speech recogniser being able to understand any user input that is relevant o the task. There is is also a powerful domain reason- ing module allowing for more or less any knowledge reasoning on issues that can be accomplished with- in the domain (Flycht-Eriksson, 1999). Our current system does, however, not have an explicit user task model, as opposed to a system task model (Dahlb~ick and JSnsson, 1999), which is included, and thus, we can not assume that the 'system' remembers utter- ances where the user explains its task. Furthermore, as our aim is system development we will not con- sider interaction outside the systems capabilities as relevant o include in the distilled dialogues.
631	Explains Non-Technical Concepts	4 Conclusions  We feel that further research is warranted on  improving summarization based on sentence selec-  tion and that its bad press is largely apocryphal and  unjustified. In fact from a document analysts point  of view material from the original document may  be preferable, carrying as it does, the style and tone  of the original document.
818	Explains Technical Concepts	5 Results Although we employed a typical statistical rank- ing model in our system, it is difficult to directly evaluate the absolute performance of the predicted ranking. Annotators only annotate a very small subset of the discriminants, and their order is not fully specified. To compare the behavior of mod- els trained with data annotated by different anno- tators, we plot the relative ranking (normalized to [0, 1] for each sentence, with 0 being the highest rank and 1 the lowest) of discriminants for 50 sen- tences in Figure 2.
548	Explains Non-Technical Concepts	ABSTRACT  We describe a technique we call Progressive Search  which is useful for developing and implementing speech  recognition systems with high computational requirements. The  scheme iteratively uses more and more complex recognition  schemes, where each iteration constrains the search space of the  next. An algorithm, the Forward-Backward Word-Life  Algorithm, is described. It can generate a word lattice in a  progressive search that would be used as a language model  embedded in a succeeding recognition pass to reduce  computation requirements. We show that speed-ups of more than  an order of magnitude are achievable with only minor costs in  accuracy.
861	Explains Technical Concepts	In order to evaluate the experiment, we clas- sifted the Japanese translation results obtained for the 23 unseen dialogues (199 utterances from a clerk, and 145 utterances from a customer, making 344 utterances in total) into two types: expressions that had to be changed to more po- lite expressions, and expressions that did not. Table 2 shows the number of utterances that in- cluded an expression which had to be changed into a more polite one (indicated by "Yes") and those that did not (indicated by "No"). We ne- glected 74 utterances whose translations were too poor to judge whether to assign a "Yes" or "No."
316	Explains Technical Concepts	The remainder of this section discusses each of the modules hown in the system architecture. 2.2.1 The Speech Recognition System (ASR) Since customer service centers are meant o be used by a variety of users, we needed a user- independent speech recognition system. In addition, since the system could not restrict he manner in which a user asked for service, the speech recognition system could not be grammar-based. Therefore, we used a general purpose dictation engine for the system. The dictation system used was Lernout & Hauspie's VoiceXPress ystem (www.lhs.com). Although the system was general purpose, we did provide to it the set of keywords and phrases that are commonly used in the domain thereby enabling it to better recognize these domain-specific keywords and phrases. The keywords and phrases used were simply the list of descriptions and product names corresponding to each part in the database. It should be noted that the set of domain-specific keywords and phrases was provided to the speech recognition system as a text document. In other words, the training was not done by a human speaking the keywords and phrases into the speech recognition system. In addition, the speech recognition system is far from perfect. The recognition rates hover around 50%, and the system has additional difficulty in identifying product names which are most often words not found in a dictionary (examples: 3MlaserCam, 8000BUCKY, etc.).
830	Explains Technical Concepts	Before describing how the rules should be read, it is necessary  to define two technical terms. In phonology, one speaks of under-  lying segments and surface segments; in orthography, characters  making up the words in the lexicon contrast with characters in  word forms that occur in texts. The term lezical character will  be used here to refer to a character in a word or morpheme in  tile lexicon, i.e., the analog of a phonological underlying segment.  Tile term sat\[ace character will be used to mean a character in a  word that could appear in text. For example, \[1 o v e + e d\] is a  string of lexieal characters, while \[I o v e d\] is a string of surface  characters.
987	Explains Non-Technical Concepts	3.4 The Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role. The text is translated from Natural anguage into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression con- taining the literal meaning of an utterance. A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowal- ski and Sergot, 1986).
446	Explains Non-Technical Concepts	Automatic construction of parallel English-Chinese corpus for cross-language information retrieval Abst rac t A major obstacle to the construction ofa probabilis- tic translation model is the lack of large parallel cor- pora. In this paper we first describe a parallel text mining system that finds parallel texts automatically on the Web. The generated Chinese-English paral- lel corpus is used to train a probabilistic translation model which translates queries for Chinese-English cross-language information retrieval (CLIR). We will discuss ome problems in translation model training and show the preliminary CUR results.
825	Assumes Prior Knowledge	1.1.1 The syntactic analysis component  We used Eric Brill's rule-based word tagger (1992,  1994a, 1994b), the de facto state of the art tagging  system, to break the questions down into part-of-  speech categories. Brill's tagger produces a single  lexical category for each word in a sentence by  first assigning tags based on the frequency of  occurrence of the word in that category, and then  applying a set of context-based re-tagging rules.  The tagged text was then passed on to Abney's  SCOL/CASS system (1996a, 1996b), an extreme  bottom-up parser. It is designed to avoid  ambiguity problems by applying rammar rules on  a level-by-level basis. Each level contains rules  that will only fire if they are correct with high  probability. Once the parse moves on to a higher  level, it will not attempt to apply lower-level rules.  In this way, the parser identifies chunks of  information, which it can be reasonably certain are  connected, even when it cannot create a complete  parse of a sentence.
502	Explains Technical Concepts	We developed a two-stage training process to deal with this  problem. First we train HMM models assuming there are no  pauses between words. Then we mark the missing silence lo-  cathms automatically by running the recognizer on the training  data constrained to the correct word sequence, but allowing op-  tional silence between words. Then we retrain the model using  the output of the recognizer as corrected transcriptions.  We find that this two-stage process increases the gain due to  using cross-word phonetic models. The word error was reduced  by 0.6% which is about a 5% reduction in word error.
179	Explains Non-Technical Concepts	For the future work, we plan to study how to  improve the model-based strategy by pruning  out the so-called ?bad? transliteration unit  pairs and re-sampling the so-called ?good? unit  pairs for better model parameters. In addition,  we also would like to explore other pivot- based transliteration methods, such as con- structing source-target training data through  pivot languages.
875	Explains Technical Concepts	Contrary to expectations, word segmentation is not  crucial for Chinese IR. Simple bigrams or short-word  with character indexing can produce very good results.  A manual stoplist is also unnecessary; one only needs  to screen out high frequency statistical stopwords.  Best results are obtained by combining retrievals using  multiple representations.
226	Explains Technical Concepts	The Tom intonational phonology model  aligns a tune with the words of an utterance  (cf. Harrington 8c Cassidy (1999)), wherc some  of these words are accented. The words of an  utterance are grouped into phrases. There are  two types of phrases, intonational and inter-  mediate ph, mses. Utterances always consist of  one or more intonational phrases which iu tm:n  consist of one or lnore intermediate phrases.  The break between two intonational 1)hrases is  greater than 1)etween two intermediate )hrases,  the bl'eak index being 4 in the former case and  3 or 2 in the latter.
370	Explains Technical Concepts	Automatic Topic Expansion  In TREC-7 we started experimenting with com-  pletely automated topic expansion. We used the  same approach to expansion as outlined below with  the following modifications:  1. Top 100 documents retrieved by the initial, un-  expanded topic are summarized, rather than 30  used in manual mode. This is because we need  to rely on a strict notion of topicality of the sum-  mary, and therefore must look at more documents  to obtain any expansion. From a user's perspec-  tive, this is entirely transparent, however.  2. We replace human selection of expanding sum-  maries by an automatic functions that measure  the overlap between the summary and the topic.  This overlap, measured over content terms (i.e.,  with exclusion of common words and certain other  words), should be high enough to prevent false  matches, while not too high to allow for topic vari-  ants to be matched.  . The summary parameters (i.e., length, spread,  etc) is set to normalize its size in such as way  as to support effective topicality detection. For  example, straight 10short documents (too short!)  or for very long documents (too long!).  Preliminary tests conducted using TREC-6 data  showed a significant increase in precision over un-  expanded queries, although still not as large as in  manual expansion. These experiments require con-  tinuation.
0	Explains Non-Technical Concepts	Abstract The paper describes a natural anguage based expert system route advisor for the public bus transport in Trondheim, Norway. The system is available on the Internet,and has been intstalled at the bus com- pany's web server since the beginning of 1999. The system is bilingual, relying on an internal anguage independent logic representation.
594	Explains Technical Concepts	A set of experiments compares two topologies: (1) a  topology for a fixed vocabulary for the keywords and the N most  common words in that task (N varies from Zero to Vocabulary  Size), forcing the recognition hypothesis to choose among the  allowable words (traditional CSR), and (2) a second topology in  which a background word model is added to the word list,  thereby allowing the recognition system to transcribe parts of the  incoming speech signal as background. While including the  background word model does increase the overall ikelihood of  the recognized transcription, the probability of using the back-  ground model is highly likely (due to the language model proba-  bilities of out of vocabulary words) and tended to replace a  number of keywords that had poor acoustic matches.  Finally, we introduce an algorithm for smoothing lan-  guage model probabilities. This algorithm combines mall task-  specific language model training data with large task-indepen-  dent language training data, and provided a 14% reduction in test  set perplexity.
396	Explains Technical Concepts	4.3 Statistical Evidence for News Events In this work, we introduce events as a middle- layer representation between words and  sentences under the assumptions that 1) events  are widely distributed in a text and that 2) they  are natural clusters of salient information in a  text. They guarantee the relevance of event to  our task ? summaries are condensed collections  of salient information in source documents. In order to confirm them, we scan the whole  dataset in our experiment, which consists of 42  200w human extracts and 39 400w human  extracts for the DUC 02 multi-document extract  task. Detailed information about the dataset can  be found in Section 6. Table 1 lists the statistics.
293	Assumes Prior Knowledge	8 Word Clustering  Although the obtained semantically related N-best  pair list constitutes already a thesaurus-like and  information-rich form of semantic knowledge  representation, many NLP applications (e.g.  language modeling) require word clusters instead  of word relations. However, since a word  similarity measure has been extracted, the  formation of clusters is a rather trivial problem,  although more complex for "soft clustering" (i.e. a  word can be classified in more than one classes).  In order to construct word classes we applied the  unsupervised agglomerative hard clustering  algorithn3 shown in Figure 1 over the set of  senmntic relations. Each distinct lexical item is  initially assigned to a cluster and then clusters are  merged into larger ones according to the average  linkage measure. Merging of clusters tops when  the distance between the more proximate clusters  exceeds a threshold proportional to the average  distance between words. Tracking the successive  merges we obtain sub-cluster hierarchies, uch as  the one shown in figure 2.
257	Explains Technical Concepts	In today's information retrieval, query expansion  usually is typically limited to adding, deleting or  re-weighting of terms. For example, content terms  from documents judged relevant are added to the  query while weights of all terms are adjusted in or-  der to reflect the relevance information. An alter-  native to term-only expansion is a full-text expan-  sion described in (Strzalkowski et al 1997). In this  approach, search topics are expanded by pasting in  entire sentences, paragraphs, and other sequences  directly from any text document. To make this pro-  cess efficient, an initial search is performed with the  unexpanded queries and the top N (10-30) returned  documents are used for query expansion. These  documents, irrespective of their overall relevancy to  the search topic, are scanned for passages contain-  ing concepts referred to in the query. The result-  ing expanded queries undergo further text process-  ing steps, before the search is run again. We need  to note that the expansion material was found in  both relevant and non-relevant documents, benefit-  ing the final query all the same. In fact, the presence  of such text in otherwise non-relevant documents  underscores the inherent limitations of distribution-  based term reweighting used in relevance feedback.
223	Assumes Prior Knowledge	D)r three classifiers (MBL, MaxEnt and  IGTree) we haw; used system-internal coral)i-  nation. These learning algorithms have pro-  cessed five dittbrent representations of the out-  put (IOB1, IOB2, IOE1, IOE2 and O-t-C) and  the results have been combined with majority  voting. The test data results can 1)e fimnd in  Table 1. In all cases, the combined results were  better than that of the best included system.  Tile results of ALLiS, 05.0, MB SL and SNoW  have tmen converted to the O and the C repre-  4Detailed results of our experiments me available on  http: / /lcg-www.uia.ae.be/-erikt /np('oml,i /  The retagging was necessary to assure that the per-  formance rates obtained here would be similar to rates  obtained for texts for which no Treebank POS tags are  available.
786	Assumes Prior Knowledge	Summar izat ion -based  top ic  expans ion   We used our automatic text summarizer to de-  rive query-specific summaries of documents returned  from the first round of retrieval. The summaries  were usually 1 or 2 consecutive paragraphs selected  from the original document text. The initial purpose  was to show to the user, by the way of a quick-read  abstract, why a document has been retrieved. If the  summary appeared relevant and moreover captured  some important aspect of relevant information, then  the user had an option to paste it into the query,  thus increasing the chances of a more successful sub-  sequent search. Note again that it wasn't important  if the summarized ocuments were themselves rele-  vant, although they usually were.
538	Explains Non-Technical Concepts	4.3 Unaligned Word Feature Guzman et al (2009) analyzed the role of the word alignment in the phrase extraction process. To bet- ter model the relation between word alignment and the phrase extraction process, they introduced two new features into the log-linear model. One fea- ture counts the number of unaligned words on the source side and the other one does the same for the target side. Using these additional features they showed improvements on the Chinese to English translation task. In order to investigate the impact on closer related languages like English and Ger- man, we incorporated those two features into our systems.
546	Explains Non-Technical Concepts	The basic idea in the FBS is to perform a search in the  forward direction to compute the probability of each word  ending at each frame. Then, a second more expensive search  in the backward irection can use these word-ending scores  to speed up the computation immensely. If we multiply the  forward score for a path by the backward score of another  path ending at the same frame, we have an estimate of the  total score for the combined path, given the entire utterance.  In a sense, the forward search provides the ideal fast match  for the backward pass, in that it gives a good estimate of the  score for each of the words that can follow in the backward  direction, including the effect of all of the remaining speech.  When we first introduced the FBS to speed up the N-best  search algorithm, the model used in the forward and back-  ward directions were identical. So the estimate of the back-  ward scores provided by the forward pass were exact. This  method has also been used in a best-first stack search \[8\], in  which it is very effective, since the forward-backward score  for any theory covers the whole utterance. The forward-  backward score solves the primary problem with the besst-  first search, which is that different hypotheses don't span the  same amount of speech.
696	Assumes Prior Knowledge	The forward-backward word-life algorithm achieves this  scaling property. In this new scheme, described below, the size of  the lattice is controlled by the LatticeThresh parameter.  1. A standard beam search recognition pass is done  using the early-pass peech recognition algorithm.  (None of the lattice building steps from Section 3.1  are taken in this forward pass).  2. During this forward pass, whenever a transition  leaving word W is within the beam-search, we record  that probability in ForwardProbability(W, frame).  3. We store the probability of the best scoring  hypothesis from the forward pass, Pbest, and  compute a pruning value  Pprune = Pbest I LatticeThresh.  4. We then recognize the same sentence over again  using the same models, but the recognition algorithm  is run backwards 1. 5. The lattice building algorithm described in Section  3.1 is used in this backward pass with the following  exception. During the backward pass, whenever  there is a transition between words W/and Wj at time  t, we compute the overall hypothesis probability Phyp  as the product of ForwardProbability(Wj,t-1), the  language model probability P(H~IWj), and the  Backward pass probability that W i ended at time t  (i.e. the probability of starting word W i at time t and  finishing the sentence). If Phyp < Pprune, then the  backward transition between Wi and Wj at time t is  blocked.  Step 5 above implements a backwards pass pruning  algorithm. This both greatly reduces the time required by the  backwards pass, and adjusts the size of the resultant lattice.
69	Assumes Prior Knowledge	For E-C CLIR, although queries in both lan- guages were provided, the English queries were not strictly translated from the original Chi- nese ones. For example, A Jg ,~ (human right situation) was translated into human right is- sue. We cannot expect he translation model to translate issue back to ~ (situation). ? The training source and the CLIR collections were from different domains. The Web cor- pus are retrieved from the parallel sites in Hong Kong while the Chinese collection is from Peo- ple's Daily and Xinhua News Agency, which are published in mainland China. As the result, some important erms such as ~$ $ (most- favored-nation) and --- I!! ~ ~ (one-nation-two- systems) in the collection are not known by the model.
482	Explains Non-Technical Concepts	Our system is based on a fairly vanilla Moses instal- lation and trained on data extracted from large in-house translation memories covering a range of EU docu- ments. The obtained models use 7-grams. We applied the Exodus system to this year?s WMT10 shared English-to-French translation task. As the test 8However, speed issues will have to be addressed before as the current system is not able to provide translations in real time.
294	Explains Non-Technical Concepts	Another utilization is in cross-language informa- tion retrieval (CLIR) where queries have to be trans- lated from one language to another language in which the documents are written. In CLIR, the qual- ity requirement for translation is relatively low. For example, the syntactic aspect is irrelevant. Even if the translated word is not a true translation but is strongly related to the original query, it is still help- ful. Therefore, CLIR is a suitable application for such a translation model.
6	Assumes Prior Knowledge	Abstract This paper describes our phrase-based Sta- tistical Machine Translation (SMT) sys- tem for the WMT10 Translation Task. We submitted translations for the German to English and English to German transla- tion tasks. Compared to state-of-the-art phrase-based systems we preformed addi- tional preprocessing and used a discrim- inative word alignment approach. The word reordering was modeled using POS information and we extended the transla- tion model with additional features.
933	Explains Technical Concepts	4 Example-based Acquisition  4.1 Ob ject ive   Consider a situation where the developer has found  a salient text segment and proceeds to extend the  IE system to extract he proper information from it.  Figure 3 shows a (paraphrased) text segment from the  MUC-6 development corpus, with the corresponding  extracted event, in the form of a database record. We  will use this example to illustrate our methodology.  In our earlier system (as in most other IE systems),  upon finding a candidate xample, the developer had  to construct a pattern capable of capturing the ex-  ample. Such a pattern consists of two parts:  ? the precondition, which seeks to match an active  clause beginning with a np of type "company",  followed by a verb group (vg) of class "appoint",  followed by a np of class "person", etc.;  ? the action which fires when the pattern matches,  and prescribes the operations to be performed on  the sentence fragments and the logical form.  Figure 4 shows an excerpt from the pattern code; it  is written in Common Lisp, with the precondition  specified using a special "pattern language". Clearly,  this method of development is quite time-consuming  and error-prone.
752	Explains Technical Concepts	Similarly, the user feld Is unexploited, being occupied  in all cases by the atom 'nil'. It serves primarily as a  place-holder, in that, while it is desirable to maintain  the possibility for users to include in an entry whatever  additional information they desire, the form which that  Information might take in practice is clearly not predict-  able.
55	Explains Non-Technical Concepts	We see significant opportunities in carrying out  further research to develop and integrate language  processing and other intelligent echniques uch as  those described above. One particularly challeng-  ing type of document is the HTML pages found on  the web. Here techniques to identify coherent sec-  tions of text are required as well as methods for  summarizing tables and groups of frames.
595	Assumes Prior Knowledge	ABSTRACT  In this paper we describe and compare the performance of a series  of cepstrum-based procedures that enable the CMU SPHINX-II  speech recognition system to maintain a high level of recognition  accuracy over a wide variety of acoustical environments. We  describe the MFCDCN algorithm, an environment-independent  extension of the efficient SDCN and FCDCN algorithms devel-  oped previously. We compare the performance of these algorithms  with the very simple RASTA and cepstral mean normalization  procedures, describing the performance of these algorithms inthe  context of the 1992 DARPA CSR evaluation using secondary  microphones, and in the DARPA stress-test evaluation.
888	Explains Non-Technical Concepts	The next problem concerns the case when 'system' utterances are changed or removed. ? Dialogue contributions provided by something or someone other than the user or the 'system' are removed. These are regarded as not being part of the interaction. This means that if some- one interrupts the current interaction, say that the telephone rings during a face-to-face inter- action, the interrupting interaction is normally removed from the corpus. Furthermore, 'system' interruptions are re- moved. A human can very well interrupt anoth- er human interlocuter, but a computer system will not do that. However, this guideline could lead to problems, for instance, when users follow up such interrup- tions. If no information is provided or the in- terrupted sequence does not affect the dialogue, we have no problems removing the interruption. The problem is what to do when information from the 'system' is used in the continuing dia- logue. For such cases we have no fixed strategy,
475	Explains Technical Concepts	Query and Document Representations  We used the following procedures to process the  queries and documents into tbrms that enabled  application of matching formulae to produce  relevance scores:  Document Segmentation. We used either the  original document segmentation from the TREC data  or a more aggressive segmentation that split  compound ocuments into their components.  Stop Word Removal. For all but one retrieval  system, we removed stopwords.  Stemming. For the various retrieval systems, we  used the Xerox stemmer, the Stone stemmer, or we  obtained word roots as a byproduct of constructing  trigrams.  Phrase Reco,~nition. For some retrieval systems, we  used a set of part-of-speech-based rules to detect and  aggregate sequences of tokens into compound  nominal phrases.  Proper Nouns. For some retrieval systems, we  detected proper nouns, and normalized multiple  expressions of the same proper noun entity to a  canonical form.
632	Assumes Prior Knowledge	For WMT 2010, we crafted a compromise  with the best properties of PA, yet alowing for a  more aggressive search in more directions. We  start with PA. As long as PA is adding new di- rection vectors, it is continued. When PA stops  adding new directions, random rotation (ortho- gonal transformation) of the coordinates is per- formed and PA is restarted in the new space. PA  almost always fails to introduce new directions  within the new coordinates, then fails again, so  another set of random coordinates is chosen. This  process repeats until convergence. In future  work, we will look at incorporating random res- tarts into the algorithm as additional insurance  against premature convergence.
62	Explains Non-Technical Concepts	9.2 Query Expansion  We expanded the original queries by: (1) adding  the highest ranked sentence of the document (a form  of pseudo-relevance feedback), (2) adding the title,  and (3) adding the title and the highest ranked  sentence.
215	Assumes Prior Knowledge	The proposed approach lneets the current rends  in natural anguage processing since:  ? it is able to deal with unrestricted text,  ? it requires minimal computational cost,  ? it is not based on specifc characteristics of a  certain domain/language.
535	Explains Technical Concepts	Let Sm and S, be two sentences that undergo the  cross-correlation procedure. If 8~={dx, x=l..xl,  xl>l }, is the set of word distances that satisfy the  equality: ci,,, , (d, ,  Wy) = c./.,, (d,-, Wy ) = 1, then the  pair (wi,wi) is stored as a hit accompanied by the  l'ollowing context similarity measure:  .v 1 .v 1  Keeping only the first term we obtain the same  result as in the WCSE method with weight  function h(d) =l/\]d\[. The second term augments the  score in proportion to the cohesion and the size o1'  the: detected pattern depending on the position of  wi (or, equivalently, wi). I)ividing (6) by the total  length of S,,, and S. (i.e. 1~.,,, =L,c?I.,, ) we obtain a  normalized measure of the cross-correlation f the  two sentences:  1,,,,,, (lv,, w.i )  F ",,,,,, (u,,, ~,j) : (7)  L  I I I l l   The total similarity measure is obtained from:  (8)  m n~n  applied throughout the corpus.
182	Explains Non-Technical Concepts	Let us consider the Japanese honorific title "sama" or "san." If the heater's gender is male, then it should be translated "Mr." and if the hearer's gender is female, then it should be translated "Ms." as shown in Figure 7. Ad- ditionally, the participant's gender is useful for translating typical expressions for males or fe- males. For example, Japanese "wa" is often at- tached at the end of the utterance by females. It is also important for a dialogue translation system to use extra-linguistic information which the system can obtain easily, in order to make a conversation proceed smoothly and comfort- ably for humans using the translation system. We expect hat other pieces of usable informa- tion can be easily obtained in the future. For example, age might be obtained from a cellular telephone if it were always carried by the same person and provided with personal information. In this case, if the system knew the hearer was a child, it could change complex expressions into easier ones.
490	Explains Technical Concepts	     A part-whole pattern indicates one object is  part of another object. For the previous example  ?There is a valley on my mattress?, we can find  that it contains a part-whole relation between  ?valley? and ?mattress?. ?valley? belongs to  ?mattress?, which is indicated by the preposi- tion ?on?. Note that ?valley? is not actually a  part of mattress, but an effect on the mattress. It  is called a pseudo part-whole relation. For sim- plicity, we will not distinguish it from an actual  part-whole relation because for our feature min- ing task, they have little difference. In this case,  ?noun1 on noun2? is a good indicative pattern  which implies noun1 is part of noun2. So if we  know ?mattress? is a class concept, we can infer  that ?valley? is a feature for ?mattress?. There  are many phrase or sentence patterns  representing this type of semantic relation  which was studied in (Girju et al 2006). Beside  part-whole patterns, ?no? pattern is another im- portant and specific feature indicator in opinion  documents. We introduce these patterns in de- tail in Sections 3.2 and 3.3.
654	Assumes Prior Knowledge	The validity of the results based on analysing dis- tilled dialogues depends part ly on how the distilla- tion has been carried out. Even when using natural dialogues we can have situations where the interac- tion is somewhat mysterious, for instance, if some of the dialogue participants behaves irrational such as not providing feedback or being too elliptical. How- ever, if careful considerations have been made to stay as close to the original dialogues as possible, we be- lieve that distilled dialogues will reflect what a hu- man would consider to be a natural interaction.
459	Explains Technical Concepts	Figure 1: Transfer ule format A transfer ule consists of a source pattern, a target pattern, and a source example. The source pattern consists of variables and con- stituent boundaries (Furuse and Iida, 1996). A constituent boundary is either a functional word or the part-of-speech of a left constituent's last word and the part-of-speech of a right con- stituent's first word. In Example (1), the con- stituent boundary IV-CN) is inserted between "accept" and "payment," because "accept" is a Verb and "payment" is a Common Noun. The target pattern consists of variables that cor- respond to variables in the source pattern and words of the target language. The source exam- ple consists of words that come from utterances referred to when a person creates transfer ules (we call such utterances closed utterances).
636	Assumes Prior Knowledge	Usually, one can pass N-best lists between dif- ferent stages in pipeline architectures, and this of- ten gives useful improvements (Hollingshead and Roark, 2007). However, effectively making use of N-best lists often requires lots of engineering and human effort (Toutanova, 2005). On the other hand, one can record the complete distribution at each stage in a pipeline, to compute or approxi- mate the complete distribution at the next stage. Doing this is generally infeasible, and this solu- tion is rarely adopted in practice. One promising way to tackle the problem of er- ror propagation is to explore joint learning which integrates evidences from multiple sources and captures mutual benefits across multiple compo- nents of a pipeline for all relevant subtasks simul- taneously (e.g., (Toutanova et al, 2005), (Poon and Domingos, 2007), (Singh et al, 2009)). Joint learning aims to handle multiple hypotheses and uncertainty information and predict many vari- ables at once such that subtasks can aid each other to boost the performance, and thus usually leads to complex model structure. However, it is typ- ically intractable to run a joint model and they sometimes can hurt the performance, since they increase the number of paths to propagate errors. Due to these difficulties, research on building joint approaches is still in the beginning stage.
476	Explains Technical Concepts	2 Mean ing  Format ion  in Sentences  Semantically, a text is regarded as a specification (denotation)  of a series of propositions. In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as noun groups, infinitive phrases  and embedded sentences. Thus a single sentence may express  several propositions. The goal in understanding a text is to  extract its propositions and specify them in a formal anguage.  A sentence can be characterized as the basic independent  structure in the language. Relating language to formal logic,  the meaning of a sentence can be described by a predicate  which is identified by the head verb of the sentence. The  arguments of the predicate are denoted by the constituents  of the sentence. Such a representation is the basis for both  systemic (Winograd 1983), c&se (Fillmore 1968) and lexical-  functional (Bresnan 1981) grammars.
139	Assumes Prior Knowledge	5.2 Structure of human tutorial dialogues In an earlier analysis (Kim, Freedman and Evens 1998) we showed that a significant portion of human-human tutorial dialogues can be modeled with the hierarchical structure of task-oriented dialogues (Grosz and Sidner 1986). Furthermore, a main building block of the discourse hierarchy, corresponding to the transaction level in Conversation Analysis (Sinclair and Coulthard 1975), matches the tutoring episode defined by VanLehn et al (1998). A tutoring episode consists of the turns necessary to help the student make one correct entry on the interface. NLU (CARMEL) Plan Library User APE < Interface I I I GUI Transient Interpreter Knowledge (Andes) Base Host (Andes)
616	Explains Technical Concepts	For microphone adaptation, we assume we have the VQ index  of the cepstmm of the Sennheiser signal, and the cepstrum of  the alternate microphone. Given this stereo data, we accumulate  the mean and variance of the cepstra of the alternate microphone  of the frames whose Sennlaeiser data falls into each of the bins  of the VQ codebook. Now, we can use this to define a new set  of Gaussians for data that comes from the new microphone. The  new Ganssians have means that are shifted relative to the original  means, where the shift can be different for each bin. In addition,  the variances are typically wider for the new microphone, due  to some nondeterminisfie differences between the microphones.  Thus the distributions typically overlap more, but only to the  degree that they should. The new set of means and variances  represents a codebook transformation that accomodates the new  microphone.
454	Assumes Prior Knowledge	Figure 3 illustrates the mapping between an  interlingua defined as a ConcS and a  corresponding English DSyntS. This example,  also taken from MeteoCogent, illustrates that the  conceptual interlingua in NLG can be closer to a  database representation f domain data than to  its linguistic representations.
75	Explains Technical Concepts	5.2. Implementing recognition schemes that  cannot be implemented with a standard  approach.  We have implemented a trigram language model on our  5,000-word recognition system. This would not be feasible using  standard decoding techniques. Typically, continuous-speech  trigram language models are implemented ither with fastmatch  technology or, more recently, with N-best schemes. However, it  has been observed at BBN that using an N-best scheme (N=100)  to implement a trigram language model for a 20,000 word  continuous peech recognition system may have significantly  reduced the potential gain from the language model. That is,  about half of the time, correct hypotheses that would have had  better (trigram) recognition scores than the other top-100  sentences were not included in the top 100 sentences generated  by a bigram-based recognition system\[8\].  We have implemented trigram-based language models  using word-lattices, expanding the finite-state network as  appropriate ounambiguously represent contexts for all trigrams.  We observed that the number of lattice nodes increased by a  factor of 2-3 and the number of lattice arcs increased by a factor  of approximately 4 (using lattices generated with beam widths of  le-38 and a LatticeThresh of le-18). The resulting decoding  times increased approximately by 50% when using trigram  lattices instead of bigram lattices.
145	Explains Technical Concepts	We used a document compression factor based on  the number of characters in the document. If this  cutoff fell in the middle of a sentence the rest of the  sentence was allowed, thus the output summary ends  up being slightly longer than the actually compression  factor.
422	Explains Non-Technical Concepts	6 Conclusions For our participation in the WMT 2010 we built translation systems for German to English and En- glish to German. We addressed to the difficult word reordering when translating from or to Ger- man by using POS-based reordering rules during decoding and by using lattice-based phrase extrac- tion during training. By applying those methods we achieved substantially better results for both translation directions.
474	Explains Technical Concepts	One way to view query expansion is to make the  user query resemble more closely the documents it is  expected to retrieve. This may include both content,  as well as some other aspects uch as composition,  style, language type, etc. If the query is indeed made  to resemble a "typical" relevant document, then sud-  denly everything about this query becomes a valid  search criterion: words, collocations, phrases, var-  ious relationships, etc. Unfortunately, an average  search query does not look anything like this, most  of the time. It is more likely to be a statement speci-  fying the semantic riteria of relevance. This means  that except for the semantic or conceptual resem-  blance (which we cannot model very well as yet)  much of the appearance of the query (which we can  model reasonably well) may be, and often is, quite  misleading for search purposes. Where can we get  the right queries?
707	Assumes Prior Knowledge	The BYBLOS speech recognition system uses a multi-pass  search strategy designed to use progressively more detailed mod-  els on a correspondingly reduced search space. It produces an  ordered list of the N top-scoring hypotheses which is then re-  ordered by several detailed knowledge sources.  1. A forward pass with a bigram grammar and discrete HMM  models saves the top word-ending scores and times \[6\].  2. A fast time-synchronous backward pass produces an inital  N-best list using the Word-Dependent N-best algorithm\[5\].  3. Each of the N hypotheses is rescored with cross-word-  boundary triphones and semi-continuous density HMMs.  4. The N-best list can be rescored with a trigram grammar (or  any other language model).
124	Assumes Prior Knowledge	We used most of these setting in our submission last year (Koehn and Haddow, 2009). The main difference to our baseline system from the submission from last year is the use of ad- ditional training data: larger releases of the News Commentary, Europarl, Czeng, and monolingual news corpora. The first two parallel corpora in- creased roughly 10-20% in size, while the Czeng parallel corpus and the monolingual news corpora are five times and twice as big, respectively. We also handled some of the corpus preparation steps with more care to avoid some data incon- sistency problems from last year (affecting mostly the French language pairs).
15	Explains Non-Technical Concepts	ABSTRACT  The word-spotting task is analogous to text-based infor-  marion retrieval tasks and message-understanding tasks in that an  exhaustive accounting of the input is not required: only a useful  subset of the full information eed be extracted in the task. Tradi-  tional approaches have focussed on the keywords involved. We  have shown that accounting for more of the data, by using a  large-vocabulary recognizer for the wordspotting task, can lead  to dramatic improvements relative to traditional approaches.  This result may well be generalizable tothe analogous text-based  tasks.
697	Assumes Prior Knowledge	The annotators also claim that the speed-up is somewhat diminished over the ?rejected? sen- tences, for which none of the candidate trees are acceptable. In such cases, the annotators still have to go through a long sequence of discriminants, and sometimes have to redo the previous steps in fear of the chain-effect of wrong decisions. How to compensate for the psychological insatisfaction of rejecting all analyses while maintaining good annotation speed and quality is a new topic for our future research.
433	Assumes Prior Knowledge	5 App ly ing  the  method To illustrate the method we will in this section try to characterise the results from our distillations. The illustration is based on 39 distilled dialogues from the previously mentioned corpus collected with a telephone operator having information on local bus time-tables and persons calling the information ser- vice.
586	Explains Technical Concepts	Is a DIP system feasible? At present, it is  difficult for NLP systems to use information from  context in the evaluation of a statement. What is  required to solve this problem is a mechanism that  determines whether a presupposed entity (an  object, an activity, an assertion, etc.) has been  established as applicable in the previous discourse  (e.g., in preceding questions).
415	Explains Technical Concepts	2.1. Fast-Match Techniques  Fast-match techniques\[l\] are similar to progressive  search in that a coarse match is used to constrain a more  advanced computationally burdensome algorithm. The fast  match, however, simply uses the local speech signal to constrain  the costly advanced technique. Since the advanced techniques  may take advantage of non-local data, the accuracy of a fast-  match is limited and will ultimately limit the overall technique's  performance. Techniques uch as progressive search can bnng  more global knowledge to bear when generating constraints, and,  thus, more effectively speed up the cosily techniques while  retaining more of their accuracy.
97	Explains Non-Technical Concepts	In conclusion, we propose a unified approach to identify-  ing non-linguistic speech features from the recorded signal  using phone-based acoustic likelihoods. This technique has  been shown to be effective for language, sex, and speaker  identification and can enable better and more friendly human  machine interaction.
975	Explains Non-Technical Concepts	1. INTRODUCTION  Tied-mixture (or semi-continuous) distributions have  rapidly become an important ool for acoustic model-  ing in speech recognition since their introduction by  Huang and Jack \[1\] and nellegarda nd iahamoo \[2\],  finding widespread use in a number of high-performance  recognition systems. Tied mixtures have a number of  advantageous properties that have contributed to their  success. Like discrete, "non-parametric" distributions,  tied mixtures can model a wide range of distributions  including those with an "irregular shape," while retain-  ing the smoothed form characteristic of simpler para-  metric models. Additionally, because the component  distributions of the mixtures are shared, the number of  free parameters i  reduced, and tied-mixtures have been  found to produce robust estimates with relatively small  amounts of training data. Under the general heading  of tied mixtures, there are a number of possible choices  of parameterization that lead to systems with different  characteristics. This paper outlines these choices and  provides a set of controlled experiments assessing trade-  otis in speaker-independent recognition on the Resource  Management corpus in the context of the stochastic seg-  ment model (SSM). In addition, we introduce new vari-  ations on training algorithms that reduce computational  requirements and generalize the tied mixture formalism  to include segment-level mixtures.
509	Assumes Prior Knowledge	5.2 Compar ing  Segmenters   Word segmentation is a big issue for Chinese since  linguistics-strong applications uch as POS tagging,  sentence parsing, machine translation, text to voice,  etc. are all dependent on words being accurately  identified to do well. It would therefore be interesting  to see if better word segmentation could lead to more  accurate retrieval.
366	Explains Non-Technical Concepts	5 Conclusions We obtained substantial gains over our systems from last year for all language pairs. To a large part, these gains are due to additional training data and our ability to exploit them. We also saw gains from adding linguistic an- notation (in form of 7-gram models over part-of- speech tags) and promising results for tree-based models. At this point, we are quite satisfied be- ing able to build competitive systems with these new models, which opens up major new research directions.
127	Explains Non-Technical Concepts	The BDSONS Corpus: BDSONS, Base de Donn6es des  Sons du Fran~ais\[2\], was designed to provide a large cor-  pus of French speech data for the study of the sounds in  the French language and to aid speech research. The cor-  pus contains an "evaluation" subcorpus consisting primarily  of isolated and connected letters, digits and words from 32  speakers (16m/16f), and an "acoustic" subcorpus which in-  cludes phonetically balanced words and sentences from 12  speakers (6m/6f).
142	Assumes Prior Knowledge	It is obvious that 10 words are not enough tbr  the sufficient discrimination of the genre  categories. Oil tile other hand, using tile 70 most  frequent words the discriminant functions are  biased to the training data.
151	Explains Technical Concepts	3.2 Labe l l ing   The labelling of the data a(:(:or(ling to SFG (:ri-  teria was obtained from Halliday (1970). The  labelling of the dater using ToBI was done l)y  a trained acoustic l)honeti(:ian. 2 The exisl;ing  recording was digitised at 20 kltz as 16 bit san>  ples, and stored on a Unix machine. The pitch  tracks were calculated using ESPS WAVES+.  The labelling of the data was done in F, MU  (Cassidy & Harrington, 1996). All the intona-  tional and inl;ermedit~te l)hrases were marked,  as', were the pit(:h ac(',ents, 1)hrasal and 1)oun(l-  ary tones.
742	Explains Non-Technical Concepts	Introduction  The development of text databases via the  Internet has given impetus to research in  computational linguistics towards the automatic  handling of this information. In particular, the  enormous amount of texts coming from  heterogeneous sources revealed the need for  robust ext classification tools which are able to  be easily ported to other domains and natural  languages and be employed with minimal  computational cost.
209	Explains Non-Technical Concepts	4.1. Effect of Number of Training Speakers  It has always been assumed that for speaker independent recogni-  tion to work well, we must train the system on as many speakers  as possible. We reported in \[9\] that when we trained a speaker-  independent system on 600 sentences from each of 12 different  speakers (a total of 7,200 sentences), the word error rate was  only slightly higher than when the system was trained on a total  of 3,990 sentences from 109 speakers. These experiments were  performed on the 1000-word Resource Management (RM) Cor-  pus. The results were dit~ficult o interpret because the number  of sentences were not exactly the same for both conditions, the  data for the 109 speakers covered a larger variety of phonetic  contexts than the data for the 12 speakers, and the 12 speakers  were carefully selected to cover the various dialectic regions of  the country (as well as is possible with only 7 male and 5 female  speakers).
723	Assumes Prior Knowledge	Several dialogue translation methods that use extra-linguistic information have been pro- posed. Horiguchi outlined how "spoken lan- guage pragmatic information" can be trans- lated (Horiguchi, 1997). However, she did not apply this idea to a dialogue translation system. LuperFoy et al proposed a software architec- *Current affiliation is ATR Spoken Language Trans- lation Research Laboratories The above mentioned methods will need time to work in practice, since it is hard to obtain the extra-linguistic nformation on which they depend.
307	Explains Non-Technical Concepts	After related works are surveyed in section 2,  we will discuss in section 3 the problem of  semantic deficiency in IR-based text processing,  which motivates building event information into  sentence representation. The details of such  representation are provided in section 4. In  section 5, we will explicate the ordering  algorithms, including layered clustering and  cluster-based ordering. The performance of the  event-enriched model will be extensively  evaluated in section 6. Section 7 will conclude  the work with directions to future work.
493	Explains Non-Technical Concepts	We have participated in all past TREC experiments  with consistently superior results. Since 1996, we have  also participated in the TIPSTER Text Phase 3  program. This report serves to summarize work that  has been done, and some of the important findings for  both English and Chinese IR. Section 2 and 3 gives an  overview of our PIRCS system and the 2-stage  retrieval strategy. Section 4 presents our work for  English ad-hoc retrieval employing term, phrasal and  topical concept levels of evidence. Section 5 describes  various Chinese retrieval experiments. Section 6 has  the conclusions.
61	Explains Technical Concepts	Although the reported experiments were car- ried out on the specific HPSG treebank, we be- lieve that the proposed ranked discriminant-based annotation method can be applied in annotation tasks concerning different linguistic frameworks, or even different layers of linguistic representa- tion. Apart from the specific features presented in Section 3.3, the model itself does not assume a phrase-structure tree annotation, and the discrimi- nants can take various forms. Assuming a ?gram- mar? produces a number of candidate analyses, the annotators can rely on the ranking model to ef- ficiently pick relevant discriminants, and focus on making linguistically relevant decisions. This is especially suitable for large annotation tasks aim- ing for parallel rich annotation by multiple anno- tators, where fully manual annotation is not fea- sible and high inter-annotator agreement hard to achieve.
108	Explains Non-Technical Concepts	2.3 Monolingual data The French and English target language models were trained on all provided monolingual data. In addition, LDC?s Gigaword collection was used for both languages. Data corresponding to the devel- opment and test periods were removed from the Gigaword collections.
734	Explains Non-Technical Concepts	ABSTRACT  Tied-mixture (or semi-continuous) distributions are an im-  portant tool for acoustic modeling, used in many high-  performance speech recognition systems today. This paper  provides a survey of the work in this area, outlining the  different options available for tied mixture modeling, intro-  ducing algorithms for reducing training time, and provid-  ing experimental results assessing the trade-offs for speaker-  independent recognition on the Resource Management ask.  Additionally, we describe an extension of tied mixtures to  segment-level distributions.
318	Explains Technical Concepts	2.2 Preprocessing The training data was preprocessed before used for training. In this step different normalizations were done like mapping different types of quotes. In the end the first word of every sentence was smart- cased.
477	Explains Non-Technical Concepts	Conclusions  We have developed a method to derive quick-read  summaries from news-like texts using a number of  shallow NLP and simple quantitative techniques.  The summary is assembled out of passages extracted  from the original text, based on a pre-determined  DMS template. This approach as produced a very  efficient and robust summarizer for news-like texts.
481	Assumes Prior Knowledge	Several variants of the conventional backoff trigram language  model were applied at the reordering stage of the N-best  paradigm. (Eventually we plan to incorporate this language  model into the A* phase of the multi-pass earch with the  USE). The best result, a 22% word error rate reduction, was  achieved with the simple, non-interpolated "backward" tri-  gram, with the conventional forward trigram finishing a close  second.
166	Explains Technical Concepts	Tokens which occur in the headline are associ-  ated with tokens in the document body using the  same criteria as the query, with the exclusion of  the dictionary lookup. The dictionary lookup was  excluded because the headline will likely use the  same lexicalization of a proper noun as that used  in a document. This is less likely to be the case  with the query.
865	Explains Non-Technical Concepts	Trondheim is a small city with a university and 140000 inhabitants. Its central bus systems has 42 bus lines, serving 590 stations, with 1900 depar- tures per day (in average). That gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base. The starting point is to automate the function of a route information agent. The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock. and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
425	Explains Technical Concepts	2.3.2 Event Merging  One of the challenges of event extraction is to  be able to recognize and merge those event  descriptions which refer to the same event.  The Template Generation module uses a set of  declarative, customizable rules to merge co-  referring events into a single event. Often, the  rules reflect pragmatic knowledge of the world.  For example, consider the rule below for the  DYING event ype. This rule establishes that  if two die events have the same subject, then  they refer to the same event (i.e., a person  cannot die more than once).  {merge  {EVENT 1 {AND {SUBTYPE DIE} {PERSON  $foo}}  {EVENT 2 {AND {SUBTYPE DIE} {PERSON  $foo}}}
700	Assumes Prior Knowledge	Table3:1 st and 2 "a Stage Chinese Retrieval Results  We have done manual analysis of our approximate  segmenter for correctness using the 54 TREC 5 & 6  topics and concluded that its recall and precision  measures for segmenting sentences into short-words  are about mid to high 80%. These figures are  approximate because ven native speakers ometimes  disagree on the correct segmentation. We have also  analyzed a segmenter f om UMASS \[13\] that is based  on a unigram model. It can be trained from a  collection that has been segmented based on a lexicon  list. It segments a sentence by evaluating possible  choices and selecting the one with the highest  probability of the trained model. Our opinion is that its  recall and precision values vary between about 90% to  low-90%, approximately 5% better than ours. We used  both segmenters to investigate the Chinese collection  and did retrieval using our PIRCS system under the  same parameter settings. The result is presented in  Table 4 below. In this table, TREC5 precision values  took account of larger lexicons (Section 5.3) and are  better than those in Table 3.
195	Explains Non-Technical Concepts	4 Experiment Setup To test the effectiveness of the discriminant rank- ing models, we carried out a series of experi- ments, investigating their effects on both annota- tion speed and quality. The experiment was done in the context of our ongoing annotation project of the WSJ sections of the PTB described in Sec- tion 2. Despite sharing the source of texts, the new project aims to create an independently an- notated corpus. Therefore, the trees from the PTB were not used to guide the disambiguation pro- cess. In this annotation project, two annotators (both graduate students, referred to as A and B below) are employed to manually disambiguate the parsing outputs of the ERG. For quality con- trol and adjudication in case of disagreement, a third linguist/grammarian annotates parts of the treebank in parallel.
925	Explains Technical Concepts	2.2 Truecasing As last year, we deal with uppercase and lowercase forms of the same words by truecasing the corpus. This means that we change each surface word oc- currence of a word to its natural case, e.g., the, Eu- rope. During truecasing, we change the first word of a sentence to its most frequent casing. During de-truecasing, we uppercase the first letter of the first word of a sentence. See Table 3 for the performance of this method. In this table, we compare the cased and uncased BLEU scores, and observe that we lose on average roughly one BLEU point due to wrong casing.
949	Explains Technical Concepts	The morphological analysis of Czech is based on the morphological dictionary developed by Jan Haji6 and Hana Skoumalov~i in 1988-99 (for latest description, see Haji~ (1998)). The dictionary contains over 700 000 dictionary entries and its typical coverage varies between 10 99% (novels) to 95% (technical texts). The morphological analysis uses the system of positional tags with 15 positions (each morphological .category, such as Part-of-speech, Number, Gender, Case, etc. has a fixed, single- symbol place in the tag). Example 2 - tags assigned to the word-form "pomoci" (help/by means of) pomoci: NFP2 .... . .  A .... \]NFS7 ...... A .... I R--2 . . . . . . . . . . . where : N - noun; R - preposition F - feminine gender S - singular, P - plural 7, 2 - case (7 - instrumental, 2 - genitive) A - affirmative (non negative). The module of morphological disambiguation is a key to the success of  the translation. It gets an average number of 3.58 tags per token (word form in text) as an input. The tagging system is purely statistical, and it uses a log-linear model of probability distribution - see Haji~, Hladkfi (1998). The learning is based on a manually tagged corpus of Czech texts (mostly from the general newspaper domain). The system learns contextual rules (features) automatically and also automatically determines feature weights. The average accuracy of tagging is between 91 and 93% and remains the same even for technical texts (if we disregard the unknown names and foreign-language t rms that are not ambiguous anyway).
655	Explains Technical Concepts	2.1 German Data: BAUFIX  The BAUFIX corpus (Sagerer el al. 1994)  consists of 22 digitally recorded German  human-human dialogues. 44 participants  co-operated in pairs as instructor and constructor,  where their task was to build a toy-plane.  Because of the limited visual contact between  dialogue partners in some given cases, subjects  had to rely on their verbal comnmnication to a  great extent. This corpus setting was especially  constructed to force subjects to repair their  speech errors. For the purpose of this paper to  investigate repair syntax, the corpus analysis is  mainly concerned with immediate self-repairs.  They were identified and hand-annotated by the  author. In total, 500 speech repairs were  classified according to their syntactic attributes  such as categories and parts of speech. They  were subsequently analysed with respect o the  5 Verbatim translation: Hc should  NEGATION-particle should promote ngineer(word  fragment) engineer so quickly DISCOURSE-particle.  Sentential translation: He shouM should not be  promoted to engineel(word fragment) engineer so  soon .
337	Explains Technical Concepts	This paper describes a method of "polite- ness" selection according to a participant's so- cial role (a clerk or a customer), which is eas- ily obtained from the extra-linguistic environ- ment. We incorporated each participant's so- cial role into transfer ules and transfer dictio- nary entries. We then conducted an experiment with 23 unseen dialogues (344 utterances). Our method achieved a recall of 65% and a preci- sion of 86%. These rates could be improved to 86% and 96%, respectively (see Section 4). It is therefore possible to use a "participant's so- cial role" (a clerk or a customer in this case) to appropriately make the translation results "polite," and to make the conversation proceed smoothly with a dialogue translation system. Section 2 analyzes the relationship between a particular participant's social role (a clerk) and politeness in Japanese. Section 3 describes our proposal in detail using an English-to-Japanese translation system. Section 4 shows an exper- iment and results, followed by a discussion in Section 5. Finally, Section 6 concludes this pa- per.
871	Explains Technical Concepts	3.3 POS n-gram Model The factored model approach (Koehn and Hoang, 2007) allows us to integrate 7-gram models over part-of-speech tags. The part-of-speech tags are produced during decoding by the phrase mapping of surface words on the source side to a factored representation of surface words and their part-of- speech tags on the target side in one translation step.
948	Assumes Prior Knowledge	4.1 Event Structure and Extraction Following (Li et al 2006), we define an event E as a structured semantic unit consisting of one  event term Term(E) and a set of event entities  Entity(E). In the news domain, event terms are  typically action verbs or deverbal nouns. Light  verbs such as ?take?, ?give?, etc. (Tan et al, 2006) are removed.
778	Explains Technical Concepts	2 Related work  Three main approaches have been proposed for  the automatic extraction of lexical semantics  knowledge: syntax-based, n-gram-based and  window-based. Syntax-based methods (referred  also as knowledge-rich in contrast to the others - knowledge-poor methods) (Pereira and Thishby,  1992; Grefenstette, 1993; Li and Abe, 1997)  represent the words under consideration asvectors  containing statistic values of their syntactic  properties in relation to a given set of words (e.g.  statistics of object syntax relations referring to a  set of verbs) and cluster the considered words  according to similarity of the corresponding  vectors. Methods that use bigrams (Brown et al,  1992) or trigrams (Martin et al, 1998) cluster  words considering as a word's context he one or  two immediately adjacent words and employ as  clustering criteria the minimal loss of average  nmtual information and the perplexity  improvement respectively. Such methods are  oriented to language modeling and aim primarily  at rough but fast clustering of large vocabularies.  Brown et al (1992) also proposed a window  method introducing the concept of "semantic  stickiness" of two words as the relatively frequent  close occurrence between them (less than 500  words distance). Although this is an efficient and  entirely knowledge-poor method tbr extracting  both semantic relations and clusters, the extracted  relations are not restricted to semantic similarity  but extend on thematic roles. Moreover its  applicability to small and specialized corpora is  uncertain.
653	Explains Non-Technical Concepts	Experiments for text-dependent speaker identification us-  ing exactly the same models and test sentences were per-  formed. For both TIMIT and BREF a performance degrada-  tion was observed (on the order of 4% using the accuracy at  the end of the sentence.) These results were contrary to our  expectations, in that typically text-dependent speaker verifi-  cation is considered to outperform text-independent\[3, 19\].  An experiment was also performed in which speaker-  adapted models were built for each of the 168 test speakers  from TIMIT without knowledge of the phonetic transcrip-  tion, using the same 8 sentences for adaptation. Performing  text-independent speaker identification as before on the re-  maining 2 sentences give the results hown in Table 6. As be-  fore if both sentences are used for identification, the speaker  identification accuracy is 100%. This experimental result  indicates that the time consuming step of providing phonetic  transcriptions is not needed for accuracte text-independent  speaker identification.
51	Assumes Prior Knowledge	Algorithms like RASTA and CMN compensate for the  effects of unknown linear filtering because linear filters pro-  duce a static compensation vector in the cepstral domain  that is the average difference between the cepstra of speech  in the training and testing environments. Because the  RASTA and CMN filters are highpass, they force the aver-  age values of cepstral coefficients to be zero in both the  training and testing domains. Nevertheless, neither CMN  nor RASTA can compensate directly for the combined  effects of additive noise and linear filtering. It is seen in  Figure 1 that the compensation vectors that maximize the  likelihood of the data vary as a function of the SNR of indi-  vidual frames of the utterance. Hence we expect compensa-  tion algorithms like MFCDCN (which incorporate this  knowledge) to be more effective than RASTA or CMN  (which do not).
321	Explains Non-Technical Concepts	2.4 Development data All development was done on news-test2008, and newstest2009 was used as internal test set. For all corpora except the French side of the bitexts used to train the French?English system (see above), the default Moses tokenization was used. How- ever, we added abbreviations for the French tok- enizer. All our models are case sensitive and in- clude punctuation. The BLEU scores reported in this paper were calculated with the multi-bleu.perl tool and are case sensitive. The BLEU score was one of metrics with the best correlation with human ratings in last year evaluation (Callison- Burch et al, 2009) for the French?English and English?French directions.
130	Explains Technical Concepts	Language Pair Cased Uncased Spanish-English 25.25 26.36 (+1.11) French-English 25.23 26.29 (+1.06) German-English 19.47 20.63 (+1.16) Czech-English 20.74 21.76 (+1.02) English-Spanish 24.20 25.47 (+1.27) English-French 23.83 25.02 (+1.19) English-German 14.68 15.18 (+0.50) English-Czech 14.63 15.13 (+0.50) avg +0.98 Table 3: Effect of truecasing: cased and uncased BLEU scores
688	Explains Non-Technical Concepts	We are in the process of obtaining corpora for other lan-  guages to extend our language identification work. However,  there are variety of applications where a bilingual system,just  French/English would be of use, including air traffic control  (where both French and English are permitted languages for  flights within France), telecommunications applications, and  many automated information centers, ticket distributors, and  tellers, where already you can select between English and  French with the keyboard or touch screen.
268	Assumes Prior Knowledge	The environmental compensation algorithms were evalu-  ated using the SPHINX-II recognition system \[12\] in the  context of the November, 1992, evaluations of continuous  speech recognition systems using a 5000-word closed-  vocabulary task consisting of dictation of sentences from  the Wall Street Journal. A component of that evaluation  involved utterances from a set of unknown "secondary"  microphones, including desktop microphones, telephone  handsets and speakerphones, stand-mounted microphones,  and lapel-mounted microphones.
529	Explains Non-Technical Concepts	1 Introduction We participated in the shared translation task of the ACL Workshop for Statistical Machine Trans- lation 2010 in all language pairs. We continued our efforts to integrate linguistic annotation into the translation process, using factored and tree- based translation models. On average we out- performed our submission from last year by 2.16 BLEU points on the same newstest2009 test set. While the submitted system follows the factored phrase-based approach, we also built hierarchical and syntax-based models for the English?German language pair and report on its performance on the development test sets. All our systems are based on the Moses toolkit (Koehn et al, 2007). We achieved gains over the systems from last year by consistently exploiting all available train- ing data, using large-scale domain-interpolated, and consistent use of the factored translation model to integrate n-gram models over speech tags. We also experimented with novel domain adaptation methods, with mixed results.
278	Assumes Prior Knowledge	A different way to capture local coherence in  sentence ordering is the Centering Theory (CT,  Grosz et al 1995)-inspired entity-transition  approach, advocated by Barzilay and Lapata  (2005, 2008). In their entity grid model,  syntactic roles played by entities and transitions  between these syntactic roles underlie the  coherence patterns between sentences and in the  whole text. An entity-parsed corpus can be used  to train a model that prefers the sentence  orderings that comply with the optimal entity  transition patterns.
643	Explains Technical Concepts	Meanwhile we find no matter how long or  how many modifiers modify the temporal noun,  the whole temporal expression holds the original  temporal reference inferred from the temporal  noun. Moreover, the key point of normalizing  temporal expressions is choosing the appropriate  reference time according to the real context ra- ther than deciding the right direction or compu- ting the measurable offset. For instance, with  regard to these two Implicit Times in Figure 1,  ?after one week? and ?this Monday?, we can  achieve the referential direction easily from the  modifiers through some mapping rules. Mean- while, the offsets are able to be understood di- rectly by machine with pattern matching. But for  the reference time, we must build the context- depending reference reasoning to trace it. The  reference link is described as Figure 2 shows.    3.2 Na?ve Bayesian Classifier  A variety of machine learning classifiers are de- signed to resolve the classification problem,  such as SVM classifier, ME classifier and the  Decision Tree family. But the performance of  these classifiers is greatly depending on the fea- tures selection. Based on the observation and  analysis in our experiments, we find the referen- tial feature holds in the temporal noun is hard to  express with some explicit denotations. For ex- ample, ?that year? and ?this year? are nearly  identical in surface feature, but the former is lo- cally context-depending while the latter is local- ly context-free. So the Na?ve Bayesian Classifier  that assumes independence among feature deno- tations is suitable to be applied to our method.  We take the single word in the temporal noun  as the object attribute ix after removing the Ex- plicit Time in the whole text. Given the class  label c , the classifier learns the conditional prob- ability of each attribute ix from training data.  Meanwhile, achieving the practical instance  of X , classification is then performed by apply- ing Bayes rules to compute the probability of c ,  and then predicting the class with the highest  posterior probability.    4 Temporal Expression Normalization  4.1 Basic Normalizing Algorithm  In the beginning, we need to achieve the report  time (RT) or the publication time (PT) of the  document to initialize the GRT and LRT. Addi- tionally, the fuzzy time can be referred to by  other times in the normalization, but we must  solve the defuzzification problem before taking  it as the reference time. With respect to this issue,  we will discuss it in the next section. Conse- quently, the practical normalizing algorithm is as  follows.  Algorithm: TimeNormalize  Input: temporal expression ti in text  Output: regular time list TList  Begin  //initialize the GRT and LRT with RT or PT of this   document      GRT ? Initialize (RT|PT)      LRT ? Initialize (RT|PT)  for each ti in text do      //segment ti into modifier and temporal noun      ti???SegmentTemporal??ti?      if IsExplicitTime (ti) is true          //update the time table with ti          LRT ??UpdateTime??ti?? ?????????//insert ti into regular time list directly          TList ? InsertList (ti)      else          if?IsLocalTime??ti???is?true? //retrieve the latest LRT from time table and then  normalize ti?              Ti???RegularizeTemporal??ti? , LRT?          else? //retrieve GRT from time table and then   normalize ti?              Ti???RegularizeTemporal??ti? , GRT?          LRT ??UpdateTime??Ti?          end if      TList ? InsertList (Ti)      end if  return TList  End Begin
368	Explains Non-Technical Concepts	1 Project Background 1.1 Translation at EU Institutions The European Union?s policy on multilingualism1 re- quires enormous amounts of documents to be trans- lated into the 23 official languages (which yield 506 translation directions). To cope with this task, the EU has the biggest translation service in the world, em- ploying almost 5000 internal staff as translators (out of which 1750 at the European Commission (EC) and 760 at the European Parliament (EP) alone), backed up by more than 2000 support staff. In 2009, the total output of the Commission?s Directorate-General for Transla- tion (DGT) and the Parliament?s Directorate-General for Translation (DG TRAD) together was more than 3 million translated pages. Thus, it is not surprising that the cost of all translation and interpreting services of all the EU institutions amounts to 1% of the annual EU budget (2008 figures). According to our estimations, this is more than e 1 billion per year. 1.2 Machine Translation and Other Translation Technologies at EU Institutions In order to make the translators? work more efficient so that they can translate more pages in the same time, a number of tools like terminology databases, bilin- gual concordancers, and, most importantly, translation memories are at their disposition, most of which are heavily used.
196	Explains Non-Technical Concepts	Text Summarization is still in the infant stage in terms  of evaluation. Many monolingual document  information retrieval results can be applied to text  summarization, but as of yet, there has been little  evaluation of these techniques. This pilot experiment  showed many areas that need to be examined in  further detail, including whether the summary selects  the most relevant sentences in the document and  whether these results generalize to more data sets and  other document genres. We also plan to explore  further the effects of query expansion using WordNet,  as well as the use the first sentence (for news stories)  in the query and/or summary. We also plan to run  experiments fixing the number of sentences for each  document as the number of relevant sentences chosen  by the assessors as well as a small number, such as  three. We are currently in the process of building a  more extensive sentence relevance database for  further evaluation. In this database, we are collecting  data on the user selected most relevant sentence(s) for  each document. We also plan to explore how to join  the relevant sections to provide a "good",  understandable, readable, relevant, non-redundant  summary.
178	Assumes Prior Knowledge	In the ATIS domain, for 78 keywords in a vocabulary of  1200, we show that the CSR approach significantly outperforms  the traditional wordspotting approach for all false alarm rates per  hour per word: the figure of merit (FOM) for the CSR recognizer  is 75.9 compared to only 48.8 for the spotting recognizer. In the  Credit Card task, the sporing of 20 keywords and their 58 vari-  ants on a subset of the Switchboard corpus, the system's perfor-  mance levels off at a 66% detection rate, limited by the system's  ability to increase the false alarm rate. Additional experiments  show that varying the vocabulary size from medium- to large-  vocabulary recognition systems (700 to 7000) does not affect he  FOM performance.
326	Explains Technical Concepts	The key notions in progressive s arch techniques are:  1. An early-pass peech recognition phase builds a  lattice, which contains all the likely recognition unit  strings (e.g. word sequences) given the techniques  used in that recognition pass.  2. A subsequent pass uses this lattice as a grammar that  constrains the search space of an advanced technique  (e.g., only the word sequences contained in a word  lattice of pass p would be considered inpass p+l).  Allowing a sufficient breadth of lattice entries should  allow later passes to recover the correct word sequence, while  ruling out very unlikely sequences, thus achieving high accuracy  and high speed speech recognition.
201	Explains Non-Technical Concepts	In these examples, the suffering agents have  been figuratively conceptualized as food. They  bear the results of intensive or slow cooking.  Thus, these agents who suffer from such cook- ing actions carried out by other agents tend to  feel pain and sadness, while the ?cooking per- forming? agents may take advantage of such  actions to achieve their intentions, such as per- suasion, punishment or even enjoyment. The  syntactic structures of some of the above exam- ples also indicate the submissive stance of the  suffering agents. E.g. in the instances, passive  sentences (?he knew he was cooked when he  saw his boss standing at the door?) have been  used to imply unwillingness and victimization  of the subject agents who are in fact the objects  of the cooking actions described by the verb  phrases (?X + copular form + passive cooking  action?). In other examples, the cooking actions  have been explicitly performed by the subject  agents towards the object agents to imply the  former?s potential willingness and enjoyment  and the latter?s potential suffering and pain (?A  + [cooking action] + B?).
641	Explains Technical Concepts	2.5 Summarizat ion-Translat ion Stage  In the Summarization-Translation Stage, the  importance of each sentence in the document is  determined using a scoring procedure, which  assigns scores to the sentences according to the  position of the sentences in the document structure  and according to the occurrences of key-words in  the sentence which belong to the set of most fre-  quent words in the document that are not in a "stop  list" (the most frequent words in a language are  considered irrelevant). We make the assumption  that these key-word represent or identify the main  concepts in the document, herefore if a sentence  contains everal of them, its score should be high  so it could be selected as part of the summary. It is  important to note here that we need a "stop list" for  each language considered in the summarization  system. Also, if a Proper Name Recognition mod-  ule is available for a specific language, we use the  information about person names, organization  names, places and dates to contribute in the scores  of sentences.
791	Explains Technical Concepts	Re la t ing  the  query  to the  document   The relationships discussed previously are approx-  imated via a series of associations between tokens  in the query, headline, and the body of the docu-  ment. Event references are captured by associating  verbs or nominalizations in the query with verbs  and nominalizations in the document.  Given three verbal forms vl in the query, v2 in  the document, and v3 in the set of all verbal forms,  where a verbal form is the morphological root of a  verb or the verb root corresponding to a nominal-  ization, vl is associated with v2 if at least one of  the following criteria are met:  1. (Vl ?v2)  Ap(vl,v2)/(p(vl)p(v2)) -->5  2. (vl =v2)  A (3v3 7~Vl I p(vl,v3)/p(vl)p(v3) -> 5)  3. (Vl = v2) A ((subject(vl) = subject(v2)) V  (object(v1) =object(v2)))
961	Assumes Prior Knowledge	2.3. Word Lattices  This technique is the most similar to progressive s arch.  In I~ath approaches, an initial-pass recognition system can  generate a lattice of word hypotheses. Subsequent passes can  searclh through the lattice to find the best recognition hypothesis.  It should be noted that, although we refer to lattices as word  lattices, they could be used at other linguistic level, such as the  phoneme, syllable, e.t.c.
291	Explains Non-Technical Concepts	If we consider document summarization by relevant-  passage extraction, we must again consider anti-  redundancy as well as relevance. Both query-free  summaries and query-relevant summaries need to avoid  redundancy, as it defeats the purpose of summarization.  For instance, scholarly articles often state their thesis in  the introduction, elaborate upon it in the body, and"  reiterate it in the conclusion. Including all three in  versions in the summary, however, leaves little room for  other useful information. If we move beyond single  document summarization to document cluster  summarization, where the summary must pool passages  from different but possibly overlapping documents,  reducing redundancy becomes an even more significant  problem.
839	Explains Non-Technical Concepts	5 Summary The goal of this work was to investigate he feasibil- ity of using a statistical translation model trained on a Web-collected corpus to do English-Chinese CLIR. In this paper, we have described the algorithm and implementation we used for parallel text mining, translation model training, and some results we ob- tained in CLIR experiments. Although further work remains to be done, we can conclude that it is pos- sible to automatically construct a Chinese-English parallel corpus from the Web. The current system can be easily adapted to other language pairs. De- spite the noisy nature of the corpus and the great difference in the languages, the evaluation lexicons generated by the translation model produced accept- able precision. While the current CLIR results are not as encouraging asthose of English-French CLIR, they could be improved in various ways, such as im- proving the alignment method by adapting cognate definitions to HTML markup, incorporating a lexi- con and/or removing some common function words in translated queries.
649	Assumes Prior Knowledge	However, these works on temporal expression  normalization do not give an effective reference  time choosing method for Implicit Times in real  texts. More specifically, the pioneer work by  Lacarides [1992] investigated various contextual  effects on different temporal-reference relations.  Then Hitzeman et al [1995] discussed the refer- ence-choosing taking into account the effects of  tense, aspect, temporal adverbials and rhetorical  relations. Dorr and Gaasterland [2002] presented  the enhanced one in addition considering the  connecting words. But they are theoretical in  nature and heavily dependent on languages. Cur- rently, the static time-value mechanism [Mani  and Wilson, 2000; Wu et al, 2005; Wu et al,  2005] and the static choosing-rules mechanism  [Vozov, 2001; Jang et al, 2004; Lin et al, 2008]  for reference time choosing are applied into  some systems widely. Nevertheless, as the dis- cussion in section 1, these two ways are not  adaptable to universal Implicit Times. In addi- tion, Vicente-Diez et al [2008; 2009] discussed  the reference date for relative times, but the al- ternative rules are not effective in experiments.  Lin et al [2008] considered the condition that  there is no report time or publication time when  choosing reference time.
455	Explains Technical Concepts	3 An updated version of DP  Based on the first results, we made a few  modifications and then reevaluated DP. In  particular, we added items to the possession  exception list based on the new corpus and made  some of the no-presupposition rules more  specific. As a more drastic change, we updated  the decision tree structure so that presupposition  indicators overrule indicators against  presuppositions, increasing the number of  reported presuppositions for cases of conflicting  indicators:  If there is evidence for a problem, report "Problem"  Else  if evidence against problem, report "No problem"  else, report "Probably not a problem"  Separate analyses show that the modification of  the decision tree accounts for most of the  performance improvement.
79	Explains Technical Concepts	4.1. Experimental Paradigm  The experiments described below were run on the  Resource Management (RM) corpus using speaker-  independent, gender-dependent models trained on the  standard SI-109 data set. The feature vectors used as  input to the system are computed at 10 millisecond in-  tervals and consist of 14 cepstral parameters, their first  differences, and differenced energy (second cepstral dif-  ferences are not currently used). In recognition, the SSM  uses an N-best rescoring formalism to reduce computa-  tion: the BBN BYBLOS system \[7\] is used to generate  20 hypotheses per sentence, which are rescored by the  SSM and combined with the number of phones, num-  ber of words, and (optionally) the BBN HMM score, to  rerank the hypotheses. The weights for recombination  are estimated on one test set and held fixed for all other  test sets. Since our previous work has indicated prob-  lems in weight estimation due to test-set mismatch, we  have recently introduced a simple time normalization of  the scores that effectively reduces the variability of scores  due to utterance length and leads to more robust perfor-  mance across test sets.
869	Explains Technical Concepts	8.2 Evaluation Code  We modified the 11-pt recall-precision curves \[21\]  commonly used for document information retrieval.  Since many documents only have a few relevant  sentences, corresponding curves for summarization  have a lot of intervals with missing data items. To  remedy this situation, we implemented a step function  for the precision values. This allowed the recall  intervals that would not naturally be filled to be  assigned an actual precision value. For example, in  the case of two relevant sentences in the document,  points 0-5 (the first five intervals) would all have the  first precision value (naturally occurring at point 5)  and points 6-10 (the second value), the second value  (naturally occurring at point 10). We interpolated the  results of each query for the composite graph to form  modified interpolated recall-precision curves.  In order to account for the fact that a compressed  summary does not have the opportunity to return the  full set of relevant sentences, we use a normalized  version of recall and a normalized version of F1 as  defined below.
193	Explains Technical Concepts	1 Testing Ground  1.1 Corpora  As regards the English language, in the previous  work on text genre detection (Karlgren and  Cutting, 1994; Kessler et al, 1997) the Brown  corpus was used as testing ground. It comprises  approximately 500 samples divided into 15  categories (e.g., press editorial, press reportage,  learned, etc.) that can be considered as genres.  However, this corpus was not built exclusively  tbr text genre detection purposes. Therefore, the  texts inchlded in the same category are not  always stylistically homogeneous. Kessler el al.,  (1997) underlined this fact and attempted to  avoid the problem by eliminating texts that did  not fall nneqnivocally into one of their  categories. Moreover, some of the categories of  the Brown corpus are either too general (e.g.,  general fiction) or unlikely to be considered in  the fi'anaework of a practical application (e.g.,  belles lettres, religion, etc.). Taking all these into  account we decided to use the Wall &reet  Journal (WSJ) corpus as testing ground for our  approach. The texts comprising this corpus  cover the majority of the press genres. Although  there is no manual categorization of the WSJ  documents according to their genre, there are  headlines that sometimes help in predicting the  corresponding text genre. The selection of the  texts included in the presented corpus was  performed automatically by reading the headline  tag (<HL>) of each doculnent. A typical  headline tag ofa WSJ document is as follows:  <HL> Market ing  & Media:  @ RJR Nabisco Hires  @ Adv iser  to Study  @ Sale of ESPN Stake  @ By Michael  J. McCarthy   @ Staff  Reporter  of The Wal l  Street  Journal  </HL>
438	Assumes Prior Knowledge	The count of counts are collected for the phrase pairs. See Table 4 for details on how this ef- fects the French?English model. For instance, we find singleton 357,929,182 phrase pairs and 24,966,751 phrase pairs that occur twice. The Good Turing formula tells us to adapt singleton counts to 24,966,751357,929,182 = 0.14. This means for our degenerate example of a single occurrence of a single French phrase that its single English transla- tion has probability 0.141 = 0.14 (we do not adjust the denominator).
984	Explains Technical Concepts	7 Outlook Further use and development of SMT at EU institutions depends on the outcome of internal evaluations, among other factors. We plan to extend our activities to other language pairs, an English-to-Greek machine transla- tion project already having started. Given a continu- ation of the currently promising results, Exodus will eventually be integrated into the CAT (computer-aided translation) tools used by EU translators.8 Further- more, we would like to release an extended EuroParl corpus not only containing parliamentary proceedings but also other types of public documents. We estimate that such a step should foster research to the benefit of both EU institutions and machine translation in gen- eral.
731	Assumes Prior Knowledge	Abstract  Information Extraction (IE) systems today are com-  monly based on pattern matching. The patterns are  regular expressions tored in a customizable knowl-  edge base. Adapting an IE system to a new subject  domain entails the construction of a new pattern base  - -  a time-consuming and expensive task. We describe  a strategy for building patterns from examples. To  adapt the IE system to a new domain quickly, the  user chooses a set of examples in a training text, and  for each example gives the logical form entries which  the example induces. The system transforms these  examples into patterns and then applies meta-rules  to generalize these patterns.
63	Assumes Prior Knowledge	The segmentation f a translation memory is a key feature for our system. The translation memory may be exported into a text file and thus allows easy manipulation with its content. Let us suppose that we have at our disposal two translation memories - one human made for the source/pivot language pair and the other created by an MT system for the pivot/target language pair. The substitution of segments of a pivot language by the segments of a target language is then only a routine procedure. The human translator translating from the source language to the target language then gets a translation memory for the required pair (source/target). The system of penalties applied in TRADOS Translator's Workbench (or a similar system) guarantees that if there is already a human-made translation present, then it gets higher priority than the translation obtained as a result of the automatic MT. This system solves both problems mentioned above - the human translators from the pivot to the target language are not needed at all and the machine- made translation memory serves only as a resource supporting the direct human translation from the source to the target language.
43	Explains Non-Technical Concepts	Abstract An important task of opinion mining is  to extract people?s opinions on features  of an entity. For example, the sentence,  ?I love the GPS function of Motorola  Droid? expresses a positive opinion on  the ?GPS function? of the Motorola  phone. ?GPS function? is the feature.  This paper focuses on mining features.  Double propagation is a state-of-the-art  technique for solving the problem. It  works well for medium-size corpora.  However, for large and small corpora, it  can result in low precision and low re- call. To deal with these two problems,  two improvements based on part-whole and ?no? patterns are introduced to in- crease the recall. Then feature ranking is  applied to the extracted feature candi- dates to improve the precision of the  top-ranked candidates. We rank feature  candidates by feature importance which  is determined by two factors: feature re- levance and feature frequency. The  problem is formulated as a bipartite  graph and the well-known web page  ranking algorithm HITS is used to find  important features and rank them high.  Experiments on diverse real-life datasets  show promising results.
304	Explains Non-Technical Concepts	The cross-document coreference system was tested  on a highly ambiguous test set which consisted of  197 articles from 1996 and 1997 editions of the  New York Times. The sole criteria for including  an article in the test set was the presence or the  absence of a string in the article which matched  the "/ John.*?Smith/" regular expression. In other  words, all of the articles either contained the name  John Smith or contained some variation with a mid-  dle initial/name. The system did not use any New  York Times data for training purposes. The an-  swer keys regarding the cross-document chains were  manually created, but the scoring was completely  automated.
854	Explains Technical Concepts	Conc lus ions   The T IPSTER phase III program has allowed us to  explore some of the potential application areas of  coreference annotation. We have reported on our  strongest results, a summarization system and a  cross-document coreference system for names.  The query-sensitive t xt summarization system  is nearly as effective as full text documents for  determining whether a document is relevant to  the query. The system uses a limited class of  coreference-based r lations between the query and  the document o select sentences which represent  instantiations of entities, events, or concepts artic-  ulated in the query.
894	Explains Technical Concepts	2 Baseline System The baseline system uses all available training data, except for the large UN and 109 corpora, as well as the optional LDC Gigaword corpus. It uses a straight-forward setup of the Moses decoder. Some relevant parameter settings are: ? maximum sentence length 80 words ? tokenization with hyphen splitting ? truecasing ? grow-diag-final-and alignment heuristic ? msd-bidirectional-fe lexicalized reordering ? interpolated 5-gram language model ? tuning on newsdev2009 ? testing during development on newstest2009 ? MBR decoding ? no reordering over punctuation ? cube pruning
359	Assumes Prior Knowledge	Figure 2: SFC tones and their meanings  The phrase tone H- represents high pitdt follow-  ing the last pitch accent. Tile tone associated  with an intonation phrase is a boundary tone  and is indicated by %. The boundary tone H%  represents a final rise and the L% boundary tone  is typically interpreted as the absence of a final  rise (cf. Ladd (1996)).
84	Explains Technical Concepts	The CIC algorithm performs inference in two steps, as shown in Algorithm 1. The first step, bootstrapping, predicts an initial labeling assignment for a unlabeled sequence xi, given the trained model P (y|x). The second step is the iterative classification process which re-estimates the labeling assignment of xi several times, picking them in a sample set S based on initial assignment for xi. Here we exploit the sampling technique (Andrieu et al, 2003). The advantages of sampling are summarized as follows. Sampling stochastically enables us to generate a wide range of inference situations, and the samples are likely to be in high probability ar- eas, increasing our chances of finding the max- imum, thus leading to more robust and accurate performance. The CIC algorithm may converge if none of the labeling assignments change dur- ing an iteration or a given number of iterations is reached.
930	Explains Technical Concepts	The second translation methodology employed was direct dictionary translation, tested only for Spanish. We used the same queries for this test. Using an on-line Spanish-English dictionary, we selected, for each word, the top (top-frequency) translation. We then submitted this word- by-word translation to PictureQuest. (Unlike AltaVista, this method spell- corrected letters entered without the necessary diacritics.) Evaluation proceeded in the same manner. The word-by-word method introduces a weakness in phrase recognition: any phrase recognition capabilities in the retrieval system are defeated if phrases are not retained in the input. We can assume that the non-English- speaking user will, however, recognize phrases in her or his own language, and look them up as phrases where possible. Thus we can expect at least those multiword phrases that have a dictionary entry to be correctly understood. We still do lose the noun phrase recognition capabilities in the retrieval system, further confounded by the fact that in Spanish adjectives follow the nouns they modify. In the hombre de negocios example in the data below, both AltaVista and Langenscheidt correctly identify the phrase as multiword, and translate it as businessman rather than man of businesses.
282	Explains Non-Technical Concepts	4.1. Memory  vs Speed Tradeoffs  One of the classical methods for saving computation is to  trade increased memory for reduced computation. Now that  memory is becoming large and inexpensive, there are several  methods open to us. The most obvious is various forms of  fast match. We propose one such memory-intensive fast  match algorithm here. Many others could be developed.  Given an unknown word, we can make several orthogonal  measures on the word to represent the acoustic realization  of that word as a single point in a multi-dimensional space.  If we quantize ach dimension independently, we determine  a single (quantized) cell in this space. We can associate  information with this cell that gives us a precomputed es-  timate of the HMM score of each word. The computation  is performed only once, and is therefore very small and in-  dependent of the size of the vocabulary. (Of course the  precompilation of the scores of each of the words given a  cell in the space can be large.) The precision of the fast  match score is limited only by the amount of memory that  we have, and our ability to represent the scores efficiently.
472	Explains Non-Technical Concepts	Abstract  This paper describes and evaluates a detector  of presuppositions (DP) for survey questions.  Incorrect presuppositions can make it  difficult to answer a question correctly.  Since they can be difficult to detect, DP is a  useful tool for questionnaire designer. DP  performs well using local characteristics of presuppositions. It reports the presupposition  to the survey methodologist who can  determine whether the presupposition is  valid.
452	Explains Technical Concepts	In addition, (iii) short phrases for questions  are also used frequently in the transcripts to gain  further communication based on context, e.g.  ?where??, ?who is Dave? or ?what?. (iv) Cha- racter names are also normally used in the user  input to indicate that the current input is in- tended for particular characters, e.g. ?Dave go  away?, ?Mrs Parton, say something?, ?Dave  what has got into you?? etc. Very often, such  expressions have been used to imply potential  emotional contextual communication between  the current speaking character and the named  character. Therefore the current speaking cha- racters may imply at least ?approval? or ?disap- proval? towards the opinions/comments pro- vided by the previous named speaking charac- ters. Finally there are also (v) some other well  known contextual indicators in Internet relay  chat such as ?yeah/yes followed by a sentence  (?yeah, we will see?)?, ?I think so?, ?no/nah fol- lowed by a sentence?, ?me too?, ?exactly?,  ?thanks?, ?sorry?, ?grrrr?, ?hahahaha?, etc.  Such expressions are normally used to indicate  affective responses to the previous input.   Since natural language is ambiguous and  there are cases in which contextual information  is required in order to appropriately interpret the  affect conveyed in the input (e.g. ?go on then?),  our approach reported in the following inte- grates the above contextual linguistic indicators  with cognitive contextual emotion prediction to  uncover affect conveyed in emotionally ambi- guous input.
152	Explains Technical Concepts	2.3 Input Process Stage  In the input stage, MINDS can accept docu-  ments written in different languages and codesets:  currently English, Japanese, Russian, Turkish and  Spanish. Also the documents can be in different  formats such as SGML, HTML, E-mail or Plain  text. A parsing stage identifies the document's  format, selects and applies the appropriate parser  and extracts the relevant ext from the document.  Once we have the text to be summarized a lan-  guage recognition module determines the language  in which the document is written and the text  encoding used in the document. Given the encod-  ing of the document the text is converted to UNI-  CODE and all the rest of the processing is carried  out on the UNICODE version of the text.
159	Explains Non-Technical Concepts	Some customer service providers have started to take advantage of the recent advances in speech recognition technology. Therefore, some of the IVR systems now allow users to say the option number (1, 2, 3 . . . . .  etc.) instead of pressing the corresponding button. In addition, some providers have taken this a step further by allowing users to say a keyword or a phrase from a list of keywords and/or phrases. For example, AT&T, the long distance company, provides their users the following options: "Please say information for information on placing a call, credit for requesting credit, or operator to speak to an operator." However, given the improved speech recognition technology, and the research done in natural anguage dialogue over the last decade, there exists tremendous potential in enhancing these customer service centers by allowing users to conduct a more natural human-like dialogue with an automated system to provide a customer-friendly s stem. In this paper we describe a system that uses natural language dialogue to provide customer service for a medical domain. The system allows field engineers to call and obtain identification numbers of parts for medical systems using natural language dialogue. We first describe some work done previously in using natural language dialogue for customer service applications. Next, we present he architecture of our system along with a description of each of the key components. Finally, we conclude by providing results from an evaluation of the system.
966	Explains Technical Concepts	5 Conclusion  In this paper, we have applied the c4.5 learning  algorithm for the task of Thai word extraction.  C4.5 can construct a good decision tree for  word/non-word disambiguation. The learned  attributes, which are mutual information, entropy,  word frequency, word length, functional words,  first two and last two characters, can capture  useful information for word extraction. Our  approach yields about 85% and 56% in precision  and recall measures respectively, which is  comparable to employing an existing dictionary.  The accuracy should be higher in larger corpora.  Our future work is to apply this algorithm with  larger corpora to build a corpus-based Thai  dictionary. And hopefully, out" approach should be  successful for other non-word-boundary  languages.
774	Explains Non-Technical Concepts	Query expansion appears to produce consistently  high gains not only for different sets of queries but  also for different systems: we asked other groups  participating in TREC to run search using our ex-  panded queries, and they reported similarly large  improvements.
81	Explains Technical Concepts	5. Conclusion  We have shown that a practical, broad-coverage  parser can be implemented without requiring the  word-breaking component to return a single  segmentation a alysis, and that it can at the same  time achieve high accuracy in POS-labeled  word-breaking. Separating the tasks of word  ident~/'l'cation a d best sequence selection offers  flexibility in enhancing both recall and precision  without sacrificing either at the cost of the other.  Our results show that morphological nd syntactic  information alone can resolve most word-breaking  ambiguities. Nonetheless, some ambiguities  require semantic and contextual information. For  example, the following sentence allows two parses  corresponding to two word-breaking analyses, of  which the first is semantically preferred:  826  (1) ocha-ni haitte-irtt arukaroido  tea-in contain-ASP alkaloid  "the alkaloid contained in lea"  (2) ocha-ni-ha itte-iru arukatwido  tea-in-TOP go-ASP alkaloid  ? ? the alkaloid that has gone to the tea"  Likewise, the sentence below allows two different  interpretations of the morpheme de, either as a  locative marker (1) or as a copula (2). Both  interpretations are syntactically and semantically  wflid; only contextual information can resolve the  ambiguity.  (1) minen-ha isuraeru-de aru  next year-TOP Israel-LOC be-held  "It will be held in Israel next year".  (2) rainen-ha isuraeru de-artt  next year-TOP Israel be-PP, ES  "It will be Israel next year".
188	Explains Technical Concepts	6 Conclusion & Future Work We propose to use a statistical ranking model to assist the discriminant-based treebank annotation. Our experiment shows that such a model, trained on annotation history, brings a huge efficiency im- provement together with slightly improved inter- annotator agreement.
841	Explains Technical Concepts	The following are generated example emo- tional profiles for the sub-theme ?Mayid starts  bullying? for the Mayid character:  1. T A A N A A [?threatening, angry, angry,  neutral, angry and angry?]   2. N A A A [?neutral, angry, angry, and an- gry?]  3. D A I A A A N A [?disapproval, angry, in- sulting, angry, angry, angry, neutral, and an- gry?]  4. I A A N [?insulting, angry, angry and neu- tral?]  The dynamic algorithm is used to find the  smallest edit distance between the test emotion- al context [angry and angry] and the training  and generated emotional context for the Mayid  character for each sub-theme. In the above ex- ample, the second and fourth emotional se- quences have the smallest edit distance (dis- tance = 2) to the test emotional context and the  former suggests ?angry? as the affect conveyed  in the current input (?go on den?) while the lat- ter implies ?neutral? expressed in the current  input. Thus we need to resort to the emotional  context of other characters to justify the rec- ommended affects. From the chatting log, we  find that Lisa was ?angry? in her most recent  input (the 1st input) while Elise was ?threaten- ing? in her most recent input (the 5th input).  Since the bully, Mayid, has a negative relation- ships with Lisa (being ?angry?) and Elise (being  ?threatening?), the imperative input (?go on  den?) may indicate ?angry? rather than ?neutral?.  Therefore our processing adjusts the affect from  ?neutral? to ?angry? for the 6th input.
929	Explains Technical Concepts	Example (1) Eng: We accept payment by credit card Standard: watashitachi-wa kurejitlo-kaado-deno shiharai-wo ukelsukemasu Polite: watashidomo-wa kurejitto-kaado-deno o_shiharai-wo ukeshimasu Gloss: We-TOP credit-card-by payment-OBJ accept used in the target pattern. The source exam- ple (("accept") ("payment")) comes from Ex- ample (1), and the other source examples come from the other closed utterances. This transfer rule means that if the source pattern is (X (V- CN) Y) then (y "wo" x) or (y "ni" x) is selected as the target pattern, where an input word pair corresponding to X and Y is semantically the most similar in a thesaurus to, or exactly the same as, the source example. For example, if an input word pair corresponding to X and Y is semantically the most similar in a thesaurus to, or exactly the same as, (("accept") ("pay- ment")), then the target pattern (y "wo" x) is selected in Figure 2. As a result, an appropriate target pattern is selected. After a target pattern is selected, TDMT cre- ates a target structure according to the pattern (X (V-CN) Y) ((y "wo" x) ((("accept") ("payment")) (("take") ("picture"))) (y "hi" x) ((("take") ("bus")) (("get") ("sunstroke"))) )
843	Assumes Prior Knowledge	Therefore, the story sub-themes could be used  as the indicators for potential emotional context  change. The emotion patterns expressed by each  character within the improvisation of each story  sub-theme could be very useful for the predic- tion of the affect shown in a similar topic con- text, although the improvisation of the charac- ters is creative within the loose scenario. It will  improve the performance of the emotional con- text prediction if we allow more emotional pro- files for each story sub-theme to be added to the  training data to reflect the creative improvisa- tion (e.g. some improvisations went deeper for a  particular topic).
608	Explains Non-Technical Concepts	We have been paying special attention to "po- liteness," because a lack of politeness can inter- fere with a smooth conversation between two participants, uch as a clerk and a customer. It is easy for a dialogue translation system to know which participant is the clerk and which is the customer from the interface (such as the wires to the microphones).
787	Explains Technical Concepts	3.2.1 Phrase pattern  In this case, the part-whole relation exists in a  phrase. NP + Prep + CP:  noun/noun phrase (NP)  contains the part word and the class concept  phrase (CP) contains the whole word. They are  connected by the preposition word (Prep). For  example, ?battery of the camera? is an instance  of this pattern where NP (battery) is the part noun and CP (camera) is the whole noun. For  our application, we only use three specific pre- positions: ?of?, ?in? and ?on?.   CP + with + NP:   likewise, CP is the class  concept phrase, and NP is the noun/noun phrase.  They are connected by the word ?with?. Here  NP is likely to be a feature. For example, in a  phrase, ?mattress with a cover?, ?cover? is a  feature for mattress. NP CP or CP NP: noun/noun phase (NP)  and class concept phrase (CP) forms a com- pound word. For example, ?mattress pad?. Here  ?pad? is a feature of ?mattress?.    3.3 ?no? Pattern  Besides opinion word and part-whole relation,  ?no? pattern is also an important pattern indicat- ing features in a corpus. Here ?no? represents  word no.  The basic form of the pattern is ?no?  word followed by noun/noun phrase. This sim- ple pattern actually is very useful to feature ex- traction. It is a specific pattern for product re- views and forum posts. People often express  their comments or opinions on features by this  short pattern. For example, in a mattress domain,  people always say that ?no noise? and ?no in- dentation?. Here ?noise? and ?indentation? are  all features for the mattress. We discover that  this pattern is frequently used in corpora and a  very good indicator for features with a fairly  high precision. But we have to take care of the  some fixed ?no? expression, like ?no problem? ?no offense?. In these cases, ?problem? and ?of- fense? should not be regarded as features. We  have a list of such words, which are manually  compiled.
440	Explains Non-Technical Concepts	4. Evaluation and Results The goal of our evaluation was to ensure that the system helped a user successfully identify parts irrespective of the performance of the speech recognition engine for the user. In other words, we wanted to see if the system was robust enough to conduct transactions with a diverse mix of users. We tested the system with 4 different users two of whom had foreign accents. For each user, we randomly selected 20 parts from the database. The results are summarized in Table 1.
140	Assumes Prior Knowledge	2 Previous Efforts, CHAT-80, PRAT-89 and HSQL The system, called BusTUC is built upon the clas- sical system CHAT-80 (Warren and Pereira, 1982). CHAT-80 was a state of the art natural anguage sys- tem that was impressive on its own merits, but also established Prolog as a viable and competitive lan- guage for Artificial Intelligence in general. The sys- tem was a brilliant masterpiece of software, efficient and sophisticated. The natural anguage system was connected to a small query system for international geography. The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
334	Explains Technical Concepts	For combining a small amount of task-specific (TS), training with a very large amount of task-independent (TI) train-  ing data, we modified the Katz back-off bigram estimation algo-  rithm \[12\]. A weight was added to reduce the effective size of the  task-independent training database as shown in Equation 1:  C(w2, wl) - Crs(w2, wl) +Y*CTt(W2, wl)  where C (w2, wl)  is the counts of the nurnher of occurrences  of word wl followed by w2, CTS (w2, wl) are the counts from  the task-specific database and Crt (w2, wl) are the counts  from the task-independent da abase. The weight 3, reduces the  effective size of the task-independent database so that these  counts don't overwhelm the counts of the task-specific database.  Table 1 shows both the training set and test set perplexity  for the credit card task as a function of T. The task-specific train-  ing consisted of 18 credit card conversations (59 K words) while  the task-independent training consisted of 1123 general conver-  satious (17 M words).
372	Explains Technical Concepts	Building effective search topics  We have been experimenting with manual and auto-  matic natural language query (or topic, in TREC  parlance) building techniques. This differs from  most query modification techniques used in IR in  that our method is to reformulate the user's state-  ment of information eed rather than the search sys-  tem's internal representation f it, as relevance feed-  back does. Our goal is to devise a method of full-  text expansion that would allow for creating exhaus-  tive search topics such that: (1) the performance  of any system using the expanded topics would be  significantly better than when the system is run us-  ing the original topics, and (2) the method of topic  expansion could eventually be automated or semi-  automated so as to be useful to a non-expert user.  Note that the first of the above requirements effec-  tively calls for a free text, unstructured, but highly  precise and exhaustive description of user's search  statement. The preliminary results from TREC  evaluations how that such an approach is indeed  very effective.
312	Assumes Prior Knowledge	Another issue, that has been discussed previously in the description of the method, is that the distilling is made based on a particular view of what a dialogue with a computer will look like. While not necessari- ly being a detailed and specific model, it is at least an instance of a class of computer dialogue models. One example of this is whether the system is meant to acquire information on the user's underlying mo- tivations or goals or not. In the examples presented, we have not assumed such capabilities, but this as- sumption is not an absolute necessity. We believe, however, that the distilling process should be based on one such model, not the least to ensure a con- sistent treatment of similar recurring phenomena t different places in the corpora.
286	Explains Technical Concepts	3.4 Lex icon  Eva luat ion To evaluate the precision of the English-Chinese translation model trained on the Web corpus, we examined two sample lexicons of 200 words, one in each direction. The 200 words for each lexicon were randomly selected from the training source. We ex- amined the most probable translation for each word. The Chinese-English lexicon was found to have a precision of 77%. The English-Chinese l xicon has a higher precision of 81.5%. Part of the lexicons are shown in Fig. 4, where t / f  indicates whether a translation is true or false. These precisions seem to be reasonably high. They are quite comparable to that obtained by Wu (1994) using a manual Chinese-English parallel cor- pus.
777	Explains Non-Technical Concepts	We restricted our training data to data that was  directly available through the workshop's web- site; we didn?t use the LDC resources mentioned  on the website (e.g., French Gigaword, English  Gigaword). Below, ?mono? refers to all mono- lingual data (Europarl, news-commentary, and  shuffle); ?mono? English is roughly three times  bigger than ?mono? French (50.6 M lines in  ?mono? English, 17.7 M lines in ?mono? French).  ?Domain? refers to all WMT parallel training  data except GigaFrEn (i.e., Europarl, news- commentary, and UN).
211	Explains Technical Concepts	4.3 Top ica l  Concept  Leve l  Ev idence   We have also investigated re-ranking of term level  results based on clustering of the retrieval output. The  idea is that it is often the case documents are ranked  high by matching a query with terms that are related to  different unwanted sub-topics or have different senses  from those used in the query. Examples of the latter  are 'bank', 'deposit' in the money sense, or their river  sense. Other terms may disambiguate he true sense in  a document, but they may not be present or sufficiently  matched to the query. Assuming there are sufficient  number of retrieved ocuments using the terms in their  different senses or for different sub-topics, one could  separate them into groups by clustering the list. Each  group will be characterized by a profile consisting of  terms with the highest occurrence frequency within  each group. The query can now be matched with the  profiles as if they were documents, and the highest  ranked profile group would be promoted in ranking.
558	Explains Technical Concepts	It is interesting that the performance of DP looks  so much better when compared to the complete  agreement score, Pcomp than when compared to  P~j. Recall that Pcomp only reports a  presupposition if all the raters report one. The  high agreement of the raters in these cases can  presumably be explained by the salience of the  presupposition problem. This indicates that DP  makes use of reliable indicators for its  performance. Good agreement with the other  measure, Pmaj, would suggest that DP additionally  reports presuppositions i  cases where humans do  not agree that a presupposition is present. The  higher agreement with the stricter measure is thus  a good result.
407	Mathematically-Oriented Paragraph	2. PREVIOUS WORK  A central problem in the statistical approach to speech  recognition is finding a good model for the probabil-  ity of acoustic observations conditioned on the state in  hidden-Markov models (HMM), or for the case of the  SSM, conditioned on a region of the model. Some of the  options that have been investigated include discrete dis-  tributions based on vector quantization, as well as Gaus-  sian, Gaussian mixture and tied-Gaussian mixture dis-  tributions. In tied-mixture modeling, distributions are  modeled as a mixture of continuous densities, but unlike  ordinary, non-tied mixtures, rather than estimating the  component Gaussian densities eparately, each mixture  is constrained to share the same component densities  with only the weights differing. The probability density  of observation vector x conditioned on being in state i  is thus  p(x Is = i) = Z wikpk(x). (1)  k Note that the component Gaussian densities, Pk(x) -'~  N(t~k, ~k), are not indexed by the state, i. In this light,  tied mixtures can be seen as a particular example of the  general technique of tying to reduce the number of model  parameters that must be trained \[3\].
596	Explains Technical Concepts	8 Conclusions  In this paper, the proposed model improves the  acquirement ability for OOV term translation  through Web mining, and solves the translation  pair selection and evaluation in a novel way by  fusing multiple features and introducing the  supervised learning based on Ranking SVM.  Furthermore, it is significant to apply the key  techniques in machine translation into OOV  term translation, such as OOV term recogni- tion, statistical machine learning, alignment of  sentence and phoneme, and WSD. All these  aspects will be our research focus in the future.
410	Explains Technical Concepts	Thus, the framework represents a generalized  processing environment that can be reused in  different ypes of natural language processing  (NLP) applications. So far the framework has  been used successfully to build a wide variety of  NLG and MT applications in several limited  domains (meteorology, battlefield messages,  object modeling) and for different languages  (English, French, Arabic, and Korean).  In the next sections, we present the design of the  core tree transduction module (Section 2),  describe the representations that it uses (Section  3) and the linguistic resources (Section 4). We  then discuss the processing performed by the  tree transduction module (Section 5) and its  instantiation for different applications (Section  6). Finally, we discuss lessons learned from  developing and using the framework (Section 7)  and describe the history of the framework  comparing it to other systems (Section 8).  2 The Framework's Tree Transduction Module  The core processing engine of the framework is  a generic tree transduction module for lexico-  structural processing, shown in Figure 1. The  module has dependency stuctures as input and  output, expressed in the same tree formalism,  although not necessarily at the same level (see  Section 3). This design facilitates the pipelining  of modules for stratificational transformation. I   fact, in an application, there are usually several  instantiations of this module.
253	Assumes Prior Knowledge	We have not incorporated co-reference resolu-  tion methods in our system yet, but it would seem  that readability can be improved by the ability to  replace pronouns with their referents would be use-  ful. It remains to be seen, however, whether suffi-  cient accuracy can be achieved to support this  method. In cases like this where an error may be  critical for a user of the system we would normally  mark the fact that the text had been added by the  system.
673	Assumes Prior Knowledge	This algorithm, unfortunately, does not scale down  well--it has the property that small attices may not contain the  best recognition hypotheses. This is because one must use small  beam widths to generate small lattices. However, a small beam  width will likely generate pruning errors.  Because of this deficiency, we have developed the  Forward/Backward Word-Life Algorithm described below.  3.2. Extending the Word-Life Algorithm Using  Forward And Backward Recognition Passes  We wish to generate word lattices that scale down  gracefully. That is, they should have the property that when a  lattice is reduced in size, the most likely hypotheses remain and  the less likely ones are removed. As was discussed, this is not the  ease if lattices are sealed down by reducing the beam search  width.
122	Explains Non-Technical Concepts	The DARPA WSJ0 Corpus: The DARPA Wall Street  Journal-based Continuous-Speech Corpus (WSJ)\[22\] has  been designed to provide general-purpose speech data (pri-  marily, read speech data) with large vocabularies. Text  materials were selected to provide training and test data  for 5K and 20K word, closed and open vocabularies, and  with both verbalized and non-verbalized punctuation. The  recorded speech material supports both speaker-dependent  and speaker-independent training and evaluation.
115	Explains Technical Concepts	1. INTRODUCTION  As increasing amounts of computer-readable texts  are becoming available on the web or on CDROMs,  text searching and detection has become an  indispensable tool for information users and analysts of  all walks of life. Up till the late 1980's, research in  text retrieval has been mainly with small collections of  a few thousand items. Since 1990, with the foresight  of the TIPSTER and TREC programs, substantial  progress has been made to advance the state-of-the-art  in text detection and ad-hoc information retrieval (IR)  methodologies. Examples include: availability,  experimentation a d uniform evaluation of gigabyte-  size collections, term weighting improvements, 2-stage  'pseudo-feedback' retrieval strategy, recognition of  difficulties of short queries versus long, use of phrases,  treatment of foreign languages for multilingual  retrieval, among others. This investigation builds upon  previous findings to bring further advances in this field  using our PIRCS system.
374	Explains Non-Technical Concepts	3 Distilling dialogues Distilling dialogues, i.e. re-writing human interac- tions in order to have them reflect what a human- computer interaction could look like involves a num- ber of considerations. The main issue is that in cor- pora of natural dialogues one of the interlocutors i not a dialogue system. The system's task is instead performed by a human and the problem is how to anticipate the behaviour of a system that does not exist based on the performance of an agent with dif- ferent performance characteristics. One important aspect is how to deal with human features that are not part of what the system is supposed to be able to handle, for instance if the user talks about things outside of the domain, such as discussing an episode of a recent TV show. It also involves issues on how to handle situations where one of the interlocuters discusses with someone lse on a different opic, e.g. discussing the up-coming Friday party with a friend in the middle of an information providing dialogue with a customer.
572	Explains Technical Concepts	The progressive search lattice is not viewed as a scored  graph of possible segmentations of the input speech. Rather, the  lattice is simply viewed as a word-transition grammar which  constrains subsequent recognition passes. Temporal and scoring  information is intentionally left out of the progressive search  lattice.
762	Explains Non-Technical Concepts	4 Event-Enriched Sentence Representation  In summarization, an event is an activity or  episode associated with participants, time, place,  and manner. Conceptually, event bridges  sentence and term/entity and partially fills the  semantic gap in the sentence representation.
453	Explains Technical Concepts	A second means of reducing the amount of prepara-  tory work is provided in the form of Multiplication  Rules (MRs). Whereas CRs add further specifications to  a single entry, MRs have the effect of Increasing the  number of entries In some principled way. One applica-  tion of MRs Is to express the fact that nouns and adjec-  tlves do not subcategorize for obligatory complements.  A MR can be written which, for each entry containing  the specification (N +) and some non-NULL value for  SUBCAT, produces a copy of that entry where the SUB-  CAT specification is replaced by (SUBCAT NULL).  The lexicon complies Into two files, one holding mor-  phemes stored in a tree-shaped structure (cf. Thorne et  al. (1968)), and the other holding the expanded entries  relating to them. The comptlatlon of a lexicon can take  a considerable amount of time; our prototype incorporates  a lexicon with approximately 3500 entries, which com-  plies In approximately ninety minutes.
332	Explains Technical Concepts	3.5 Acqu is i t ion  f rom a Raw Corpus   In this section, we show that a raw cortms instead of  a tagged corpus can be used to train the lnodel, if it  is first analyzed by a parser. We used the lnorl)holog-  ical analyzer JUMAN and a tmrser KNP (Kurohashi,  11198) which is based on a det)endency grainlnar,  it, order to extract iuforumtion from a raw corpus  for detecting whether or not each feature is found.  'l?tm accuracy of JUMAN for detecting inorphologi-  cal boundaries and part-of-speech tags is about 98%,  and the parsecs dependency accuracy is about 90%.  These results were obtained from analyzing Mainichi  newspaper articles.
562	Assumes Prior Knowledge	An essential and non-trivial task for the  languages that exhibit inexplicit word boundary  such as Thai, Japanese, and many other Asian  languages undoubtedly is the task in identifying  word boundary. "Word", generally, means a unit  of expression which has universal intuitive  recognition by native speakers. Linguistically,  word can be considered as the most stable unit  which has little potential to rearrangement and is  uninterrupted as well. "Uninterrupted" here  attracts our lexical knowledge bases so much.  There are a lot of uninterrupted sequences of  words functioning as a single constituent of a  sentence. These uninterrupted strings, of course  are not the lexical entries in a dictionary, but each  occurs in a very high frequency. The way to point  out whether they are words or not is not  distinguishable even by native speakers. Actually,  it depends on individual judgement. For example,  a Thai may consider 'oonfila~mu' (exercise) a whole  word, but another may consider 'n~n~m~' as a  compound: 'oon' (take)+ 'filg~' (power)+ 'too' (body).  Computationally, it is also difficult to decide  where to separate a string into words. Even  though it is reported that the accuracy of recent  word segmentation using a dictionary and some  heuristic methods is in a high level. Currently,  lexicographers can make use of large corpora and  show the convincing results from the experiments  over corpora. We, therefore, introduce here a new  efficient method for consistently extracting and  identifying a list of acceptable Thai words.
726	Assumes Prior Knowledge	Although the filler does not appear to be very helpful for the current application domain, it is an important part of the architecture for other application domains. For example, the current PartslD system is a descendant from an earlier system which allowed users to process financial transactions where the filler was instrumental in helping the dialogue manager determine the type of transaction being carried out by the user (Bagga et al, 2000).
270	Explains Technical Concepts	2.2 Data representat ion  In our example sentence in section 2.1, noun  phrases are represented by bracket structures.  It has been shown by Mufioz et al (1999)  that for baseNP recognition, the representa-  tion with brackets outperforms other data rep-  resentations. One classifier can be trained to  recognize open brackets (O) and another can  handle close brackets (C). Their results can be  combined by making pairs of open and close  brackets with large probability scores. We have  used this bracket representation (O+C) as well.  However, we have not used the combination  strategy from Mufioz et al (1999) trot in-  stead used the strategy outlined in Tjong Kim  Sang (2000): regard only the shortest possi-  ble phrases between candidate open and close  brackets as base noun phrases.
90	Explains Non-Technical Concepts	2 A Par t i c ipant ' s  Soc ia l  Ro le  and Po l i teness This section focuses on one participant's social role. We investigated Japanese outputs of a di- alogue translation system to see how many ut- terances hould be polite expressions in a cur- rent translation system for travel arrangement. We input 1,409 clerk utterances into a Transfer Driven Machine Translation system (Sumita and others, 1999) (TDMT for short). The in- puts were closed utterances, meaning the sys- tem already knew the utterances, enabling the utterances to be transferred at a good quality. Therefore, we used closed utterances as the in- puts to avoid translation errors.
3	Assumes Prior Knowledge	6 Related work Wenger (1987), still the chief textbook on ITSs, states that using a global planner to control an ITS is too inefficient to try. This is no longer true, if indeed it ever was. Vassileva (1995) proposes a system based on AND-OR graphs with a separate set of rules for reacting to unexpected events. Lehuen, Nicolle and Luzzati (1996) present a method of dialogue analysis that produces schemata very similar to ours. Earlier dialogue- based ITSs that use augmented finite-state machines or equivalent include CIRCSIM-Tutor (Woo et al 1991, Zhouet al 1999) and the system described by Woolf (1984). Cook (1998) uses levels of finite-state machines. None of these systems provides for predicates with variables or unification.
946	Assumes Prior Knowledge	As a result, it was shown that about 70% (952) of all utterances should be improved to use polite expressions. This result shows that a cur- rent translation system is not enough to make a conversation smoothly. Not surprisingly, if all expressions were polite, some Japanese speakers would feel insulted. Therefore, Japanese speak- ers do not have to use polite expression in all utterances.
119	Mathematically-Oriented Paragraph	Examples of grammar rules. which is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a statement. Movement is easily handled in Consensical Gram- mar without making special phrase rules for each kind of movement. The following example shows how TUC manages a variety of analyses using move- ments: Max said Bill thought Joe believed Fido Barked. Who said Bill thought Joe believed Fido barked? ==> Max Who did Max say thought Joe believed Fido barked? ==> Bill statement(P) ---> noun_phrase(X,VP,P), verb_phrase(X,VP). statement(Q) ---> verb_complementsO(VC), ZZ initial optional verb complements statement(Q) -... verb_complementsO(VC). ZZ may be inserted after a gap whoseq(P) ---> Z whose dog barked? \[whose\], hOlm(N), whoq(P) - ~ without gap (\[who\],\[has\],\[a\],noun(N),\[that\]). whoq(P) ---> \[who\], whichq(P) - (\[which\],\[person\]). whichq(which(X)::P) ---> \[which\], statement(P) - the(X). Example: Whose dog barked? is analysed as if the sentence had been Who has a dog that  barked? which is analysed as Which person has a dog that  barked? Who did Max say Bill thought believed Fido barked? ==> Joe
45	Assumes Prior Knowledge	One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s ta t ion of departure, station of arrival, earliest departure timeand/or latest arrival time. It is a myth that natural language is a better way of communication because it is "natural language". The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode. To do that, the system has to be correct, user friendly and almost complete within the actual domain.
716	Explains Technical Concepts	The detector for presuppositions (DP) is part of the  computer tool QUAID (Graesser, Wiemer-  Hastings, Kreuz, Wiemer-Hastings & Marquis, in  press), which helps survey methodologists design  questions that are easy to process. DP detects a  presupposition and reports it to the survey  methodologist, who can examine if the  presupposition is correct. QUAID is a  computerized QUEST questionnaire valuation  aid. It is based on QUEST (Graesser & Franklin,  1990), a computational model of the cognitive  processes underlying human question answering.  QUAID critiques questions with respect to  unfamiliar technical terms, vague terms, working  memory overload, complex syntax, incorrect  presuppositions, and unclear question purpose or  category. These problems are a subset of potential  problems that have been identified by Graesser,  Bommareddy, Swamer, and Golding (1996; see  also Graesser, Kennedy, Wiemer-Hastings &  Ottati, 1999).
887	Explains Technical Concepts	Another key feature of this paper is the careful selection of an accuracy measure appropriate to the circumstances of the application. The standard measure, percent of monolingual performance achieved, is used, with a firm focus on precision. In this application, users are able to evaluate only what they see, and generally have no idea what else is present in the collection. As a result, precision is of far more interest o customers than recall. Recall is, however, of interest to image suppliers, and in any case it would not be prudent to optimize for precision without taking into account the recall tradeoff.
776	Explains Non-Technical Concepts	1 Introduction This paper describes the machine translation sys- tems developed by the Computer Science labora- tory at the University of Le Mans (LIUM) for the 2010 WMT shared task evaluation. We only con- sidered the translation between French and En- glish (in both directions). The main differences with respect to previous year?s system (Schwenk et al, 2009) are as follows: restriction to the data recommended for the workshop, usage of the (fil- tered) French?English gigaword bitext, pruning of the phrase table, and usage of automatic trans- lations of the monolingual news corpus to im- prove the translation model. We also used a larger amount of bilingual data extracted from compara- ble corpora than was done in 2009. These different points are described in the rest of the paper, to- gether with a summary of the experimental results showing the impact of each component.
724	Explains Technical Concepts	The ranking model is based on annotation his- tory and influences the future progress of tree- banking. It can be dynamically integrated into the treebank development cycles in which the anno- tation habit evolves over time. Such a model can also shorten the training period for new annota- tors, which is an interesting aspect for our future investigation.
856	Explains Technical Concepts	To implement this approach we use a simple "bakery" al-  gorithm to assign tasks: as each machine becomes free, it  reads and increments the value of a counter from a com-  mon location indicating the sentences in the database  it should work on next. This approach provides load  balancing, allowing us to make efficient use of machines  that may differ in speed. Because of the coarse grain of  parallelism (one task typically consists of processing 10  sentences), we can use the relatively simple mechanism  of file locking for synchronization a d mutual exclusion,  with no noticeable fficiency penalty. Finally, one pro-  cessor is distinguished as the "master" processor and is  assigned to perform the collation and normalization of  counts at the end of each pass. With this approach, we  obtain a speedup in training linear with the number of  machines used, providing a much faster environment for  experimentation.
141	Explains Non-Technical Concepts	Other applications, uch as for financial or banking trans-  actions, or access to confidential information, such as fi-  nancial, medical or insurance records, etc., require accurate  identification or verification of the user. Typically security  is provided by the human who "recognizes" the voice of  the client he is used to dealing with (and often will also be  confirmed by a fax), or for automated systems by the use  of cards and/or codes, which must be provided in order to  access the data. With the widespread use of telephones,  and the new payment and information retrieval services of-  fered by telephone, it is a logical extension to explore the  use of speech for user identification. An advantage is that  if text-independent speaker verification techniques are used,  the speaker's identity can be continually verified during the  transaction, in a manner completely transparent to the user.  This can avoid the problems encountered by theft or dupli-  cation of cards, and pre-recording of the user's voice during  an earlier transaction.
820	Assumes Prior Knowledge	Cross -document  Core ference   Cross-document coreference occurs when the same  person, place, event, or concept is discussed in more  than one text source. Computer ecognition of this  phenomenon is important because it helps break  "the document boundary" by allowing a user to  examine information about a particular entity from  multiple text sources at the same time. In partic-  ular, resolving cross-document coreferences allows  a user to identify trends and dependencies across  documents. Cross-document coreference can also  be used as the central tool for producing summaries  from multiple documents, and for information fu-  sion, both of which have been identified as advanced  areas of research by the T IPSTER Phase I I I  pro-  gram. Cross-document coreference was also iden-  tified as one of the potential tasks for the Sixth  Message Understanding Conference (MUC-6) but  was not included as a formal task because it was  considered too ambitious \[10\].
444	Explains Technical Concepts	In addition, there is another ule base for actually generating the natural anguage answers (120 rules). The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the com- munication and CGI-scripts. At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog). Average response time is usually less than 2 sec- onds, but there are queries that demand up to 10 seconds. The error rate for single, correct, complete and relevant questions is about 2 percent.
918	Assumes Prior Knowledge	To further visualize the effect of the ranking model, we highlighted with color the discrimi- nants which are manually annotated by annotator B under a basic setting without using the ranking models. 75% of these ?prominent? discriminants are grouped within the top-25% region of the plot. Without surprise, the model B gives an average relative ranking of 0.18 as oppose to 0.21 with model A. The overall distribution of rankings for manually disambiguated discriminants are shown in Figure 3.
494	Explains Non-Technical Concepts	The other works can be found in the  research on the Japanese language. Nagao et al  (1994) has provided an effective method to  construct a sorted file that facilitates the  calculation of n-gram data. But their algorithm did  not yield satisfactory accuracy; there were many  iuwflid substrings extracted. The following work  (lkehara et al, 1995) improved the sorted file to  avoid repeating in counting strings. The extraction  cesult was better, but the determination of the  longest strings is always made consecutively from  left to right. If an erroneous tring is extracted, its  errors will propagate through the rest of the input  strings.
682	Explains Technical Concepts	Table 7 shows the gains obtained from us- ing this corpus in both the translation model and the language model opposed to a baseline sys- tem trained with otherwise the same settings. For French?English we see large gains (+1.23), but not for English?French (+0.10). Our official submission for the French?English language pairs used these models. They did not in- clude a part-of-speech language model and bigger beam sizes.
70	Assumes Prior Knowledge	We also tbund a very close correspondence  between the ~ibnic in SFG and the nuclear ac-  cented syllable in the Tom analysis: In virtu-  ally all cases they were in exactly the same place  in the analyses. When the utteran(:es are more  (:on lplex, e.g., they have a 1)retonic segment, or  consist of sequences, in l;he ToBI analysis 1)itch  accents are also lint in other places, not just  on the mmlear accented syllaMe. ToBI analysis,  unlike SFC, allows for more than just the nu-  clear accented syllable to be marked up. The  extra pitch accents from the ToBI analysis are  potential ly a problem for a ToBI-SFG mapping.  However, closer examination of the placelnent of  these other 1)itch accents revealed that they al-  ways fall on the first syllable of a foot (also when  that is not the one carrying the nuclear stress).  This suggests that the SFG feet can give some  information about where these other pitch ac-  cents are likely to tM1 or, that these other pitch  accents may be an indication of toot boundaries.
937	Assumes Prior Knowledge	5.1. Reducing the time required to perform  speech recognition experiments  At SRI, we've been experimenting with large-  vocabulary tied-mixture speech recognition systems. Using a  standard ecoding approach, and average decoding times for  recognizing speech with a 5,000-word bigram language model  were 46 times real time. Using lattices generated with beam  widths of le-38 and a LatticeThresh of le-18 we were able to  decode in 5.6 times real time). Further, there was no difference in  recognition accuracy between the original and the lattice-based  system.
806	Explains Technical Concepts	In order to overcome these limitations, many IR  systems allow varying degrees of user interaction  that facilitates query optimization and calibration  to closer match user's information seeking goals. A  popular technique here is relevance feedback, where  the user or the system judges the relevance of a sam-  ple of results returned from an initial search, and the  query is subsequently rebuilt to reflect this informa-  tion. Automatic relevance feedback techniques can  lead to a very close mapping of known relevant doc-  uments, however, they also tend to overfit, which in  turn reduces their ability of finding new documents  on the same subject. Therefore, a serious challenge  for information retrieval is to devise methods for  building better queries, or in assisting user to do  SO.
819	Explains Technical Concepts	Automated document summarization dates back to  Luhn's work at IBM in the 1950's \[12\], and evolved  through several efforts including Tait \[24\] and Paice in  the 1980s \[17, 18\]. Much early work focused on the  structure of the document to select information. In the  1990's everal approaches to summarization blossomed,  include trainable methods \[10\], linguistic approaches \[8,  15\] and our information-centric method \[2\], the first to  focus on query-relevant summaries and anti-redundancy  measures. As part of the TIPSTER program \[25\], new  investigations have started into summary creation using  a variety of strategies. These new efforts address query  relevant as well as "generic" summaries and utilize a  variety of approaches including using co-reference  chains (from the University of Pennsylvania) \[25\], the  combination of statistical and linguistic approaches  (Smart and Empire) from SaBir Research, Cornell  University and GE R&D Labs, topic identification and  interpretation from the ISI, and template based  summarization from New Mexico State University \[25\].  In this paper, we discuss the Maximal Marginal  Relevance method (Section 2), its use for document  reranking (Section 3), our approach to query-based  single document summarization (Section 4), and our  approach to long documents (Section 6) and multi-  document summarization (Section 6). We also discuss  our evaluation efforts of single document summarization  (Section 7-8) and our preliminary results (Section 9).
729	Assumes Prior Knowledge	This algorithm develops a grammar which contains all  whole-word hypotheses the early-pass speech recognition  algorithm considered. If a word hypothesis was active and the  word was processed by the recognition system until the word  finished (was not pruned before transitioning to another word),  then this word will be generated as a lattice node. Therefore, the  size of the lattice is directly controlled by the recognition  seareh's beam width.
958	Explains Non-Technical Concepts	In real translation production scenarios, Machine Translation is usually used to complement transla- tion memory tools (TM tool). Translation memories are databases that contain text segments (usually sen- tences) that are stored together with their translations. Each such pair of source and target language segments is called a translation unit. Translation units also con- tain useful meta-data (creation date, document type, client, etc.) that allow us to filter the data both for trans- lation and machine translation purposes. A TM tool tries to match the segments within a doc- ument that needs to be translated with segments in the translation memory and propose translations. If the memory contains an identical string then we have a so- called exact or 100% match which yields a very reliable translation. Approximate or partial matches are called fuzzy matches and usually, the minimum value of a fuzzy match is set to 65%?70%. Lower matches are not considered as usable since they demand more edit- ing time than typing a translation from scratch. First experiments have shown that the quality of SMT out- put for certain language pairs is equal or similar to 70% fuzzy matches.
292	Explains Technical Concepts	Abst ract   Tlle paper describes the results of a compari-  son of two annotation systems for isstoslal;ion,  the tone-based ToBI al)proach and the 1;une-  based api)roach proposed by Systemic Func-  ti(mal Grammar (SFO). The goal of this compar-  ison is to detine a mapping between the two sys-  tems tbr the purpose of concept-to-speech gen-  eration of English. Since ToB: is widely used  in Sl)eech synthesis and SFG is widely used in  nal;ural language generation and oft~rs a lin-  guistically motivated aecollnt of intonation, it;  appears a promising step to comt)ine the two  approaches for concept-to-speech. A corpus of  English utterances has been analysed with both  ~\].~()13I and SFG categories; eomparison of the  analysis results has lead to the identification of  some basic equivalents between the two systems  on which a mapping can be based.
213	Explains Technical Concepts	Count Count of Count Discount Count* 1 357,929,182 0.140 0.140 2 24,966,751 0.487 0.975 3 8,112,930 0.671 2.014 4 4,084,365 0.714 2.858 5 2,334,274 0.817 4.088 Table 4: Good Turing smoothing, as in the French?English model: counts, counts of counts, discounting factor and discounted count
330	Explains Technical Concepts	As we mentioned previously, our approach has  been to balance methods of relating the query to  sentences in the document. The nearly 100% recall  of the dry-run summaries encouraged us, and we  even used the output of those summaries to pro-  vide a test-bed for evaluating our summaries. Al-  though we never actively sought o emulate aspects  of other systems directly, our final algorithm does  share some basic ideas and approaches from those  systems. Some of the similarities are listed below:  In \[4\], they eliminate redundant information from  summaries by classifying sentences according to  Maximal Marginal Relevance (MMR). MMR ranks  text chunks according to their dissimilarity to one  another. Summaries can then be produced with  sentences that are maximally dissimilar, thereby  increasing the likelihood that distinguishing infor-  mation will be in the summary. One can view our  coverage requirement for terms in the query as an  attempt o pick dissimilar sentences from the doc-  ument. Instead of MMR, we use the fact that a  sentence which does not contain redundantly re-  ferring phrases to the query is more highly ranked  than a sentence that does.
748	Explains Technical Concepts	3.1 System (~ESiLKO The greatest problem of the word-for-word translation approach (for languages with very similar syntax and word order, but different morphological system) is the problem of morphological ambiguity of individual word forms. The type of ambiguity is slightly different in languages with a rich inflection (majority of Slavic languages) and in languages which do not have such a wide variety of forms derived from a single lemma. For example, in Czech there are only rare cases of part-of-speech ambiguities ( t~t \[to stay/the state\], zena \[woman/chasing\] or tri \[three/rub(imperative)\]), much more frequent is the ambiguity of gender, number and case (for example, the form of the adjective jam\[ \[spring\] is 27-times ambiguous). The main problem is that even though several Slavic languages have the same property as Czech, the ambiguity is not preserved. It is distributed in a different manner and the "form-for-form" translation is not applicable.
905	Assumes Prior Knowledge	3.2 Transfer Rules and Entries according to Information on Dialogue Part ic ipants For this research, we modified the transfer ules and the transfer dictionary entries, as shown in Figures 5 and 6. In Figure 5, the target pattern "target pattern 11" and the source word "source example 1" are used to change the translation according to information on dialogue partici- pants. For example, if ":pattern-cond 11" is de- fined as ":h-gender male" as shown in Figure 7, then "target pattern 11" is selected when the hearer is a male, that is, "("Mr." x)" is selected. Moreover, if ":word-cond 11" is defined as ":s- role clerk" as shown in Figure 8, then "source example 1" is translated into "target word 11" when the speaker is a clerk, that is, "accept" is translated into "oukesuru." Translations uch as "target word 11" are valid only in the source pattern; that is, a source example might not always be translated into one of these target words. If we always want to produce transla- tions according to information on dialogue par- ticipants, then we need to modify the entries in the transfer dictionary like Figure 6 shows. Conversely, if we do not want to always change the translation, then we should not modify the entries but modify the transfer ules. Several conditions can also be given to ":word-cond" and ":pattern-cond." For example, ":s-role cus- tomer and :s-gender female," which means the speaker is a customer and a female, can be given. In Figure 5, ":default" means the de- fault target pattern or word if no condition is matched. The condition is checked from up to down in order; that is, first, ":pattern-cond 11," second, ":pattern-cond 1~," ... and so on. (X (V-CN) Y) ((y "wo" x) ((("accept") ("payment")) (("take") ("picture"))) ((("accept") -~ ("oukesuru"):s-role clerk ( "accept" ) --+ ( "uketsukeru" ) )) ) Figure 8: Transfer ule example with a partici- pant's role ((("payment") --~ ("oshiharai") :s-role clerk ( "payment" ) ---* ( "shiharai" )) (("we") --* ("watashidomo") :s-role clerk ("we") --~ ("watashltachi")))
901	Explains Non-Technical Concepts	3. SEARCH  The DECIPHER a~ system uses a time-synchronous  beam search. A partial Viterbi baektrace \[6\] is used to locate the  most-likely Viterbi path in a continuous running utterance. The  Viterbi backtrace contains both language model information  (grammar t ansition probabilities into and out of the keyword),  acoustic log likelihood probabilities for the keyword, and the  duration of the keyword hypothesis.  A duration-normalized likelihood score for each key-  word is computed using the following Equation 2:  AP + GP + Constant  KeyScore = Duration  where AP is the acoustic log-likelihood score for the keyword,  and GP is the log probability of the grammar transition into the  keyword, and Constant is a constant added to the score to penal-  ize keyword hypotheses that have a short duration. None of the  earlier HMM keyword systems used a bigram language in either  the decoding or the scoring. Many previous systems did use  weights on the keywords to adjust he operating location on the  ROC curve.
836	Explains Non-Technical Concepts	The DARPA TIMIT Corpus: The DARPA TIMIT  Acoustic-Phonetic Continuous Speech Corpus\[4\] is a cor-  pus of read speech designed to provide speech data for the  acquisition of acoustic-phonetic knowledge and for the de-  velopment and evaluation of automatic speech recognition  systems. TIMIT contains a total of 6300 sentences, 10 sen-  tences poken by each of 630 speakers from 8 major dialect  regions of the U.S. The TIMIT CDROM\[4\] contains atrain-  ing/test subdivision of the data that ensures that there is no  overlap in the text materials. All of the utterances in TIMIT  have associated time-aligned phonetic transcriptions.
87	Explains Technical Concepts	2.2.4 The Dialogue Manager (DM) The DM receives as input from the filler the set of templates which are checked off. In addition, it also receives two lists containing the list of description words, and product word uttered by the user. The DM proceeds using the following algorithm: 1) It first checks the set of checked off templates input from the filler. If there is exactly one template in this set, the DM asks the user to confirm the part that the template corresponds to. Upon receipt of the confirmation from the user, it returns the identification number of the part to the user. 2) Otherwise, for each description word uttered by the user, the DM looks up the set of parts (or templates) containing the word from the descriptions inverted hash table. It then computes the intersection of these sets. If the intersection is empty, the DM computes the union of these sets and proceeds treating the union as the intersection. 3) If the intersection obtained from (2) above contains exactly one template, the DM asks the user to confirm the part corresponding to the template as in (1) above. 4) Otherwise, the DM looks at the set of product words uttered by the user. If this set is empty, the DM queries the user for the product name. Since the DM is expecting a product name here, the input provided by the user is handled by the context-based parser. Since most product names consist of non- standard words consisting of alpha-numeric characters (examples: AMX3, 8000BUCKY, etc.), the recognition quality is quite poor. Therefore, the context-based parser anks the input received from the user using a sub-string matching algorithm that uses character-based unigram and bigram counts (details are provided in the next section). The sub-string matching algorithm greatly enhances the performance of the system (as shown in the sample dialogue below). 5) If the set of product words is non-empty, or if the DM has successfully queried the user for a product name, it extracts the set of parts (templates) containing each product word from the product words inverted hash table. It then computes an intersection of these sets with the intersection set of description words obtained from (2) above. The resulting intersection is the joint product and description i tersection. 6) If the joint intersection has exactly one template, the DM proceeds as in (1) above. Alternatively, if the number of templates in the joint intersection is less than 4, the DM lists the parts corresponding toeach of these and asks the user to confirm the correct one. 7) If there are more than 4 templates in the joint intersection, the DM ranks the templates based upon word overlap with the description words uttered by the user. If the number of resulting top-ranked templates i less than 4, the DM proceeds as in the second half of (6) above. 8) If the joint intersection is empty, or in the highly unlikely case of there being more than 4 top-ranked templates in (7), the DM asks the user to enter additional disambiguating information.
652	Explains Technical Concepts	Dynamic  Core ference-Based Summar izat ion   We have developed a query-sensitive t xt summa-  rization technology well suited for the task of deter-  mining whether a document is relevant o a query.  Enough of the document is displayed for the user  to determine whether the document should be read  in its entirety. Evaluations indicate that summaries  are classified for relevance nearly as well as full doc-  uments. This approach is based on the concept that  a good summary will represent each of the topics  in the query and is realized by selecting sentences  from the document until all the phrases in the query  which are represented in the summary are 'covered.'  A phrase in the document is considered to cover a  phrase in the query if it is coreferent with it. This  approach maximizes the space of entities retained  in the summary with minimal redundancy. The  software is built upon the CAMP NLP system \[3\].
428	Explains Technical Concepts	We also investigated phrasal evidence for retrieval,  but only to the extent hat it is used to refine results that  have been obtained via term level retrieval. Only long  queries are considered since queries with too few  phrases would not provide sufficient evidence to work  with. Specifically, we use phrasal evidence to re-rank  a retrieved document list so as to promote more  relevant documents earlier in the list. This could lead  to higher density of true relevant documents in the 1 st  stage retrieval, thereby improving 'pseudo-feedback'  for the 2 nd stage downstream. The 2 "d stage retrieval  list could similarly be re-ranked to return better  effectiveness a well.
202	Explains Technical Concepts	There was a significant drop between the  training and blind sets in event extraction: 11  points. We believe that the main reason is that  the total number of events in the training set is  fairly low: 801 instances of 61 types of events  (an average of 13/event), where 35 of the event  types had fewer than 10 instances. In fact, 9  out of the 14 event types which scored lower  than 40% F-Measure had fewer than I0  examples. In comparison, there were 34,000  instances of 39 types of relations in the training  set.
842	Assumes Prior Knowledge	2 Integrated planning and execution for dialogue generation 2.1 'Practical reason' and the BDI model For an ITS, planning is required in order to ensure a coherent conversation as well as to accomplish tutorial goals. But it is impossible to plan a whole conversation in advance when the student can respond freely at every turn, just as human beings cannot plan their daily lives in advance because of possible changes in conditions. Classical planning algorithms are inappropriate because the tutor must be able to change plans based on the student's responses.
543	Explains Non-Technical Concepts	Figure 4: Specification ofLexeme SELL  At the conceptual level, the conceptual lexicon  associates lexical-structural mapping with  concepts in a similar way. Figure 5 illustrates  the mapping at the deep-syntactic level  associated with the concept #TEMPERATURE.  Except for the slight differences in the labelling,  this type of specification is similar to the one  used on the lexical level. The first mapping rule  corresponds to one of the lexico-structural  transformations u ed to convert he interlingual  ConcS of Figure 3 to the corresponding DSyntS.  ZONCEPT:  #TEMPERATURE  5EXICAL:  \[  L~-RULE:   #TEMPERATURE ( #min imum SX  #maxim~ $Y  <- ->  LOW ( ATTR $X  ATTR TO  ( II H IGH  ( ATTR SY ) ) )  LEX-RULE:   #TEMPERATURE ( #min im~ SX  <- ->  LOW ( ATTR $X )  LEX-RULE:   #TEMPE~TURE ( #max imum $X  <- ->  H IGH ( ATTR SX )  \]
497	Explains Non-Technical Concepts	To obtain empirical data for the Atlas-Andes plan operators, we analyzed portions of a corpus of human tutors helping students olve similar physics problems. Two experienced tutors were used. Tutor A was a graduate student in computer science who had majored in physics; tutor B was a professional physics tutor.
435	Explains Technical Concepts	2.1. Wall Street Journal Corpus  The Wall Street Joumal (WSJ) pilot CSR corpus contains training  speech read from processed versions of the Wall Street Journal.  The vocabulary is inherently unlimited. The text of 35M words  available for language modeling contains about 160,000 different  words. ?~e data used for speech recognition training and test  was constrained to come from sentences that contained only the  64,000 most frequent words.
940	Explains Non-Technical Concepts	1 In t roduct ion   in the Thai language, there is no explicit word  boundary; this causes a lot of problems in Thai  language processing including word  segmentation, information retrieval, machine  translation, and so on. Unless there is regularity in  defining word entries, Thai language processing  will never be effectively done. The existing Thai  language processing tasks mostly rely on the  hand-coded dictionaries to acquire the information  about words. These manually created ictionaries  have a lot of drawbacks. First, it cannot deal with  words that are not registered in the dictionaries.  Second, because these dictionaries are manually  created, they will never cover all words that occur  in real corpora. This paper, therefore, proposes an  automatic word-extraction algorithm, which  hopefully can overcome this Thai language-  processing barrier.
277	Explains Technical Concepts	DEFINITIONS AND RESEARCH QUESTIONS  We define a q,ery as a natural language  expression of a user's need. For sonic query and  some collection of documents, it is possible for a  human to attribute the relevance of the document o  the query. A retrieral system is a machine that  accepts a query and full texts of documents, and  produces, for each document, a relevance score for  the query-document pair. A measure of the  effectiveness of a retrieval system for a query and a  collection is precision, the proportion of the N  documents with the highest relevance scores that are  relevant (in our study, N is 5, 10, or 30).
625	Explains Technical Concepts	In recent years, computational grammars have been employed to assist the construction of such language resources. A typical development model involves a parser which generates candidate anal- yses, and human annotators who manually iden- tify the desired tree structure. This treebanking method dramatically reduces the cost of train- ing annotators, for they are not required to spon- taneously produce linguistic solutions to vari- ous phenomena. Instead, they are trained to associate their language intuition with specific linguistically-relevant decisions. How to select and carefully present such decisions to the an- notators is thus crucial for achieving high an- notation speed and quality. On the other hand, for large treebanking projects, parallel annota- tion with multiple annotators is usually neces- sary. Inter-annotator agreement is a crucial quality measure in such cases. But improvements on an- notation speed should not be achieved at expense of the quality of the treebank.
281	Explains Technical Concepts	There are two speech training sets. One has 600 sentences  from each of 12 speakers (6 male and 6 female). The other has  a total of 7,200 sentences from 84 different speakers. The total  vocabulary in the training set is about 13,000 words. There are  two different standard bigram language models that are typically  used - oue with 5,000 (SK) words and one with 20,000 (20K)  words. 'Hie 5K language models were designed to include aU of  the words in the 5K test set. The 20K language models contain  the most likely 20K words in the corpus. As a result, about 2% of  the words in the test speech are not in this vocabulary. In addition,  there are two variants depending on whether the punctuation is  read out loud: verbalized punctuation (VP) and nonverbalized  punctuation (NVP).
845	Assumes Prior Knowledge	2.2.6 The Presentation Module The presentation module works in one of two possible modes: over the phone, and over the web. This module takes as input a string generated by the question-generation module and presents this string to the user in the appropriate mode of communication. If the speech option for the system is turned on, the speech-based output is generated using Lernout "~ld. 34 and Hauspie's RealSpeak text-to-speech system. Although the system currently cannot use both modes of communication simultaneously, we plan to incorporate this feature sometime in the future.
898	Explains Non-Technical Concepts	Abstract  We discuss those techniques which, in the  opinion of the authors, are needed to support  robust automatic summarization. Many of  these methods are already incorporated in a  multi-lingual summarization system, MINDS,  developed at CRL. The approach is sentence  selection, but includes techniques to improve  coherence and also to perform sentence reduc-  tion. Our methods are in distinct contrast o  those approaches to summarization by deep  analysis of a document followed by text gener-  ation.
732	Explains Non-Technical Concepts	5 Conc lus ions   In this paper we have presented the results  of a comparison between the ToBI and the  SFG systems for analysing intonation. The  goal of this comparison has been to establish  equivalents between them. The motivation be-  hind this is to make the two systems collabo-  rate in concept-to-speech generation: Tom is a  phonetic-phonological approach to the deserip-  tion of intonation, SFG offers a linguistic ap-  proach to intonation, tbcusing on the meaning-  ful intonation patterns. ToBI i8 widely used in  speech synthesis, SFG is widely used in natu-  ral language generation. It seems therefore a  promising step to combine the two approaches  tbr concept-to-speech generation.
624	Assumes Prior Knowledge	1.3 Lessons learned  f rom RUSLAN We have learned several lessons regarding the MT of closely related languages: ? The transfer-based approach provides a similar quality of translation both for closely related and typologically different languages ? Two main bottlenecks of full-fledged transfer-based systems are: - complexity of the syntactic dictionary - relative unreliability of the syntactic analysis of the source language Even a relatively simple component (transducing dictionary) was equally complex for English-to-Czech and Czech-to-Russian translation Limited text domains do not exist in real life, it is necessary to work with a high coverage dictionary at least for the source language.
32	Explains Technical Concepts	4 Conc lus ion  This paper described a method of acquiring word or-  der froln corpora. We defined word order as the order  of lnodifiers which depend on tile same lnodifiee. The  lnethod uses a model which estimates the likelihood  of the apt)ropriate word order. The lnodel automat-  ically discovers what the tendency of the word order  in Japanese is by nsing various ldnds of information  in and arouud the target bunsetsus plus syntactic  and contextual inforlnation. The contribution rate  of each piece of inforination in deciding word order  is efficiently learned by a model implemented within  an ),,I.E. framework. Comparing results of experi-  ments controlling for each piece of information, we  found that the type of inforinatiou having the great~  est influence was the case marker or inflection type in  a bunsetsu. Analyzing the relationship between the  amount of training data and the agreement rate, we  fimnd that word order could be acquired even with  a small set of training data. We also folmd that a  raw cortms as well as a tagged cortms can be used to  train the model, if it is first, analyzed by a parser. The  agreement rate was 75.41% for the Kyoto University  corpus. We analyzed the lnodifiees whose modifiers'  word order did not agree with that in the original  text, and folmd that 48% of theln were in a natural  order. This shows that, in umny cases, word order  in Japanese is relatively free and several orders are  acceptable.
398	Assumes Prior Knowledge	Another important clue to sentence ordering  is the sentence positional information in a  source document, or ?precedence relation?,  which is utilized by Okazaki et al (2004) in  combination with topical clustering. Those works are all relevant to the current  work because we seek ordering clues from  chronological order, lexical cohesion, entity  transition, and sentence precedence. But we also  add an important member to the panoply ? event.   Despite its intuitive and conceptual appeal,  event is not as extensively used in  summarization as term or entity. Filatova and  Hatzivassiloglou (2004) use ?atomic events? as  conceptual representations in MDS content  selection, followed by Li et al (2006) who treat  event terms and named entities as graph nodes  in their PageRank algorithm. Yoshioka and  Haraguchi (2004) report an event reference- based approach to MDS content selection for  Japanese articles. Although ?sentence  reordering? is a component of their model, it  relies merely on textual and chronological order.  Few published works report using event  information in MDS sentence ordering. Our work will represent text content at two  levels: event vectors and sentence vectors. This  is close in spirit to Bromberg?s (2006) enriched  LSA-coherence model, where both sentence and  word vectors are used to compute a centroid as  the topic of the text.
129	Explains Non-Technical Concepts	Abstract  Metaphorical and contextual affect de- tection from open-ended text-based di- alogue is challenging but essential for  the building of effective intelligent user  interfaces. In this paper, we report up- dated developments of an affect detec- tion model from text, including affect  detection from one particular type of  metaphorical affective expression and  affect detection based on context. The  overall affect detection model has been  embedded in an intelligent conversa- tional AI agent interacting with human  users under loose scenarios. Evaluation  for the updated affect detection compo- nent is also provided. Our work contri- butes to the conference themes on sen- timent analysis and opinion mining and  the development of dialogue and con- versational agents.
126	Assumes Prior Knowledge	7 Related Work A number of previous researchers have taken steps toward joint models in NLP and informa- tion extraction, and we mention some recently proposed, closely related approaches here. Roth and Yih (2007) considered multiple constraints between variables from tasks such as named en- tities and relations, and developed a integer lin- ear programming formulation to seek an optimal global assignment to these variables. Zhang and Clark (2008) employed the generalized per- ceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. Toutanova et al (2008) presented a model captur- ing the linguistic intuition that a semantic argu- ment frame is a joint structure, with strong depen- dencies among the arguments. Finkel and Man- ning (2009) proposed a discriminative feature- based constituency parser for joint named entity recognition and parsing. And Dahlmeier et al (2009) proposed a joint model for word sense dis- ambiguation of prepositions and semantic role la- beling of prepositional phrases. However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role label- ing, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. Since we capture rich and com- plex dependencies between subtasks via potential functions in probabilistic graphical models, our approach is general and can be easily applied to a variety of NLP and IE tasks.
341	Explains Technical Concepts	This means that the minimun W(ij) is selected  among the EDGs which share VTX(i) on their left side.  By repeating (2) and (3), the minimum sum of the  weighted-morpheme number can be got as W(O). Then a  quasi-best path which has  a least weighted-morpheme  number can be easily obtained by tracing the minimum  W(ij) starting from the VTX(O). Since the complexity of  the above process is on an oder of n, the quasi-best path  can be obtained very efficiently.
441	Mathematically-Oriented Paragraph	 Figure 5: l~2rror rate vs. training data size.  The role of the punctuation marks in achieving  reliable classification restilts can be further  illustrated by examining the relation between  classification accuracy and training data size.  Towards this end, we applied discriminant  analysis to different raining corpora consisting  of 10 to 20 text samples from each genre taking  into account he frequencies of occurrence of the  30 most flequent words of the BNC. This  procedure was followed once again taking into  account he eight additional style markers of the  punctuation marks (i.e., totally 38 style  markers). The comparative results are given in  figure 5. As can been seen, the perlbrmance of  the model taking into account only word  frequencies is affected dramatically by the  decrease of the training data. On the other hand,  the performance of the model taking into  account both word and punctuation mark  frequencies remains satisfactory (i.e., error rate  < 7%) using 13 to 20 text san\]ples from each  genre.
970	Explains Non-Technical Concepts	Introduction  Presuppositions are propositions that take some  information as given, or as "the logical  assumptions underlying utterances" (Dijkstra &  de Smedt, 1996, p. 255; for a general overview,  see McCawley, 1981). Presupposed information  includes state of affairs, such as being married;  events., such as a graduation; possessions, uch as  a house, children, knowledge about something;  and others. For example, the question, "when did  you graduate from college", presupposes the  event that the respondent did in fact graduate  from college. The answer options may be ranges  of years, such as "between 1970 and 1980".  Someone who has never attended college can  either not respond at all, or give a random (and  false) reply. Thus, incorrect presuppositions  cause two problems. First, the question is  difficult o answer. Second, assuming that people  feel obliged to answer them anyway, their  answers present false information. This biases  survey statistics, or, in an extreme case, makes  them useless.
765	Explains Technical Concepts	3.1. Semi-Continuous HMMs  The semi-continuous hidden Markov model (SCHMM) \[12\]  has provided us with an an excellent tool for achieving detailed  modeling through parameter sharing. Intuitively, from the  continuous mixture HMM point of view, SCHMMs employ a  shared mixture of continuous output probability densities for  each individual HMM. Shared mixtures ubstantially reduce  the number of free parameters and computational complex-  ity in comparison with the continuous mixture HMM, while  maintaining, reasonably, its modeling power. From the dis-  crete HMM point of view, SCHMMs integrate quantization  accuracy into the HMM, and robustly estimate the discrete  output probabilities by considering multiple codeword can-  didates in the VQ procedure. It mutually optimizes the VQ  codebook and HMM parameters under a unified probabilistic  framework \[13\], where each VQ codeword is regarded as a  continuous probability density function.  For the SCHMM, an appropriate acoustic representation for  the diagonal Gaussian density function is crucial to the recog-  nition accuracy \[13\]. We first performed exploratory semi-  continuous experiments on our three-codebook system. The  SCHMM was extended to accommodate a multiple feature  front-end \[13\]. All codebook means and covariance matrices  were reestimated together with the HMM parameters except  the power covariance matrices, which were fixed. When three  codebooks were used, the diagonal SCHMM reduced the er-  ror rate of the discrete HMM by 10-15% for the RM task \[16\].  When we used our improved 4-codebook MFCC front-end,  the error rate reduction is more than 20% over the discrete  HMM.
80	Explains Non-Technical Concepts	1 I n t roduct ion   A large ('.vent consists of a, number of smaller  events. These component events are usually  related trot such relations may not be strong  enough to define larger topics. For example, a  war may consist of opening, battles, negotia-  tions, and so on. These relatively independent  events are considered to be topics by themselves  and would accordingly be reported in multiple  news re'titles.
788	Explains Technical Concepts	In the unified stochastic engine (USE), not only can we iter-  atively adjust language probabilities to fit our given acous-  tic representations but also acoustic models. Our multi-pass  search algorithm generates N-best hypotheses which are used  to optimize language weights or implement many discrimina-  tive training methods, where recognition errors can be used  as the objective function \[20, 25\]. With the progress of new  database construction such as DARPA's CSR Phase II, we be-  lieve acoustically-driven language modeling will eventually  provide us with dramatic performance improvements.
954	Assumes Prior Knowledge	In general information extraction, there are  two approaches: rule-based and statistical. Early  extraction systems are mainly based on rules  (e.g., Riloff, 1993). In statistical methods, the  most popular models are Hidden Markov Mod- els (HMM) (Rabiner, 1989), Maximum Entropy  Models (ME) (Chieu et al, 2002) and Condi- tional Random Fields (CRF) (Lafferty et al,  2001). CRF has been shown to be the most ef- fective method. It was used in (Stoyanov et al,  2008). However, a limitation of CRF is that it  only captures local patterns rather than long  range patterns. It has been shown in (Qiu et al,  2009) that many feature and opinion word pairs  have long range dependencies. Experimental  results in (Qiu et al, 2009) indicate that CRF  does not perform well.
153	Assumes Prior Knowledge	The parameters of the model are estimated by the open-source maximum entropy parameter es- timation toolkit TADM1. For training, we use all the manually disambiguated discriminants as positive instances, and automatically inferred dis- criminants as negative instances.
510	Assumes Prior Knowledge	Our proposal has a limitation in that if the system does not know who or what the agent of an action in an utterance is, it cannot ap- propriately select a polite expression. We are considering ways to enable identification of the agent of an action in an utterance and to ex- pand the current framework to improve the level of politeness even more. In addition, we intend to apply other extra-linguistic nformation to a dialogue translation system.
829	Explains Technical Concepts	Every intermediate phrase must have at least  one pitch accent. By definition, the last ac-  cented word in any intermediate phrase is al-  ways the nuclear accented word, and it is usu-  ally perceived as more prominent han any other  accented word. The utterance (a) in Fig. 1 is  produced by an H'L-L% combination and typ-  ically interpreted as a neutral declarative. The  second utterance (b) has a H 'L 'H -H% combi-  nation (yes/no question). The final example  (c) illustrates a complex ntterance, made up of  more than one intonation phrase.
943	Explains Technical Concepts	Probabilistic (PRB). This system applies a match  formula that sums term frequencies of query terms in  the document, weighted by terms' inverse document  frequencies, and adjusts tor document length. We  applied this formula to a vocabulary of single terms.  Subiect Field Code (SFC). This system applies a  vector similarity metric to query and document  representations in TextWise's Subject Field Code  space to obtain relevance scores.  N-gram (NG3). This system applies a vector  similarity metric to query and document  representations obtained by counting the occurrences  of 3-letter sequences (after squeezing out blanks,  newlines, and other non-alphabetic characters I.  Latent Semantic Indexing (LSI). This system obtains  query and document representations by applying a  translation matrix to single terms (excluding  compound nominals and proper nouns). We obtained  the translation matrix by singular value  decomposition of a matrix of (. idf weights for single  terms from a 1/3 sample of the Wall Street Journal.  We used a vector similarity metric to obtain  relevance scores.
382	Explains Technical Concepts	 6 D iscuss ion   We have described a comprehensive methodology for  acquiring patterns from examples and automatically  expanding their coverage. Other IE systems employ  variants of example-based pattern acquisition. One  system, developed at University of Massachusetts at Amherst, \[10\], used unsupervised training to learn  patterns from the MUC training corpus. However,  unsupervised learning can degrade in the face of \[1\]  sparse data; the UMass system seemed to require one  more order of magnitude of training data than was  available in MUC-6. The HASTEN system, devel-  oped by SRA \[7\], used a somewhat different example-  based approach: they seek to broaden coverage by  allowing statistically approximate matches, a strat-  egy that lacks a syntactic basis, and may result in  overgeneration. \[2\]  9A meta-rule mechanism is also included in the SRI FAS-  TUS system\[2\].  1?where rn is a pre-defined sub-pattern that matches various  right noun-phrase modifiers, a is a sentence adjunct, and pass-  vg is a passive verb group.
430	Assumes Prior Knowledge	3 Extending the Summarization Capability  Our goal is to improve the usability and flexi-  bility of the summarization system, while still  retaining robustness. This is one of the main rea-  sons why we favor the sentence selection method  rather than approaches based on deep analysis and  generation (Beale 94, Carlson & Nirenburg 90).  Though much disparaged for lack of readability,  cohesion etc. systems based in the sentence selec-  tion method performed well in the recent Tipster  summarization evaluation. In fact the readability as  assessed by the evaluators was as high for summa-  ties of about 30% of the document length as it was  for the original documents. We are developing  summarization techniques based on information  extraction and text generation. These will not give  very good coverage, because of their domain speci-  ficity, but do offer advantages, particularly in the  area of cross document summarization.  Our experiments have shown for English that  the inclusion of other language processing tech-  niques Can indeed increase the flexibility and per-  formance of the summarizer. In particular proper  name recognition, co-reference resolution, part of  speech tagging and partial parsing can all contrib-  ute to the performance of the system.
999	Explains Non-Technical Concepts	5 Conclusions  In this paper, we reported on a fast, portable,  large-scale event and relation extraction system  REES. To the best of our knowledge, this is  the first attempt to develop an IE system which  can extract such a wide range of relations and  events with high accuracy. It performs  particularly well on relation extraction, and it  achieves 70% or higher F-Measure for 26 types  of events already. In addition, the design of  REES is highly portable for future addition of  new relations and events.
464	Explains Non-Technical Concepts	Introduction Currently people deal with customer service centers either over the phone or on the world wide web on a regular basis. These service centers upport a wide variety of tasks including checking the balance of a bank or a credit card account, transferring money from one account o another, buying airline tickets, and filing one's income tax returns. Most of these customer service centers use interactive voice response (IVR) systems on the front-end for determining the user's need by providing a list of options that the user can choose from, and then routing the call appropriately. The IVRs also gather essential information like the user's bank account number, social security number, etc. For back-end support, the customer service centers use either specialized computer systems (example: a system that retrieves the account balance from a database), or, as in most cases, human operators.
344	Explains Technical Concepts	3.2.5 Functional Words  Functional words such as '~' (will) and '~' (then)  are frequently used in Thai texts. These functional  words are used often enough to mislead the  occurrences of string patterns. To filter out these  noisy patterns from word extraction process,  discrete attribute Func(s):  Func(s) : 1 if string s contains  fnnctional words,  = 0 if otherwise,  is applied.
527	Explains Non-Technical Concepts	 5 Conc lus ion   Summarization of multiple documents about  nmltiple topics has been discussed in this pa-  pet'. The method proposed here uses spread-  ing activation over documents syntactically and  semanticMly annotated with GDA tags. It is  capable of:  ? extraction of the opening and settlement  articles from fifty articles about a hostage  incident,  ? creation of an entity-relation graph of im-  portant relations among important entities,  ? extraction and pruning of important sen-  tences, gnd  ? substitution of expressions with more con-  crete ones using cross-document corefer-  ences.
764	Explains Non-Technical Concepts	4. MODELING & ESTIMATION  TRADE-OFFS  Within the framework of tied Gaussian mixtures, there  are a number of modeling and training variations that  have been proposed. In this section, we will describe sev-  eral experiments hat investigate the performance impli-  cations of some of these choices.
462	Assumes Prior Knowledge	3 Our Approach  3.1 The C4.5 Learning Algorithm  Decision tree induction algorithms have been  successfully applied for NLP problems such as  sentence boundary dismnbiguation (Pahner et al  1997), parsing (Magerman 1995) and word  segmentation (Mekuavin et al 1997). We employ  the c4.5 (Quinhln 1993) decision tree induction  program as the learning algorithm for word  extraction.
745	Explains Non-Technical Concepts	Information Retrieval has made tremendous ad-  vances over the last 30 years in terms of accuracy,  efficiency and robustness. It has also been widely  commercialized in recent years, particularly on the  Internet. In spite of this progress, many challenges  remain, and more research is needed to achieve per-  formance levels that would approach human-level ac-  curacy. We believe that this requires a tighter inte-  gration of NLP.
161	Assumes Prior Knowledge	A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming. A simple spell correction is a part of the system ( essentially 1 character errors). The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewrit- ing system with 580 rules.
542	Explains Technical Concepts	Realistic and useful natural  language processing sys-  tems such as database front-ends require large numbers  of words, together with associated syntactic and semantic  Information, to be efficiently stored in machine-readable  form. Our system is Intended to provide the necessary  facilities, being designed to store a large number (at least  10,000) of words and to perform morphological analysis  on them, covering both Inflectional and derlvatlonal mor-  phology. In pursuit of these objectives, the dictionary  associates with each word information concerning its  morphosyntactlc properties. Users are free to modify the  system In a number of ways; they may add to the lexi-  cal entries Lisp functions that perform semantic manipu-  latlons, and tailor the dictionary to the particular subject  matter they are interested in (different databases, for  example). It Is also hoped that the system is general  enough to be of use to linguists wishing to Investigate  the morphology of English and other languages. Con-  tents of the basle data files may be altered or replaced:  1. A 'Word Grammar' file contains rules assigning inter-  nal structure to complex words,  2. A 'Lexicon' file holds the morpheme entries which  include syntactic and other Information associated  with stems and affixes.  3. A 'Spelling Rules' file contains rules governing permis-  sible correspondences between the form of morphemes  listed in the lextcon and complex words consisting of  sequences of these morphemes.  Once these data flies have been prepared, they are com-  piled using a number of pre-processtng functions that  operate to produce a set of output files. These  constitute a ful ly  expanded and cross-Indexed ictionary  which can then be accessed from within LISP.  The process of morphological analysis consists of pars-  lng a sequence of Input morphemes with respect to the  word grammar, It Is Implemented as an active chart  parser (Thompson & Rltchle (1984)), and builds a struc-  ture in the form of a tree in which each node has two  associated values, a morphosyntactlc category, and a rule  Identifier.
16	Assumes Prior Knowledge	So far the attempt has not been successful. Several  difficulties are noted: the clustering algorithm  sometimes does not work well in separating relevant  and irrelevant documents into different clusters; often  the query may not pick the right cluster to re-rank; and  even if the right cluster has been picked, the relevant  documents may not rank sufficiently high within the  cluster so that a lower AvPre measure may result. The  investigation is still ongoing.
550	Explains Non-Technical Concepts	3. Mach ine  t rans lat ion of  (very) closely related Slavic languages In the group of Slavic languages, there are more closely related languages than Czech and Russian. Apart from the pair of Serbian and Croatian languages, which are almost identical and were considered one language just a few years ago, the most closely related languages in this group are Czech and Slovak.
633	Explains Technical Concepts	In the traditional word-lattice approach, the word lattice  is viewed as a scored graph of possible segmentations of the  input speech. The lattice contains information such as the  acoustic match between the input speech and the lattice word, as  well as segmentation information.
775	Assumes Prior Knowledge	This algorithm can be efficiently implemented, even for  large vocabulary recognition systems. That is, the extra work  required to build the "word-life lattice" is minimal compared to  the work required to recognize the large vocabulary with a early-  pass speech recognition algorithm.
506	Explains Technical Concepts	3.4. Cepstral Mean Removal  One of the areas of interest is recognition when the microphone  for the test speech is unknown. We tried a few different methods  to solve this problem, which will be described in a later section.  However, during the course of trying different methods, we found  that the simplest of all methods, which is to subtract he mean  cepstmm from every frame's cepstrum vector actually resulted in  a very small improvement in recognition accuracy even when the  microphone was the same for training and test. This resulted in  a 0.3% reduction in word error rate.
588	Explains Technical Concepts	1.1 The presupposition detector (DP)  We constructed a set of presupposition detection  rules based on the content analysis. The rules use  a wide range of linguistic information about the  input sentences, including particular words (such  as "why"), part of speech categories (e.g., wh-  pronoun), and complex syntactic subtrees (such as  a quantification clause, followed by a noun  phrase).
457	Assumes Prior Knowledge	The dialogues can roughly be divided into five dif- ferent categories based on the users task. The dis- cussion in twenty five dialogues were on bus times between various places, often one departure and one arrival but five dialogues involved more places. In five dialogues the discussion was one price and var- ious types of discounts. Five users wanted to know the telephone number to 'the Lost property office', two discussed only bus stops and two discussed how they could utilise their season ticket to travel out- side the trafficking area of the bus company. It is interesting to note that there is no correspondence between the task being performed uring the inter- action and the amount of changes made to the dia- logue. Thus, if we can assume that the amount of distillation indicates omething about a user's inter- action style, other factors than the task are impor- tant when characterising user behaviour.
826	Explains Non-Technical Concepts	5 Results 5.1 WMT10 Evaluation In one of the tasks of the WMT10 human evaluation campaign, people were asked to rank competing trans- lations. From each 1-through-5 ranking of a set of 5 system outputs, 10 pairwise comparisons are extracted. Then, for each system, a score is computed that tells how often it was ranked equally or better than the other system. For our system, this score is 32.35%, meaning it ranked 17th out of 19 systems for English-to-French. A number of automatic scores were also calculated and appear in Table 1.
677	Explains Technical Concepts	 For the evaluation matrix, we mainly use  top-1 accuracy (ACC) (Li et al, 2009a) to  measure transliteration performance. For refer- ence purpose, we also report the performance  using all the other evaluation matrixes used in  NEWS 2009 benchmarking (Li et al, 2009a),  including F-score, MRR, MAP_ref, MAP_10  and MAP_sys. It is reported that F-score has  less correlation with other matrixes (Li et al,  2009a).
308	Explains Technical Concepts	2.2 Implementation via reactive planning Bratman's approach has been elaborated in a computer science context by subsequent researchers (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al 1998). Reactive planning (Georgeff and Ingrand 1989, Wilkins et al. 1995), originally known as "integrated planning and execution," is one way of implementing Bratman's model. Originally developed for real-time control of the space shuttle, reactive planning has since been used in a variety of other domains. For the Atlas project we have developed a reactive planner called APE (Atlas Planning Engine) which uses these ideas to conduct a conversation. After each student response, the planner can choose to continue with its previous intention or change something in the plan to respond better to the student's utterance. Like most reactive planners, APE is a hierarchical task network (HTN) style planner (Yang 1990, Erol, Hendler and Nau 1994).
848	Explains Technical Concepts	Another consideration i designing a phonetic dictionary is the  tradeoff between the number of parameters and the accuracy of  the estimates. Finer phonetic distinctions in the dictionary can  result in improved modeling, but they also increase the need for  training data. Lori Lame1 had previously repoRed \[7\] that the  error rate on the RM corpus was reduced when the number of  phonemes was reduced, ignoring some phonetic distinctions. In  particular, she suggested replacing some diphthongs, affricates,  and syllabic consonants with two-vowel sequences. She also  suggested removing some phonetic distinctions. The fist of sub  stitutions is listed in Table 1 below.
427	Explains Technical Concepts	5.6 Combin ing  Representat ions   Since short-word with character and bigram  representations separately returns comparable good  results, this leads us to investigate whether they can  perhaps reinforce each other. Short-words provide  effective term matching between a query and a  document, but one might have wrong segmentations.  Bigrams however are exhaustive and can remedy the  situation. Given a collection, we index it both ways.  For each query we also index it both ways and  perform separate retrievals. Their retrieval ists are  then combined based on the RSV of each document i  as follows (with ct=l/2):  RSVi = tx*RSVil + (1- ot)*RSVi2
263	Explains Technical Concepts	4 Translation Model The translation model was trained on the parallel corpus and the word alignment was generated by a discriminative word alignment model, which is described below. The phrase table was trained us- ing the Moses training scripts, but for the German to English system we used a different phrase ex- traction method described in detail in Section 4.2. In addition, we applied phrase table smoothing as described in Foster et al (2006). Furthermore, we extended the translation model by additional fea- tures for unaligned words and introduced bilingual language models.
363	Explains Non-Technical Concepts	In initial experiments with just female speakers, we used  diagonal covariance Gaussians and compared 200- ver-  sus 300-density mixture models, exploring the range  typically reported by other researchers. With context-  independent models, after several training passes, both  systems got 6.5% word error on the Feb89 test set. For  context-dependent models, the 300-density system per-  formed substantially better, with a 2.8% error rate, com-  pared with 4.2% for the 200 density system. These re-  sults compare favorably with the baseline SSM which  has an error rate on the Feb89 female speakers of 7.7%  for context-independent models and 4.8% for context-  dependent models.
54	Explains Non-Technical Concepts	DISCUSSION  So far, our results suggest hat, for our choice of  retrieval systems, there is an opportunity to improve  retrieval performance by using dynamic fusion  functions instead of using a single static fusion  function \['or all queries. One possible qualification to  these results is that limiting ourselves to a linear form  for the static fusion models may result in artificially  low baseline retrieval for the single overall static  function. The volatility of the K-NN technique in the  context of our data made it difficult to say whether or  not a non-linear form for the fusion model is  necessary.
156	Explains Technical Concepts	Finally, we experimented with including or excluding the  background word model in the CSR lexicon. While including the  background word model does increase the overall ikelihood of  the recognized transcription, the probability of using the back-  ground model is highly likely (due to the language model proba-  bilities of OOV words) and tended to replace a number of  keywords that had poor acoustic matches. Table 5 shows that a  slight improvement can be gained by eliminating this back-  ground word model.
977	Explains Technical Concepts	Through this study we have established some  basic matches between SFG tones and ToBI se-  quences of pitch accents and edge tones. Here,  we have concentrated on the SFG tones 1, 2 and  4. We have analysed tones 3 and 5 as well and  identified their ToBI equiwdents using the same  method (cf. Sections 3 and 4). In the next step  we will integrate the SFG description of intona-  tion for English in the existing SFG-based Pen-  man generation system and then interface the  FESTIVAL synthesizer with the generator using  the correspendences tablished by our analy-  ses.
798	Explains Non-Technical Concepts	Training and Evaluation  We use the remaining 197 queries lor training.  For these queries, we have used all the documents to  find coefficient vectors for optimal linear static  fusion models. These coefficient vectors constitute  the "target" outputs the mixture expert will be trained  to reproduce.
420	Explains Technical Concepts	5 Cell Pruning Whole cells can be pruned from the chart by tag- ging words in a sentence. Roark and Hollingshead (2009) used a binary tagging approach to prune a CFG CKY chart, where tags are assigned to input words to indicate whether they can be the start or end of multiple-word constituents. We adapt their method to CCG chart pruning. We also show the limitation of binary tagging, and propose a novel tagging method which leads to increased speeds and accuracies over the binary taggers.
838	Assumes Prior Knowledge	6. MULT I -DOCUMENT SUMMARIES   As discussed earlier, MMR passage selection  works equally well for summarizing single documents  or clusters of topically related documents. Our  method for multi-document summarization follows  the same basic procedure as that of single document  summarization (see section 4). In step 2 (Section 4),  we identify the N most relevant passages from each of  the documents in the collection and use them to form  the passage set to be MMR re-ranked. N is dependent  on the desired resultant length of the summary. We  used N relevant passages from each document  collection rather than the top relevant passages in the  entire collection so that each article had a chance to  provide a query-relevant contribution. In the future  we intend to compare this to using MMR ranking  where the entire document set is treated as a single  document. Steps 2, 3 and 4 are primarily the same.  The TIPSTER evaluation corpus provided several  sets of topical clusters to which we applied MMR  summarization. In one such example on a cluster of  apartheid-related documents, we used the topic  description as the query (see Figure 3) and N was set  to 4 (4 sentences per article were reranked). The top  10 sentences for ~ = 1 (effectively query relevance,  but no MMR) and k = .3 (both query relevance and  MMR anti-redundancy) are shown in Figures 4 and 5  respectively.
915	Explains Non-Technical Concepts	There is, however, one major difference between the evaluation as carried out in WMT10 and our in-house evaluation: The test data of WMT10 consists exclu- sively of news articles and is thus out-of-domain for our system intended for use within the European Parlia- ment. This means that the impact of training our system on the in-domain data we obtain from our translation memories cannot be assessed properly, i.e. taking into consideration our specific translation production needs. Therefore, we would like to invite other interested groups to also translate our in-domain test data with the goal of seeing how our translation scenario could benefit from their setups. Due to legal issues, however, we unfortunately cannot provide our internal training data at this moment.
523	Explains Technical Concepts	4.1 Relationship Interpretation  Relationships between characters in drama im- provisation are very crucial for the contextual  affect interpretation for the emotionally ambi- guous users? input. During the improvisation of  each scenario, like any other drama progression,  normally the recorded transcripts for creative  roleplays are composed of three main improvi- sational sections, including the starting of the  drama, the climax and the final ending. Rela- tionships in these three drama progression stag- es between characters are different from one  another. E.g. in the climax of the improvisation  of the school bullying scenario, we normally  expect very negative relationships between the  bully and the bullied victim (Lisa) & her friends  since the big bully is very aggressive at Lisa and  her friends who try to stop the bullying. Moreo- ver, in nearly the end of the improvisational ses- sion, sometimes the big bully feels sorry for his  behavior and is cared by Lisa and her friends  since he is abused by his uncle. The intense  negative relationships between the big bully and  Lisa & her friends are changed to those with at  least less negativity or even normal relation- ships. Because of the creative nature of the im- provisation, sometimes the bully and the victim  may even have a positive relationship towards  the ending of the drama improvisation.   However in our current study, we only as- sume consistent negative relationships between  the bully and the bullied victim & her friends  throughout the improvisation to simplify the  processing. We will report our work on relation- ship interpretation using fuzzy logic to dynami- cally capture the changing relationships be- tween characters as the drama progresses in the  near future.
397	Explains Non-Technical Concepts	Abstract This paper describes the development of French?English and English?French ma- chine translation systems for the 2010 WMT shared task evaluation. These sys- tems were standard phrase-based statisti- cal systems based on the Moses decoder, trained on the provided data only. Most of our efforts were devoted to the choice and extraction of bilingual data used for training. We filtered out some bilingual corpora and pruned the phrase table. We also investigated the impact of adding two types of additional bilingual texts, ex- tracted automatically from the available monolingual data. We first collected bilin- gual data by performing automatic trans- lations of monolingual texts. The second type of bilingual text was harvested from comparable corpora with Information Re- trieval techniques.
610	Assumes Prior Knowledge	In the PictureQuest application, these pitfalls are minimized because the queries are short, not paragraph-long descriptions as in TREC (see, e.g., Voorhees and Harman 1999). This would be a problem for a statistical approach, since the queries present little context, but, since we are not relying on context (because reducing ambiguity is not our top priority) it makes our task simpler. Assuming that the translation program keeps multi-term concepts intact, or at least that it preserves the modifier-head structure, we can successfully match phrases. The captions (i.e. the documents o be retrieved) are mostly in sentences, and their phrases are intact. The phrase recognizer identifies meaningful phrases (e.g. fire engine) and handles them as a unit. The pattern matcher recognizes core noun phrases and makes it more likely that hey will match correctly.
895	Explains Non-Technical Concepts	Abstract  This paper reports on a large-scale, end-to-  end relation and event extraction system. At  present, the system extracts a total of 100  types of relations and events, which  represents a much wider coverage than is  typical of extraction systems. The system  consists of three specialized pattem-based  tagging modules, a high-precision co-  reference resolution module, and a  configurable template generation module.  We report quantitative valuation results,  analyze the results in detail, and discuss  future directions.
753	Explains Technical Concepts	Part of speech tagging and phrase recognition  allows us to carry out certain kinds of text compac-  tion. This is particularly important when very short  summaries (10%) of short documents are required.  Our experiments with this kind of compaction have  showed reductions of about 1/3 of the summary  size with some loss of readability. A single sen-  tence example shows the usefulness of this tech-  nique.
539	Explains Non-Technical Concepts	1 Introduction Many NLP tasks and applications require the pro- cessing of massive amounts of textual data. For example, knowledge acquisition efforts can in- volve processing billions of words of text (Cur- ran, 2004). Also, the increasing need to process large amounts of web data places an efficiency demand on existing NLP tools. TextRunner, for example, is a system that performs open infor- mation extraction on the web (Lin et al, 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimen- tary: finite-state shallow parsing technology that is now decades old. TextRunner uses this technol- ogy largely for efficiency reasons.
513	Explains Non-Technical Concepts	2 Problem Formulation 2.1 Problem Description This problem involves identifying entities and dis- covering semantic relationships between entity pairs from English encyclopedic articles. The ba- sic document is an article, which mainly defines and describes an entity (known as principal en- tity). This document mentions some other entities as secondary entities related to the principal en- tity. Clearly, our task consists of two subtasks ? first, for entity identification, we need to recog- nize the secondary entities (both the boundaries and types of them) in the document 1. Second, 1Since the topic/title of an article usually defines a princi- pal entity (e.g., a famous person) and it is easy to identify, in after all the secondary entities are identified, our goal for relation extraction is to predict what rela- tion, if any, each secondary entity has to the prin- cipal entity. We assume that there is no relation- ship between any two secondary entities in one document.
927	Explains Non-Technical Concepts	10. CONCLUSION  We have shown that MMR ranking provides a  useful and beneficial manner of providing  information to the user by allowing the user to  minimize redundancy. This is especially true in the  case of query-relevant multi-document summarization  in this one data collection. We are currently  performing studies on how this extends to additional  document collections. In the future we will also be  investigating how to handle co-reference in our  system as well as analyzing the most suitable ~,  par/maeters and clustering the output results.
157	Explains Technical Concepts	5. MICROPHONE INDEPENDENCE  DARPA has placed a high priority on microphone independence.  That is, if a new user plugs in any microphone (e.g., a lapel  microphone or a telephone) without informing the system of the  change, the recognition system is expected to work as well as it  does with the microphone that was used for training.  We considered two different ypes of methods to alleviate this  problem. The first attempts to use features that are independent of the microphone, while the second attempts to adapt he system or  the input to observed ifferences in the incoming signal in order  to make the speech models match better.
559	Explains Technical Concepts	We will refer to the faster speech recognition techniques  as "earlier-pass techniques", and the slower more accurate  techniques as "advanced techniques." Constraining the costly  advanced techniques in this way can make them run significantly  faster without significant loss in accuracy.
505	Explains Technical Concepts	4 Exper imental  Results  4.1 The Results  To measure the accuracy of the algorithln, we  consider two statistical values: precision and  recall. The precision of our algorithm is 87.3% for  the training set and 84.1% for the test set. The  recall of extraction is 56% in both training and  test sets. We compare the recall of our word  extraction with the recall from using the Thai  Royal Institute dictionary (RID). The recall froln  our approach and from using RID are comparable  and our approach should outperform the existing  dictionary for larger corpora. Both precision and  recall fiom training and test sets are quite close.  This indicates that the created decision tree is  robust for unseen data. Table 3 also shows that  more than 30% of the extracted words are not  found in RID.
367	Assumes Prior Knowledge	Abstract Cross-Language Multimedia Information Retrieval Simple measures can achieve high-accuracy cross-language r trieval in carefully chosen applications. Image retrieval is one of those applications, with results ranging from 68% of human translator performance for German, to 100% for French.
783	Explains Technical Concepts	3.2 Evaluation of results The problem how to evaluate results of automatic translation is very difficult. For the evaluation of our system we have exploited the close connection between our system and the TRADOS Translator's Workbench. The method is simple - the human translator eceives the translation memory created by our system and translates the text using this memory. The translator is free to make any changes to the text proposed by the translation memory. The target text created by a human translator is then compared with the text created by the mechanical application of translation memory to the source text. TRADOS then evaluates the percentage of matching in the same manner as it normally evaluates the percentage of matching of source text with sentences in translation memory. Our system achieved about 90% match (as defined by the TRADOS match module) with the results of human translation, based on a relatively large (more than 10,000 words) test sample.
705	Explains Technical Concepts	5.1 English-German The baseline system for English-German applies short-range reordering rules and discriminative word alignment. The language model is trained on the News corpus. By expanding the coverage of the rules to enable long-range reordering, the score on the test set could be slightly improved. We then combined the target language part of the Europarl and News Commentary corpora with the News corpus to build a bigger language model which resulted in an increase of 0.11 BLEU points on the development set and an increase of 0.25 points on the test set. Applying the bilingual lan- guage model as described above led to 0.04 points improvement on the test set.
936	Explains Technical Concepts	10 Pipeline for Running MT Experiments Reproducing other researchers? machine transla- tion experiments is difficult because the pipeline is too complex to fully detail in short conference pa- pers. We have put together a workflow framework for designing and running reproducible machine translation experiments using Joshua (Schwartz, under review). Each step in the machine transla- tion workflow (data preprocessing, grammar train- ing, MERT, decoding, etc) is modeled by a Make script that defines how to run the tools used in that step, and an auxiliary configuration file that de- fines the exact parameters to be used in that step for a particular experimental setup. Workflows configured using this framework allow a complete experiment to be run ? from downloading data and software through scoring the final translated re- sults ? by executing a single Makefile.
76	Explains Technical Concepts	3. The  Forward-Backward  Search  Algorithm  We developed the Forward-Backward Search (FBS) algo-  rithm in 1986 as a way to greatly reduce the computation  needed to search a large language model. While many sites  have adopted this paradigm for computation of the N-best  sentence hypotheses, we feel that its full use may not be  fully understood. Therefore, we will discuss the use of the  FBS at some length in this section.
715	Explains Technical Concepts	1 Introduction One of the crucial steps in multi-document  summarization (MDS) is information ordering,  right after content selection and before sentence  realization (Jurafsky and Martin, 2009:832? 834). Problems with this step are the culprit for  much of the dissatisfaction with automatic  summaries. While textual order may guide the  ordering in single-document summarization, no  such guidance is available for MDS ordering.  A sensible solution is ordering sentences by  enhancing coherence since incoherence is the  source of disorder. Recent researches in this  direction mostly focus on local coherence by  studying lexical cohesion (Conroy et al, 2006)  or entity overlap and transition (Barzilay and  Lapata, 2008). But global coherence, i.e.,  coherence between sentence groups with the  whole text in view, is largely unaccounted for  and few efforts are made at levels higher than  entity or word in measuring sentence coherence. On the other hand, event as a high-level  construct has proved useful in MDS content  selection (Filatova and Hatzivassiloglou, 2004;  Li et al, 2006). But the potential of event in  summarization has not been fully gauged and  few publications report using event in MDS  information ordering. We will argue that event  is instrumental for MDS information ordering,  especially multi-document news summarization  (MDNS). Ordering algorithms based on event  and entity information outperform those based  only on entity information.
648	Explains Technical Concepts	The new Kana-Kanji translation method presented  in this paper treats both types of ambiguity in the same  way based on a syntactic and semantic analysis. In the  new method, translation is performed in two steps. In the  first step, the both kinds of ambiguity are detected by  morphological nalysis and are stored in a network form.  In the second step the best path, which is a string of  morphemes, is chosen from the network by syntactic and  semantic analysis based on the case grammar.
628	Explains Technical Concepts	It is well known that domain adaptation is an important step in optimizing machine translation systems. A relatively simple and straight-forward method is the linear interpolation of the language model, as we explored previously (Koehn and Schroeder, 2007; Schwenk and Koehn, 2008). We trained domain-specific language models separately and then linearly interpolated them us- ing SRILM toolkit (Stolke, 2002) with weights op- timized on the development set newsdev2009.
676	Explains Technical Concepts	Another advantage ofusing the SCHMM is that it requires less  training data in comparison with the discrete HMM. There-  fore, given the current limitations on the size of the training  data set, more detailed models can be employed to improve  the recognition accuracy. One way to increase the number  of parameters is to use speaker-clustered models. Due to the  smoothing abilities of the SCHMM, we were able to train  multiple sets of models for different speakers. We investi-  gated automatic speaker clustering as well as explicit male,  female, and generic models. By using sex dependent models  with the SCHMM, the error rate is further educed by 10% on  the WSJ task.    3.2. Senones  To share parameters among different word models, context-  dependent subword models have been used successfully in  many state-of-the-art speech recognition systems \[26, 21, 17\].  The principle of parameter sharing can also be extended to  subphonetic models \[19, 18\]. We treat the state in pho-  netic hidden Markov models as the basic subphonetic unit  senone. Senones are constructed by clustering the state-  dependent output distributions across different phonetic mod-  els. The total number of senones can be determined by clus-  tering all the triphone HMM states as the shared-distribution  models \[18\]. States of different phonetic models may thus  be tied to the same senone if they are close according to  the distance measure. Under the senonic modeling frame-  work, we could also use a senonic decision tree to predict un-  seen triphones. This is particularly important for vocabulary-  inc~pendence \[10\], as we need to find subword models which  are detailed, consistent, trainable and especially generalizable.  Recently we have developed a new senonic decision-tree to predict he subword units not covered in the training set \[18\].  The decision tree classifies enones by asking questions in a  hierarchical manner \[7\]. These questions were first created  using speech knowledge from human experts. The tree was  automatically constructed by searching for simple as well as  composite questions. Finally, the tree was pruned using cross  validation. When the algorithm terminated, the leaf nodes  of the tree represented the senones to be used. For the WSJ  task, our overall senone models gave us 35% error reduction  in comparison with the baseline SPHINX results.
249	Explains Technical Concepts	Automatically generating text extraction summaries  based on a query or high frequency words from the  text can produce a reasonable looking summary, yet  this summary can be far from the optimal goal of  quality summaries: readable, useful, intelligible,  appropriate length summaries from which the  information that the user is seeking can be extracted.  Jones & Galliers define this type of evaluation as  intrinsic (measuring a system's quality) compared to  extrinsic (measuring a system's performance in a  given task) \[7\].
20	Explains Technical Concepts	The approach is based on training a set of large phone-  based ergodic HMMs for each non-linguistic feature to be  identified (language, gender, speaker, ...), and identifying the  feature as that associated with the model having the high-  est acoustic likelihood of the set. The decoding procedure  is efficiently implemented by processing all the models in  parallel using a time-synchronous beam search strategy.  This has been shown to be a powerful technique for sex-,  language-, and speaker-identification, and has other possible  applications uch as for dialect identification (including for-  eign accents), or identification of speech disfluencies. Sex-  identification for BREF and WSJ was error-free, and 99%  accurate for TIMIT with 2s of speech. With 2s of speech  the language is correctly identified as English or French with  over 99% accuracy. Speaker identification accuracies of  98.5% on TIMIT (168 speakers) and 99.1% on BREF (65  speakers) were obtained with one utterance per speaker, and  100% if 2 utterances were used for identification. The same  identification accuracy was obtained on the 168 speakers of  TIMIT using unsupervised adaptation, verifying that it is  not necessary to provide phonetic transcription for accurate  speaker identification. Being independent of the spoken text,  and requiring only a small amount of speech (on the order  of 2.5s), this technique is promising for a variety of appli-  cations, particularly those for which continual verification is  preferable.
737	Assumes Prior Knowledge	The plot shows a strong positive linear correla- tion between the two ranking models. The partic- ularly strong correlation at the low and high ends of the ranking shows that the two annotators share a similar behavior pattern concerning the most and least preferred discriminants. The correlation is slightly weaker in the middle ranking zone, where different preferences or annotation styles can be observed.
250	Explains Technical Concepts	SRI applied two different recognition systems to the  ATIS keyword spotting task. The first system was SRI's large-  vocabulary speaker-independent speech recognition system that  we have used for the ATIS speech-recognition task \[3\]. The  vocabulary used in this system is about 1200 words, and a back-  off bigram language model was trained using the ATIS MAD-  COW training data \[13\]. Many of the words in the vocabulary  use word-specific or triphone acoustic models, with biphone and  context-independent models used for those words that occur  infrequently.
94	Explains Technical Concepts	   Popescu and Etzioni (2005) investigated the  same problem. Their algorithm requires that the  product class is known. The algorithm deter- mines whether a noun/noun phrase is a feature  by computing the pointwise mutual information  (PMI) score between the phrase and class- specific discriminators, e.g., ?of xx?, ?xx has?, ?xx comes with?, etc., where xx is a product  class. This work first used part-whole patterns  for feature mining, but it finds part-whole based  features by searching the Web. Querying the  Web is time-consuming. In our method, we use  predefined part-whole relation patterns to ex- tract features in a domain corpus. These patterns  are domain-independent and fairly accurate.
103	Assumes Prior Knowledge	2.3 Evaluation of the DP  DP ratings were significantly correlated only with  Pcomp (0.35). DP and P~o~ ratings were in  agreement for 67% of the questions. Table 3 lists  hit and false alarm rates for DP, separately for P~j  and P~omp. The hit rate indicates how many of the  presuppositions identified by the human ratings  were detected by DP. The false alarm rate  indicates how often DP reported a presupposition  when the human raters did not. The measures look  better with respect to the complete agreement  criterion, P~omp-  Table 3 further lists recall and precision scores.  The recall rate indicates how many  presuppositions DP detects out of the  presuppositions reported by the human rating  criterion (computed as hits, divided by the sum of  hits and misses). The precision score (computed  as hits, divided by the sum of hits and false  alarms) measures how many presuppositions  reported by DP are actually present, as reported by  the human ratings.
342	Explains Technical Concepts	Following what has become a general assumption in  syntactic theory, we take the major lexlcal categories to  be partitioned into four classes by the two binary-valued  features \ [+ N\] and \[:k V\]. The major lexlcat categories  have phrasal projections; these are distinguished from  their lexlcal counterparts by their value for the feature  BAR. Lexlcal categories have the value 0, and phrasal  categories (including sentences) have the value 1 or 2.  Thus, a Noun Phrase is of the category:  ((V -) (N +) (BAR 2))  In our analysis, 'bound morphemes', that is to say  prefxes and suffixes, are distinguished from others by  their BAR specification; tile suffix ing is the sole member  of the category:  ((V 4-) (N -) (VFORM ING) (BAR -1))  As in other GPSG-based work, our analysis encodes the  subcategorlzational prbpertles of lexlcal Items in the value  of a feature SUBCAT. Transitive verbs such as devour  are specified as (SUBCAT NP), and Intransitives uch as  elapse as (SUBCAT NULL).
251	Explains Technical Concepts	Maxspan tags take the same values as level tags. However, the meanings of maxspan tags and level tags are different. While a level tag indicates the row from which a column or diagonal of cells is pruned, a maxspan tag represents the size of the largest constituent a word begins or ends. For ex- ample, in Figure 3, the level end tag for the word ?games? has value 3, since the largest constituent this words ends spans ?playing card games?. We use the standard maximum entropy trigram tagger for maxspan tagging, where features are extracted from tag trigrams and surrounding five- word windows, as for the binary taggers. Parse trees can be turned directly into training data for a maxspan tagger. Since the level tag set is fi- nite, we a require a maximum value N that a level tag can take. We experimented with N = 2 and N = 4, which reflects the limited range of the features used by the taggers.5 During decoding, the maxspan tagger uses the forward-backward algorithm to compute the prob- ability of maxspan tag values for each word in the 5Higher values of N did not lead to improvements during development experiments.
913	Explains Technical Concepts	4 The Framework's Linguistic Resources  As mentioned previously, the framework is  composed of instantiations of the tree  transduction module shown in Figure 1. Each  module has the following resources:  ? Feature Data-Base: This consists of the  feature system defining available features  and their possible values in the module.  ? Lexicon: This consists of the available  lexemes or concepts, depending on whether  the module works at syntactic or conceptual  level. Each lexeme and concept is defined  with its features, and may contain specific  lexico-structural ules: transfer rules for MT,  mapping rules to the next level of  representation for surface realization of  DSyntS or lexicalization of ConcS.
565	Explains Technical Concepts	Therefore, we build lattices that encode the dif- ferent reorderings for every training sentence, as described in Niehues et al (2009). Then we can not only extract phrase pairs from the monotone source path, but also from the reordered paths. So it would be possible to extract the example men- tioned before, if both parts of the verb were put together by a reordering rule. To limit the num- ber of extracted phrase pairs, we extract a source phrase only once per sentence even if it may be found on different paths. Furthermore, we do not use the weights in the lattice.
191	Explains Technical Concepts	The N-best Paradigm has the potential problem that if a  knowledge source is not used to find the N-best hypothe-  ses, the answer that would ultimately have the highest score  including this knowledge source may be missing from the  top N hypotheses. This becomes more likely as the error  rate becomes higher and the utterances become longer. We  have found empirically that this problem does not occur for  smaller vocabularies, but it does occur when we use vocab-  ularies of 20,000 words and trigram language models in the  rescoring pass.
671	Assumes Prior Knowledge	3.4 Bigger Beam Sizes As a final general improvement, we adjusted the beam settings during decoding. We increased the pop-limit from 5,000 to 20,000 and the translation table limit from the default 20 to 50. The decoder is quite fast, partly due to multi- threaded decoding using 4 cores machines (Had- dow, 2010). Increasing the beam sizes slowed down decoding speed from about 2 seconds per sentence to about 8 sec/sentence. However, this resulted only in minimal gains, on average +0.03 BLEU. For details refer back to Table 1.
42	Explains Technical Concepts	4 Problems and Solutions  4.1 Fixing LMERT   Just after the evaluation, we noticed a discrepan- cy for E-F between BLEU scores computed dur- ing LMERT optimization and scores from the 1- best list immediately after decoding. Our  LMERT code had a bug that garbled any ac- cented word in the version of the French refer- ence in memory; previous LMERT experiments  had English as target language, so the bug hadn?t  showed up. The bug didn?t affect characters in  the 7-bit ASCII set, such as English ones, only  accented characters. Words in candidate transla- tions were not garbled, so correct translations  with accents received a lower BLEU score than  they should have. As Table 1 shows, this bug  cost us about 0.5 BLEU for WMT 2010 E-F after  rescoring (according to NRC?s internal version  of BLEU, which differs slightly from WMT?s  BLEU). Despite this bug, the system tuned with  buggy LMERT (and submitted) was still better  than the best system we obtained with N-best  MERT. The bug didn?t affect F-E scores.     4.2 Fixing odd translations  After the evaluation, we carefully studied the  system outputs on the WMT 2010 test data, par- ticularly for E-F. Apart from truecasing errors,  we noticed two kinds of bad behaviour: transla- tions of proper names and apparent passthrough  of English words to the French side.   Examples of E-F translations of proper names  from our WMT 2010 submission (each from a  different sentence):     Contamination can take strange forms. We ex- pected to see English sentences copied over in- tact to the French side, and we did, but we did  not expect to see so many ?French? sentences  that interleaved short English word sequences  with short French word sequences, apparently  because text with an English and a French col- umn had been copied by taking lines from alter- nate columns. We found many of these inter- leaved ?French? sentences, and found some of  them in exactly this form on the Web (i.e., the  corruption didn?t occur during WMT data collec- tion). The details may not matter: whenever the  ?French? training sentence contains words from  its English twin, there can be serious damage via  backward probabilities.
899	Assumes Prior Knowledge	4. MULTI -PASS SEARCH  Recent work on search algorithms for continuous peech  recognition has focused on the problems related to large vo-  cabularies, long distance language models and detailed acous-  tic modeling. A variety of approaches based on Viterbi beam  search \[28, 24\] or stack decoding \[5\] form the basis for most  of this work. In comparison with stack decoding, Viterbi  beam search is more efficient but less optimal in the sense  of MAR For stack decoding, a fast-match is necessary to re-  duce a prohibitively arge search space. A reliable fast-match  should make full use of detailed acoustic and language mod-  els to avoid the introduction of possibly unrecoverable errors.  Recently, several systems have been proposed that use Viterbi  beam search as a fast-match \[27, 29\], for stack decoding or the  N-best paradigm \[25\]. In these systems, N-best hypotheses  are produced with very simple acoustic and language models.  A multi-pass rescoring is subsequently applied to these hy-  potheses to produce the final recognition output. One problem  in this paradigm is that decisions made by the initial phase  are based on simplified models. This results in errors that  the N-best hypothesis list cannot recover. Another problem  is that the rescoring procedure could be very expensive per  se as many hypotheses may have to be rescored. The chal-  lenge here is to design a search that makes the appropriate  compromises among memory bandwidth, memory size, and  computational power \[3\].
683	Explains Non-Technical Concepts	3 Syntax   The goal of the work being deserlbcd was an analyzer that would  be easy to use. In the area of syntax, this entails two subgoal.s.  First, it should be easy to specify which morphemes may com-  bine with which, and second, when tile recognition tlas been com-  pleted, the result shnuld be something that can easily be used by  a parser or some other program.
879	Explains Technical Concepts	4.4 Bilingual Word language model Motivated by the improvements in translation quality that could be achieved by using the n-gram based approach to statistical machine translation, for example by Allauzen et al (2009), we tried to integrate a bilingual language model into our phrase-based translation system.
668	Explains Technical Concepts	3 Specifying Constraints on Translation Integrating output from specialized modules (like transliterators, morphological analyzers, and modality translators) into the MT pipeline can improve translation performance, particularly for low-resource languages. We have implemented an XML interface that allows external modules to propose alternate translation rules (constraints) for a particular word span to the decoder (Irvine et al, 2010). Processing that is separate from the MT engine can suggest translations for some set of source side words and phrases. The XML format allows for both hard constraints, which must be used, and soft constraints, which compete with standard extracted translation rules, as well as specifying associated feature weights. In ad- dition to specifying translations, the XML format allows constraints on the lefthand side of SCFG rules, which allows constraints like forcing a par- ticular span to be translated as an NP. We modi- fied Joshua?s chart-based decoder to support these constraints.
92	Assumes Prior Knowledge	3 Translation Methodology We performed preliminary testing using two translation methodologies. For the initial tests, we chose European languages: French, Spanish, and German. Certainly this choice simplifies the translation problem, but in our case it also reflects the most pressing business need for translation. For the French, Spanish, and German tests, we used Systran as provided by AltaVista (Babelfish); we also tested several other Web translation programs. We used native speakers to craft queries and then translated those queries either manually or automatically and submitted them to PictureQuest. The resulting image set was evaluated for precision and, in a limited fashion, for recall.
458	Explains Non-Technical Concepts	We view these results as favorable as well since our  accuracy is 65.3% using 17.0% of the document on  average compared to 69.3% accuracy using the en-  tire document. The discrepancy between the two  evaluations appears to be based on the assessors in  the second evaluation using a stricter criteria for  relevance than that used by the previous evalua-  tion's assessors or the TREC assessors.  It was noted after the first evaluation that dif-  ferent criteria for relevance accounted for some of  the disagreement between our assessors and the  TREC assessors. Many documents considered rele-  vant were marked as irrelevant due to different no-  tions of relevance and not because the summary  failed to provide material on which to base a correct  decision. These difficulties only hinder the evalua-  tion of a summary system and not its use in an ap-  plication, since a user will have a clear idea of his  or her intentions when determining a document's  relevance.
91	Explains Non-Technical Concepts	We conducted an experiment using the trans- fer rules and transfer dictionary for a clerk with 23 unseen dialogues (344 utterances). Our input was off-line, i.e., a transcription of dialogues, which was encoded with the participant's social role. In the on-line situation, our system can not infer whether the participant's social role is a clerk or a customer, but can instead etermine the role without error from the interface (such as a microphone or a button).
790	Explains Non-Technical Concepts	3. Building Progressive Search Lattices  The basic step of a progressive search system is using a  speech recognition algorithm to make a lattice which will be  used as a grammar for a more advanced speech recognition  algorithm. This section discusses how these lattices may be  generated. We focus on generating word lattices, though these  same algorithms are easily extended to other levels.
615	Explains Non-Technical Concepts	In addition, there is well-known research  work on the development of emotional conver- sational agents. Egges et al (2003) provided  virtual characters with conversational emotional  responsiveness. Aylett et al (2006) also focused  on the development of affective behavior plan- ning for their synthetic characters. Cavazza et  al. (2008) reported on a conversational agent  embodied in a wireless robot to provide sugges- tions for users on a healthy living life-style.  Hierarchical Task Networks (HTN) planner and  semantic interpretation have been used in this  work. The cognitive planner plays an important  role in assisting with dialogue management. The  user?s response has also been considered for the  generation of a new plan. However, the system  will hesitate when open-ended user input going  beyond the planner?s knowledge has been used  intensively during interaction. The system we  present here intends to deal with such challenge.  Our work focuses on the following aspects:  (1) affect detection from metaphorical expres- sions; (2) real-time affect sensing for basic and  complex emotions in improvisational role-play  situations; (3) affect detection for second and  third person cases (e.g. ?you?, ?she?); and (4)  affect interpretation based on context profiles.
369	Assumes Prior Knowledge	Figure 3: ConcS Interlingua nd English DSyntS  Finally, the PSyntSs correspond to the parser  outputs represented using RealPro's dependency  structure formalism. The PSyntSs may not be  valid directly for realization or transfer since  they may contain unsupported features or  dependency relations. However, the PSyntSs  are represented in a way to allow the framework  to convert hem into valid DSyntS via lexico-  structural processing. This conversion is done  via conversion grammars customized for each  parser. There is a practical need to convert one  syntactic formalism to another and so far we  have implemented converters for three off-the-  shelf parsers (Palmer et al, 1998).
252	Explains Technical Concepts	4. ENGL ISH AD-HOC RETRIEVAL   An important finding in the TREC experiments i   that short queries have substantially different retrieval  properties from long ones. We consider short queries  as those with a few content erms and are popular in  casual environments such as web searching. Serious  users wanting more exhaustive and accurate searching  should issue longer paragraph-size queries with some  related conceptual terms. They usually return better  effectiveness because longer exposition of needs can  reduce the ambiguity problem due to homographs and  the descriptive deficiency due to synonyms. The 2-  stage retrieval approach has been shown in several  years of TREC experiments o improve over 1-stage  for both query types. Our work has investigated  additional methods to enhance retrieval accuracy for  this strategy.
203	Mathematically-Oriented Paragraph	 6.3 Methodology We perform four-fold cross-validation on this dataset, and take the average performance. For performance evaluation, we use the standard mea- sures of Precision (P), Recall (R), and F-measure (the harmonic mean of P and R: 2PRP+R ) for bothentity identification and relation extraction. We conduct holdout methodology for parameter tun- ing and optimization of our model. We compare our approach with a series of linear-chain CRFs: CRF+CRF and a joint model DCRF (Sutton et al., 2007): dynamic probabilistic models com- bined with factored approach to multiple sequence labeling. CRF+CRF perform entity identification and relation extraction separately. Relation ex- traction is viewed as a sequence labeling problem in the second CRF. All these models exploit stan- dard parameter learning and inference algorithms in our experiments. To avoid over-fitting, penal- ization techniques on likelihood are performed. We also use the same set of features for all these models.
314	Assumes Prior Knowledge	German language direction, which is of particu- lar interest to us due to the rich target side mor- phology and large degree of reordering, resulting in relatively poor performance. See Table 10 for experimental results with the two traditional mod- els (phrase-based model and a factored model that includes a 7-gram morphological tag model) and the two newer models (hierarchical and target syn- tax). The performance of the phrase-based, hierar- chical, and target syntax model are close in terms of BLEU.
709	Explains Technical Concepts	One might expect hat there would be a severe problem with  this approach if the latter knowledge sources were much more  powerful than those used in the initial N-best pass. However, we  have found that this is not the case, as long as the initial error  rate is not too high and the sentences are not too long.  In tests on the ATIS corpus (class A+D sentences only), we  obtained a 40% reduction in word error rate by rescoring the N-  best sentence hypotheses with a trigram language model. In this  test, we used a value of 100 for N. 'Ibis shows that the tfigram  language model is much more powerful than the bigram language  model used in finding the N-best sentences. But there were many  utterances for which the correct answer was not found within the  N-best hypotheses. It was important to determine whether the  system was being hampered by restricting its consideration to  the N-best sentences before using the trigram language model.  Therefore, we artificially added the correct sentence to the N-  best list before rescoring with the trigram model. We found that  the word error only decreased by another 7%. We must remember  that in this experiment, he performance with the correct sentence  added was an optimistic estimate, since we did not add all of  the other sentence hypotheses that scored worse than the 100th  hypothesis, but better than the correct answer.
959	Assumes Prior Knowledge	The experiences with Consensical grammars are a bit mixed however. The main problem is the parsing method itself, which is top down with backtracking. Many principles that would prove elegant for small domains turned out to be too costly for larger do- mains, due to the wide variety of modes of expres- sions, incredible ambiguities and the sheer size of the covered language.
187	Assumes Prior Knowledge	Traditionally, real relevance feedback can give very  large improvements in average precision, like 50 to  over 100%. Experiments with our PIRCS system have  shown that this 2-stage of ad-hoc method works more  often than not, about 2 out 3 times (35 queries in  TREC-5 and 32 in TREC-6 out of 50 queries each),  and the average precision for a set of queries can  improve a few to over 20%. The process of a 2-stage  retrieval is depicted in Fig.2.  In all of our work, this 2-stage approach is used in  our retrieval experiments. Some tables below show  initial 1-stage results for comparison purposes.
265	Explains Technical Concepts	5 The Rule Processing  Before being processed, the rules are first  compiled and indexed for optimisation. Each  module applies the following processing.  The rules are assumed to be ordered from most  specific to least specific. The application of the  rules to the structures i  top-down in a recursive  way from the f'n-st rule to the last. For the main  grammar, before applying a grammar ule to a  given node, dictionary lookup is carried out in  order to first apply the lexeme- or concept-  specific rules associated with this node. These  are also assumed to be ordered from the most  specific to the least specific.  If a lexico-structural transformation involves  switching a governor node with one of its  dependents in the tree, the process is reapplied  with the new node governor. When no more  rules can be applied, the same process is applied  to each dependent of the current governor.  When all nodes have been processed, the  processing is completed,
364	Explains Technical Concepts	The second system is a more traditional word-spotting  system. There are 66 keywords plus 12 variants of those key-  words for a total of 78 keyword models. There is a background  model (see Figure 1) that tries to account for the rest of the  observed acoustics, making a total of 79 words in this second  system. This second system also uses a back-off bigram gram-  mar, but all non-keywords are replaced with the background  word when computing language model probabilities.
904	Assumes Prior Knowledge	Second, there is a theoretical lnotivation.  With a mapping between tile ToBI and the slpo  systems for intonation almotation, it will be  possible to link the 1)honetic analysis of speech  data to an interpretation f intonational mean-  ing as it is proposed by SFO. Existing speech  corpora that are acoustically analysed and an-  notated with ToBI tail then be used to test  some of the assumptions brought forward by  SFO about the natm:e of intonation. Also, with  a mapping between ~oBI and SFG annotations,  an exchange of annotated corpora between ToBI  and SFO users would be possible.
585	Explains Technical Concepts	Since the training is an iterative hill climbing tech-  nique, initialization can be important o avoid converg-  ing to a poor solution. In our system, we choose ini-  tial models, using one of the two methods described be-  low. These models are used as input to several iterations  of context-independent training followed by context-  dependent training. We add a small padding value to  the weight estimates in the early training passes to de-  lay premature parameter convergence.
220	Explains Non-Technical Concepts	The remaining of this paper is structured as fol- lows: Section 2 gives an overview of the diffi- culties in syntactic annotation, and the potential ways of improving the annotation efficiency with- out damaging the quality; Section 3 presents the new annotation method which is based on a statis- tical discriminant ranking model; Sections 4 and 5 describe the setup and results of a series of anno- tation experiments; Section 6 concludes the paper and proposes future research directions.
256	Explains Technical Concepts	The use of phrase recognition has been shown to be helpful, and, optimally, we would like to include it. Hull and Grefenstette 1996 showed the upper bound of the improvements possible by using lexicalized phrases. Every phrase that appeared was added to the dictionary, and that tactic did aid retrieval. Both statistical co-occurrence and syntactic phrases are also possible approaches. Unfortunately, the extra-system approach we take here relies heavily on the external machine translation to preserve phrases intact. If AltaVista (or, in the case of Langenscheidt, he user) recognizes a phrase and translates it as a unit, the translation is better and retrieval is likely to be better. If, however, the translation mistakenly misses a phrase, retrieval quality is likely to be worse. As for compositional noun phrases, if the translation preserves normal word order, then the PicmreQuest-internal oun phrase recognition will take effect. That is, ifjeune fille translates as young girl, then PictureQuest will understand that young is an adjective modifying girl. In the more difficult case, if the translation preserves the correct order in translating la selva africana, i.e. the African jungle, then noun phrase recognition will work. If, however, it comes out as the jungle African, then retrieval will be worse. In the architecture d scribed here, fixing this problem requires access to the internals of the machine translation program.
114	Explains Technical Concepts	2.3. Pseudo T ime-Synchronous  Stack  Search   A compromise between the strict time-synchronous search  and the best-first stack search can be called the Pseudo Time-  Synchronous Stack Search. In this search, the shortest hy-  pothesis (i.e. the one that ends earliest in the signal) is  updated first. Thus, all of the active hypotheses are within  a short time delay of the end of the speech signal. To keep  the algorithm from requiring exponential time, a beam-type  pruning is applied to all of the hypotheses that end at the  same time. Since this method advances one hypothesis at  a time, it can take advantage of a powerful fast match al-  gorithm. In addition, it is possible to use a higher order  language model without he computation growing with the  number of states in the language model.
466	Assumes Prior Knowledge	Given the above definition, MMR computes  incrementally the standard relevance-ranked list when  the parameter ~=1, and computes a maximal diversity  ranking among the documents in R when X=0. For  intermediate values of ~, in the interval \[0,1\], a linear  combination of both criteria is optimized. Users wishing  to sample the information space around the query,  should set ~, at a smaller value, and those wishing to  focus in on multiple potentially overlapping or  reinforcing relevant documents, hould set ~, to a value  closer to 1. For document retrieval, we found that a  particularly effective search strategy (reinforced by the  user study discussed below) is to start with a small L  (e.g. ~, = .3) in order to understand the information space  in the region of the query, and then to focus on the most  important parts using a reformulated query (possibly via  relevance feedback) and a larger value of ~ (e.g. ~, = .7).  Note that the similarity metric Sim 1 used in document  retrieval and relevance ranking between documents and  query could be the same as Sim2 between documents  (e.g., both could be cosine similarity), but this need not  be the case. A more accurate, but computationally more  costly metric could be used when applied only to the  elements of the retrieved ocument set R, given that IRI  << ICI, if MMR is applied for re-ranking the top portion  of the ranked list produced by a standard IR system.
324	Assumes Prior Knowledge	4.3 Experimental Results  We first compare our results with double propa- gation on recall and precision for different cor- pus sizes. The results are presented in Tables 2,  3, and 4 for the four data sets. They show the  precision and recall of 1000, 2000, and 3000  sentences from these data sets. We did not try  more sentences because manually checking the  recall and precision becomes prohibitive. Note  that there are less than 3000 sentences for ?Cars?  and ?LCD? data sets. Thus, the columns for  ?Cars? and ?LCD? are empty in Table 4. In the  Tables, ?DP? represents the double propagation  method; ?Ours? represents our proposed method;  ?Pr? represents precision, and ?Re? represents  recall.       From the tables, we can see that for corpora in  all domains, our method outperforms double  propagation on recall with only a small loss in  precision. In data sets for ?Phone? and ?Mat- tress?, the precisions are even better. We also  find that with the increase of the data size, the  recall gap between the two methods becomes  smaller gradually and the precisions of both me- thods also drop. However, in this case, feature  ranking plays an important role in discovering  important features.
65	Explains Non-Technical Concepts	Introduction  One major goal of information extraction (IE)  technology is to help users quickly identify a  variety of relations and events and their key  players in a large volume of documents. In  contrast with this goal, state-of-the-art  information extraction systems, as shown in the  various Message Understanding Conferences  (MUCs), extract a small number of relations and  events. For instance, the most recent MUC,  MUC-7, called for the extraction of 3 relations  (person-employer, maker-product, and  organization-location) and 1 event (spacecraft  launches). Our goal is to develop an IE system  which scales up to extract as many types of  relations and events as possible with a minimum  amount of porting effort combined with high  accuracy. Currently, REES handles 100 types of  relations and events, and it does so in a modular,  configurable, and scalable manner.  Below, Section 1 presents the ontologies of  relations and events that we have developed.  Section 2 describes REES' system architecture.  Section 3 evaluates the system's performance,  and offers a qualitative analysis of system errors.  Section 4 discusses future directions.
996	Assumes Prior Knowledge	ABSTRACT  In the past year at Carnegie Mellon steady progress has been made  in the area of acoustic and language modeling. The result has been  a dramatic reduction in speech recognition errors in the SPHINX-II  system. In this paper, we review SPHINX-I/and summarize our re-  cent efforts on improved speech recognition. Recently SPHINX-I/  achieved the lowest error ate in the November 1992 DARPA eval-  uations. For 5000-word, speaker-independent, continuous, peech  recognition, the error ate was reduced to 5%.
408	Assumes Prior Knowledge	We report on the analysis of a sl)eech cor-  pus compiled fl'om Halliday (1970) with ToBI  and SFO labels (See. 3). The intonation analy-  sis is based on an acoustic analysis of the speech  data in terms of fundamental frequency (F0).
320	Assumes Prior Knowledge	4.1 Data Sets  We used four diverse data sets to evaluate our  techniques. They were obtained from a com- mercial company that provides opinion mining  services. Table 1 shows the domains (based on  their names) and the number of sentences in  each data set (?Sent.? means the sentence). The  data in ?Cars? and ?Mattress? are product re- views extracted from some online review sites.  ?Phone? and ?LCD? are forum discussion posts  extracted from some online forum sites. We  split each review/post into sentences and the  sentences are POS-tagged using the Brill?s tag- ger (Brill, 1995). The tagged sentences are the  input to our system.
400	Explains Technical Concepts	To test this hypothesis, we filtered all parallel  and monolingual training data for the E-F system  with a language guessing tool called text_cat  (Cavnar and Trenkle, 1994). From parallel data,  we filtered out sentence pairs whose French side  had a high probability of not being French; from  LM training data, sentences with a high non- French probability. We set the filtering level by  inspecting the guesser?s assessment of news- commentary sentences, choosing a rather aggres- sive level that eliminated 0.7% of news- commentary sentence pairs. We used the same  level to filter Europarl (0.8% of sentence pairs  removed), UN (3.4%), GigaFrEn (4.7%), and  ?mono? (4.3% of sentences).     Table 2 shows the results: a small but consis- tent gain (about +0.2 BLEU without rescoring).  We have not yet confirmed the hypothesis that  copies of source-language words in the paired  target sentence within training data can damage  system performance via backward probabilities.      4.5 Fixing truecasing   Our truecaser doesn?t work as well as truecasers  of other WMT groups: we lost 1.4 BLEU by tru- ecasing in both language directions, while others  lost 1.0 or less. To improve our truecaser, we  tried: 1. Training it on all relevant data and 2.  Collecting 3-gram case-pattern statistics instead  of unigrams. Neither of these helped significant- ly. One way of improving the truecaser would be  to let case information from source words influ- ence the case of the corresponding target words.  Alternatively, one of the reviewers stated that  several labs involved in WMT have no separate  truecaser and simply train on truecase text. We  had previously tried this approach for NIST Chi- nese-English and discarded it because of its poor  performance. We are currently re-trying it on  WMT data; if it works better than having a sepa- rate truecaser, this was yet another area where  lessons from Chinese-English were misleading.
991	Explains Technical Concepts	2.2 File Name Fetching We now assume that a pair of parallel texts exists on the same site. To search for parallel pairs on a site, PTMiner first has to obtain all (or at least part of) the HTML file names on the site. From these names pairs are scanned. It is possible to use a Web crawler to explore the candidate sites completely. However, we can take advantage of the search engines again to accelerate the process. As the first step, we submit the following query to the search engines: host : hostname to fetch the Web pages that they indexed from this site. If we only require a small amount of parallel texts, this result may be sufficient. For our purpose, however, we need to explore the sites more thor- oughly using a host crawler. Therefore, we continue our search for files with a host crawler which uses the documents found by the search engines as the starting point.
327	Explains Non-Technical Concepts	1. Previous Work As mentioned earlier, some customer service centers now allow users to say either the option number or a keyword from a list of options/descriptions. However, the only known work which automates part of a customer service center using natural language dialogue is the one by Chu-Carroll and Carpenter (1999). The system described here is used as the front-end of a bank's customer service center. It routes calls by extracting key phrases from a user utterance and then by statistically comparing these phrases to phrases extracted from utterances in a training corpus consisting of pre-recorded calls where the routing was done by a human. The call is routed to the destination of the utterance from the training corpus that is most "similar" to the current utterance. On occasion, the system will interact with the user to clarify the user's request by asking a question. For example, if the user wishes to reach the loan department, the system will ask if the loan is for an automobile, or a home. Other related work is (Georgila et al, 1998).
703	Explains Non-Technical Concepts	At the time when the work was terminated in 1990, the system had a main translation dictionary of about 8000 words, accompanied by so called transducing dictionary covering another 2000 words. The transducing dictionary was based on the original idea described in Kirschner (1987). It aimed at the exploitation of the fact that technical terms are based (in a majority of European languages) on Greek or Latin stems, adopted according to the particular derivational rules of the given languages. This fact allows for the "translation" of technical terms by means of a direct transcription of productive ndings and a slight (regular) adjustment of the spelling of the stem. For example, the English words localization and discrimination can be transcribed into Czech as "lokalizace" and "diskriminace" with a productive nding -ation being transcribed to -ace. It was generally assumed that for the pair Czech/Russian the transducing dictionary would be able to profit from a substantially greater number of productive rules. This hypothesis proved to be wrong, too (see B6mov~, Kubofi (1990)). The set of productive ndings for both pairs (English/Czech, as developed for an earlier MT system from English to Czech, and Czech/Russian) was very similar.
10	Mathematically-Oriented Paragraph	In the present case, tile multivariate vectors are  tile fi'equencies of occurrence of tile most  fi'equent words of the BNC lbr each text sample  and the naturally occurring groups are the four  text genres. We applied discriminant analysis to  tile training genre-corpus using 5 to 75 most  fiequent words of BNC with a step of 5 words.  Tile classification models were, then, cross-  validated by applying them to the corresponding  test corpus. The same procedure was followed  using as style markers tile fi'equencies of  occurrence of the most fi'equent words of the  training corpus (according to the original  method of l~urrows). Comparative results in  terms of classification error rate are given in  figure 2. As can been seen, tile best perlbrnlance  achieved by our approach is 2.5% error rate  (2/80) based oil the 30 most frequent words of  the BNC while the best performance of the  Burrows' approach is 6.25% error rate (5180)  based on the 55 most fi'equent words of the  training corpus.
27	Explains Non-Technical Concepts	4 .2  Exper iments   We conducted several series of experiments to explore  issues associated with parameter allocation and train-  ing. The results are compared to a baseline, non-mixture  SSM that uses full covariance Gaussian distributions.  The first set of experiments examined the number of  component densities in the mixture, together with the  choice of full- or diagonal-covariance matrices for the  mixture component densities. Although the full covari-  ance assumption provides a more detailed description  of the correlation between features, diagonal covariance  models require substantially ess computation and it may  be possible to obtain very detailed models using a larger  number of diagonal models.
804	Assumes Prior Knowledge	The morphological synthesis of Slovak is based on a monolingual dictionary of SIovak, developed by J.Hric (1991-99), covering more than \]00,000 dictionary entries. The coverage of the dictionary is not as high as of  the Czech one, but it is still growing. It aims at a similar coverage of Slovak as we enjoy for Czech. ad 7. The export of  the output of the system (~ESILKO into the translation memory (of TRADOS Translator's Workbench) amounts mainly to cleaning of all irrelevant SGML markers. The whole resulting Slovak sentence is inserted into the appropriate location in the original translation memory file. The following example also shows that the marker <CrU> contains an information that the target language sentence was created by an MT system. Example 3. -A  sample of the translation memory containing the results of MT <RTF Preamble>...</RTF Preamble> <TrU> <CRD>23051999 <CrU>MT! <Seg L=CS_01>Pomoci v~kazu ad-hoc mfi~ete rychle a jednodu~e vytv~i~et re,erie. <Seg L=SK_01>Pomoci v~kazov ad-hoc m6~ete r~chio a jednoducho vytvhrat' re,erie. </TrU>
964	Explains Technical Concepts	The data shows that our method exceeds the  compared ones evidently. The accuracy increas- es by 15.13% at most, and the errors by referent  decreases by 3.92% at least. In contrast to the  SCRM, we avoid the limitation that SCRM only  concentrates on the nearest distance for choosing  referent. Meanwhile, because the SCRM pays no  attention to the normalization for fuzzy temporal  expressions, the error by others (e.g. granularity)  is greater than ours. Additionally, the STVM  method applies the report time or the publication  time of the document as the reference time for  the whole text, so there is no referent updating in  process. We mark all errors as referent errors as  long as they involve with false reference time in  results analysis, therefore, the STVM gets the  highest referent errors ratio.  With respect to the defuzzification, we eva- luate it on fuzzy times separately. All defuzzi- fied fuzzy times are assessed by human, and  then decided whether they are acceptable to the  context. The evaluation results are shown in Ta- ble 7.
212	Mathematically-Oriented Paragraph	The function L is concave, and can be effi- ciently maximized by standard techniques such as stochastic gradient and limited memory quasi- Newton (L-BFGS) algorithms. The parameters ?k ?w and ?t are optimized iteratively until converge. 5 Finding the Most Likely Assignments The objective of inference is to find y? = {r?, s?} = argmax{r,s} P (r, s|x) such that both s? and r? are optimized simultaneously. Unfortu- nately, exact inference to this problem is generally prohibitive, since it requires enumerating all pos- sible segmentation and corresponding relation as- signments. Consequently, approximate inference becomes an alternative.
931	Explains Non-Technical Concepts	 5. Summary  We have discussed the search problem in speech recognition  and concluded that, in our opinion, it is no longer worth con-  sidering parallel or special propose hardware for the speech  problem, because we have been able to make faster progress  by modifying the basic search algorithm in software. At  present, the fastest recognition systems are based entirely  on software implementations. We reviewed several search  algorithms briefly, and discussed the advantage of time-  synchronous earch algorithms over other basic strategies.  The Forward-Backward Search algorithm has turned out to  be an algorithm of major importance in that it has made  possible the first real-time recognition of 20,000-word vo-  cabularies in continuous peech. Finally, we demonstrated  that the computation required by this algorithm grows as the  cube root of the vocabulary size, which means that real-time  recognition with extremely large vocabularies i feasible.
322	Explains Non-Technical Concepts	5. SUMMARY  This paper describes how SRI has applied our speaker-  independent large-vocabulary CSR system (DECIPHER TM) to  the keyword-spotting task. A transcription is generated for the  incoming spontaneous speech by using a CSR system, and any  keywords that occur in the transcription are hypothesized. We  show that the use of improved models of non-keyword speech  with a CSR system can yield significantly improved keyword  spotting performance.
488	Explains Technical Concepts	Over the years DARPA has funded major programs in  special-purpose VLSI and parallel computing environments  specifically for speech recognition, because it was taken for  granted that this was the only way that real-time speech  recognition would be possible. However, these directions  became major efforts in themselves. Using a small num-  ber of processors in parallel was easy, but efficient use of  a large number of processors required a careful redesign of  the recognition algorithms. By the time high efficiency was  obtained, there were often faster uniprocessors available.  Design of special-purpose VLSI obviously requires consid-  erable effort. Often by the time the design is completed, the  algorithms implemented are obsolete and much faster gen-  eral purpose processors are available in workstations. The  result is that neither of these approaches has resulted in real-  time recognition with vocabularies of 1,000 words or more.  Another approach to the speech recognition search problem  is to reduce the computation needed by changing the search  algorithm. For example, IBM has developed a flexible stack-  based search algorithm and several fast match algorithms  that reduce the search space by quickly eliminating a large  fraction of the possible words at each point in the search.  In 1989 we, at BBN \[1\], and others \[2, 3\] developed the  N-best Paradigm, in which we use a powerful but inexpen-  sive model for speech to find the top N sentence hypotheses  for an utterance, and then we rescore ach of these hypothe-  ses with more complex models. The result was that the  huge search space described by the complex models could  be avoided, since the space was constrained to the list of  N hypotheses. Even so, an exact algorithm for the N-best  sentence hypotheses required about 100 times more com-  putation than the simple Viterbi search for the most likely  sentence.
757	Explains Technical Concepts	First, we establish four lists for English pre- fixes, English suffixes, Chinese prefixes and Chi- nese suffixes. For example: Engl ish P re f ix  = {e, en, e_, en_, e - ,  en - ,  ...}. For each file in one lan- guage, if a segment in its name corresponds to one of the language affixes, several new names are gener- ated by changing the segment to the possible corre- sponding affixes of the other language. If a generated name corresponds to an existing file, then the file is considered as a candidate parallel document of the original file.
698	Explains Non-Technical Concepts	There are a number of ways to obtain "phrases"  from text. These include generating simple col-  locations, statistically validated N-grams, part-of-  speech tagged sequences, syntactic structures, and  even semantic oncepts. Some of these techniques  are aimed primarily at identifying multi-word terms  that have come to function like ordinary words, for  example "white collar" or "electric car", and cap-  turing other co-occurrence idiosyncrasies associated  with certain types of texts. This simple approach  has proven quite effective for some systems, for ex-  ample the Cornell group reported (Buckley et al,  1995) that adding simple collocations to the list of  available terms can increase retrieval precision by as  much as 10%.
792	Assumes Prior Knowledge	It is worih noting that tile performance of the  classification model is not improved using more  words beyond a certain threshold (in our  approach 30 words). This is due to tlle training  data overfitting. Figure 3 shows the training  corpus in tile space of the first two discriminant  functions based on the 10, 30, and 70 most  frequent words of tile BNC.
555	Explains Non-Technical Concepts	We have investigated two methods for choosing the ini-  tial models. In the first, we cluster the training data  using the K-means algorithm and then estimate a mean  and covariance from the data corresponding to each clus-  ter. These are then used as the parameters of the compo-  nent Gaussian densities of the initial mixture. In the sec-  ond method, we initialize from models trained in a non-  mixture version of the SSM. The initial densities are cho-  sen as means of triphone models, with covariances chosen  from the corresponding context-independent model. For  each phone in our phone alphabet we iteratively choose  the triphone model of that phone with the highest fre-  quency of occurrence in training. The object of this pro-  cedure is to attempt o cover the space of phones while  using robustly estimated models.    A different way to reduce the parameter dimension is to  continue to model the complete trajectory across a seg-  ment, but assume independence b tween subsets of the  features of a frame. This case can be expressed in the  general form of (2) if we reinterpret the Yj as vectors  with the same number of frames as the complete seg-  ment, but for each frame, only a specific subset of the  original frame's features are used. We can of course com-  bine these two approaches, and assume independence  between observations representing feature subsets of dif-  ferent microsegmental units. There are clearly a large  number of possible decompositions of the complete seg-  ment into time and feature subsets, and the correspond-  ing models for each may have different properties. In  general, because of constraints of model dimensionality  and finite training data, we expect a trade-off between  the ability to model trajectories across time and to model  the correlation of features within a local time region.  Although no single model of this form may have all the  properties we desire, we do not necessarily have to choose  one to the exclusion of all others. All the models dis-  cussed here compute probabilities over the same obser-  vation space, allowing for a straightforward combination  of different models, once again using the simple mecha-  nism of non-tied mixtures:  P(Y I oc) = ~I~wi jkP(Y j la i j k ) .   i j k  In this case, each of the i components of the leftmost  summation is some particular ealization of the general  model expressed in Equation (2). Such a mixture can  combine component models that individually have ben-  eficial properties for modeling either time or frequency  correlation, and the combined model may be able to  model both aspects well. We note that, in principle,  this model can also be extended to larger units, such as  syllables or words.
621	Explains Technical Concepts	This system was used to create the submis- sion to the Shared Translation Task of the WMT 2010. After submission we performed additional experiments which only led to inconclusive re- sults. Adding the bilingual POS language model and introducing the unaligned word feature to the phrase table only improved on the development set, while the scores on the test set decreased. A third bilingual language model based on stem in- formation again only showed noteworthy effects on the development set.
445	Explains Technical Concepts	4.2 The Relationship of Accuracy, Occurrence  and Length  In this section, we consider the relationship of the  extraction accuracy to the string lengths and  occurrences. Figure 2 and 3 depict that both  precision and recall have tendency to increase as  string occurrences are getting higher. This implies  that the accuracy should be higher for larger  corpora. Similarly, in Figure 4 and 5, the accuracy  tends to be higher in longer strings. The new  created words or loan words have tendency to be  long. Our extraction, then, give a high accuracy  and very useful for extracting these new created  words.
105	Explains Technical Concepts	4 Evaluation Evaluating precision and recall on a large corpus is a difficult task. We used the evaluation methods detailed in Flank 1998. Precision was evaluated using a crossing measure, whereby any image ranked higher than a better match was penalized. Recall per se was measured only with respect o a defined subset of the images. Ranking incorporates some recall measures into the precision score, since images ranked too low are a recall problem, and images marked too high are a precision problem. If there are three good matches, and the third shows up as #4, the bogus #3 is a precision problem, and the too-low #4 is a recall problem. For evaluation of the overall cross-language retrieval performance, we simply measured the ratio between the cross-language and monolingual retrieval accuracy (C/M%).
735	Assumes Prior Knowledge	2 Related Work  In general, several research works on  normalizing temporal expressions, which are  involved in English [Mani and Wilson, 2000],  French [Vozov, 2001], Spanish [Saquete et al,  2002], Korean [Jang et al, 2004] and Chinese  [Wu et al, 2005; Lin et al, 2008], have been  reported in recent years. Among them, the hand- crafted rules-based methods [Saquete et al, 2002;  Schilder and Habel, 2001; Mani and Wilson,  2000] can deal with various temporal  expressions, but the procedure to build a robust  rules system is quite time-consuming. With  regard to the machine learning for normalization  [Jang et al, 2004; Wu et al, 2005; Vicente-Diez  et al, 2008], the potential task is the  classification which is deciding one explanation  of a temporal expression from several  alternatives.
247	Explains Technical Concepts	These four modules operate by matching patterns  of successively increasing complexity against the in-  put. The patterns are regular expressions which trig-  ger associated actions. The actions perform opera-  tions on the logical \]orm representation (LF) of the  processed segments of the discourse. The discourse is  4Name recognition is a well-researched topic, with the best  available systems today reaching 96% accuracy in narrow do-  mains.
781	Explains Technical Concepts	2.2 Network Representat ion f Ambiguity  The ambiguous morphemes extracted in the  morphological nalysis are stored as common data in a  network form to reduce both storage and processing  overhead. Figure 2 shows an example of a morpheme  network. Each morpheme is represented by an arc. Both  ends of each morpheme are indicated by circles. A double  circle corresponds to the end of a Bunsetsu, whereas a  single circle corresponds to the boundary of a morpheme  in a Bunsetsu.
733	Assumes Prior Knowledge	4.2 Comparison Wi th  MT Systems One advantage of a parallel text-based translation model is that it is easier to build than an MT system. Now that we have examined the CLIR performance of the translation model, we will compare it with two existing MT systems. Both systems were tested in E-C CLIR.
118	Explains Technical Concepts	ALLiS 2 (Architecture for Learning Linguistic  Structures) is a learning system which uses the-  ory refinement in order to learn non-recursive  NP and VP structures (Ddjean, 2000). ALLiS  generates a regular expression grammar which  describes the phrase structure (NP or VP). This  grammar is then used by the CASS parser (Ab-  hey, 1996). Following the principle of theory re-  finement, tile learning task is composed of two  steps. The first step is the generation of an  initial wa, mmar. The generation of this grmn-  mar uses the notion of default values and some  background knowledge which provides general  expectations concerning the immr structure of  NPs and VPs. This initial grammar provides  an incomplete and/or incorrect analysis of tile  data. The second step is the refinement of this  grammar. During this step, the validity of the  rules of the initial grammar is checked and the  rules are improved (refined) if necessary. This  refinement relies on the use of two operations:  the contextualization (i which contexts uch a  tag always belongs to the phrase) and lexical-  ization (use of information about the words and  not only about POS).
165	Explains Non-Technical Concepts	7 Conclusions In this paper we described APE, an integrated planner and execution system that we have implemented as part of the Atlas dialogue manager. APE uses HTN-style operators and is based on reactive planning concepts. Although APE is intended largely for use in domains with hierarchical, multi-turn plans, it can be used to implement any conversation-based system, where turns in the 'conversation' may include graphical actions and/or text. We illustrated the use of APE with an example from the Atlas-Andes physics tutor. We showed that previous models based on finite-state machines are insufficient to handle the nested subdialogues and abandoned partial subdialogues that occur in practical applications. We showed how APE generated a sample dialogue that earlier systems could not handle.
900	Explains Technical Concepts	Similarly, the limited power of the tree  transformation rule formalism distinguishes the  framework from other NLP frameworks based  on more general processing paradigms uch as  unification of FUF/SURGE in the generation  domain (Elhadad and Robin, 1992).
245	Explains Technical Concepts	4 Formal Modelling  With regard to relations of repair syntax and the  editing structuring in repairs, instead of only  looking into their surface structure, the syntactic  regularity in German and Chinese NP-repairs  can be modelled in the form of finite state  automata. We again take German as example.
924	Explains Technical Concepts	2. L inguist ic Assumptions  The grammatical framework underlying the linguistic  aspects of the system is that of Generalized Phrase  Structure Grammar, as set out in Gazdar et al (1985).  Morphological categories employed here correspond to the  syntactic categories in that work, and the type of syn-  tactic information present in dictionary entries is  intended to facilitate the use of the system as part of a  more general GPSG-based program. In developing our  prototype, we have adopted many of the proposals made  in that work. To that extent, certain assumptions about  a correct analysis of English sentence syntax are built in  to the lexlcal entries, but this should not preclude adap-  tation by users to suit different analyses.
630	Explains Non-Technical Concepts	1. Introduction  Word-breaking is an unavoidable and crucial first  step toward sentence analysis in Japanese. In a  sequential model of word-breaking and syntactic  analysis without a feedback loop, the syntactic  analyzer assumes that the results of word-breaking  are correct, so for the parse to be successful, the  input from the word-breaking component must  include all words needed for a desired syntactic  analysis. Previous approaches to Japanese word  segmentation have relied on heuristics- or  statistics-based models to find the single most  likely sequence of words for a given string, which  can then be passed to the syntactic omponent for  further processing. The most common  heuristics-based approach utilizes a connectivity  matrix between parts-of-speech and word  probabilities. The most likely analysis can be  obtained by searching for the path with the  minimum connective cost (Hisamitsu and Nitta  1990), often supplemented by additional heuristic  devices such as the longest-string-match or the  least-number-of-bunsetsu (phrase). Despite its  popularity, the connective cost method has a major  disadvantage in that hand-tuning is not only  labor-intensive but also unsafe, since adjusting the  cost for one string may cause another to break.  Various heuristic (e.g. Kurohashi and Nagao 1998)  and statistical (e.g. Takeuchi and Matsumoto 1997)  augmentations of the minimum connective cost  method have been proposed, bringing  segmentation accuracy up to around 98-99% (e.g.  Kurohashi and Nagao 1998, Fuchi and Takagi  1998).
609	Explains Non-Technical Concepts	Many of the popular wide-coverage parsers available today operate at around one newspa- per sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are de- pendency parsers that operate orders of magni- tude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing pro- cess (Nivre and Scholz, 2004).
823	Explains Non-Technical Concepts	Other related works on feature extraction  mainly use topic modeling to capture topics in  reviews (Mei et al, 2007). In (Su et al, 2008),  the authors also proposed a clustering based  method with mutual reinforcement to identify  features. However, topic modeling or clustering  is only able to find some general/rough features,  and has difficulty in finding fine-grained or pre- cise features, which is more related to informa- tion extraction.
