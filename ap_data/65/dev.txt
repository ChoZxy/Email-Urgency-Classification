840	Explains Technical Concepts	Without he analysis of at least nominal groups it is often very difficult to solve this problem, because for example the actual morphemic categories of adjectives are in Czech distinguishable only on the basis of gender, number and case agreement between an adjective and its governing noun. An alternative way to the solution of this problem was the application of a stochastically based morphological disambiguator (morphological tagger) for Czech whose success rate is close to 92?/'0. Our system therefore consists of the following modules: 1. Import of the input from so-called 'empty' translation memory 2. Morphological analysis of Czech 3. Morphological disambiguation 4. Domain-related bilingual glossaries (incl. single- and multiword terminology) 5. General bilingual dictionary 6. Morphological synthesis of Slovak 7. Export of the output o the original translation memory
793	Explains Technical Concepts	Double Propagation (Qiu et al, 2009) is a  state-of-the-art unsupervised technique for  solving the problem. It mainly extracts noun  features, and works well for medium-size  corpora. But for large corpora, this method can  introduce a great deal of noise (low precision),  and for small corpora, it can miss important  features. To deal with these two problems, we  propose a new feature mining method, which  enhances that in (Qiu et al, 2009). Firstly, two  improvements based on part-whole patterns and  ?no? patterns are introduced to increase recall.  Part-whole or meronymy is an important  semantic relation in NLP, which indicates that  one or more objects are parts of another object.
859	Assumes Prior Knowledge	The lemmatization immediately follows tagging; it chooses the first lemma with a possible tag corresponding to the tag selected. Despite this simple lemmatization method, and also thanks to the fact that Czech words are rarely ambiguous in their Part-of-speech, it works with an accuracy exceeding 98%.
612	Explains Non-Technical Concepts	In fact, a type check of the compliances of verbs, nouns adjectives and prepositions i  not only neces- sary for the semantic processing but is essential for the syntax analysis for the disambiguation aswell. In TUC, the legal combinations are carefully assem- bled in the semantic network, which then serves a dual purpose. These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope. The man saw a dog with a telescope. to be treated differently because with telescope may modify the noun man but not the noun dog, while with telescope modifies the verb see, re- stricted to person.
46	Explains Technical Concepts	6.1 Data We use the dataset of the DUC 02  summarization track for MDS because it  includes an extraction task for which model  extracts are provided. For every document set, 2  model extracts are provided each for the 200w  and 400w length categories. We use 1 randomly  chosen model extract per document set per  length category as the gold standard. We intended to use all the 59 document sets  on DUC 02 but found that for some categories,  both model extracts contain material from  sections such as the title, lead, or even byline. Those extracts are incompatible with our design  tailored for news body extracts. Therefore we  have to filter them and retain only those extracts  with all units selected from the news body. As a  result, we collect 42 200w extracts and 39 400w  extracts as our experimental dataset.
883	Explains Non-Technical Concepts	In order to test the parser on Wikipedia text, we created two test sets. The first, Wiki 300, for testing accuracy, consists of 300 sentences man- ually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). An example sentence is given in Figure 1. The data was created by manually correcting the output of the parser on these sentences, with the annotation being performed by Clark and Rimell, including checks on a subset of these cases to ensure con- sistency across the two annotators. For the ac- curacy evaluation, we calculated precision, recall and balanced F-measure over the GRs in the stan- dard way.
283	Explains Technical Concepts	For the German text, additional preprocessing steps were applied. First, the older German data uses the old German orthography whereas the newer parts of the corpus use the new German orthography. We tried to normalize the text by converting the whole text to the new German or- thography. In a first step, we search for words that are only correct according to the old writing rules. Therefore, we selected all words in the corpus, that are correct according to the hunspell lexicon1 us- ing the old rules, but not correct according to the hunspell lexicon using the new rules. In a second step we tried to find the correct spelling according to the new rules. We first applied rules describing how words changed from one spelling system to the other, for example replacing ??? by ?ss?. If the new word is a correct word according to the hun- spell lexicon using the new spelling rules, we map the words.
468	Assumes Prior Knowledge	ABSTRACT  This paper describes everal key experiments in large vocabu-  lay  speech recognition. We demonstrate that, counter to our  intuitions, given a fixed amount of training speech, the num-  ber of training speakers has little effect on the accuracy. We  show how much speech is needed for speaker-independent (SI) recognition in order to achieve the same performance as speaker-  dependent (SD) recognition. We demonstrate that, though the  N-Best Paradigm works quite well up to vocabularies of 5,000  words, it begins to break down with 20,000 words and long sen-  tences. We compare the performance of two feature preprocess-  ing algorithms for microphone independence and we describe a  new microphone adaptation algorithm based on selection among  several codebook transformations.
844	Explains Non-Technical Concepts	2 The  GDA Tagset   GI)A is a project to make on-line documents  ntachinc-ullderstanda.ble on the basis of a lin-  guistic ta.gset, while developing and si)read-  ing technologies of content-based presentation,  retrieval, question-answering, smnma.rization,  translation, among othe, rs, with much higher  quality than before. GI)A thus proposes an  integrated global plattbrm for e,h',ctronic on-  tent authoring, t)resental;ion, a,nd reuse. The  GI)A tagset 2 is an XM1, (eXtensible Markup  l,anguage) insta,nce which allows ma.chines to  automatically infex the semantic and pra.gma.tic  structures uncle, flying the raw (locuments.  Under the current sta.te of the art, GI)A-  tagging is senfiautomatic and calls for manual  correction by human mmotators; othe, rwise an-  notation would ma,ke no sense. "l~h( ,, cost in-  volved here pays, because annota,ted ocuments  are generic information contents from which to  rend(',r diverse types of 1)resenta.tions, poi;en-  tially involving summariza.tion, arra,tion, visu-  alization, translation, information retriewfl, in-  formation extra.ction, and so forth. The present  p~,per concerns summarization only, trot the  merit of GI)A-tagging is not a,t all restricted to  smmnarization, and that is why it is considered  reasonable to assume Gl)A-tagged input here.
660	Explains Technical Concepts	Abstract  In this paper we describe an implemented  framework for developing monolingual or  multilingual natural language generation  (NLG) applications and machine translation  (MT) applications. The framework  demonstrates a uniform approach to  generation and transfer based on declarative  lexico-structural transformations of  dependency structures of syntactic or  conceptual levels ("uniform lexico-structural  processing"). We describe how this  framework has been used in practical NLG  and MT applications, and report he lessons  learned.
577	Explains Technical Concepts	ABSTRACT  We investigated both English and Chinese ad-hoc  information retrieval (IR). Part of our objectives is to  study the use of term, phrasal and topical concept level  evidence, either individually or in combination, to  improve retrieval accuracy. For short queries, we  studied five term level techniques that together lead to  improvements over standard ad-hoc 2-stage retrieval  some 20% to 40% for TREC5 & 6 experiments.  For long queries, we studied linguistic phrases as  evidence to re-rank outputs of term level retrieval. It  brings small improvements in both TREC5 & 6  experiments, but needs further confirmation. We also  investigated clustering of output documents from term  level retrieval. Our aim is to separate relevant and  irrelevant documents into different clusters, and to re-  rank the output list by groups based on query and  cluster-profile matching. Investigation is still on-going.  For Chinese IR, many results were confirmed or  discovered. For example, accurate word segmentation  is not as important as first thought, but short-word  segmentation is preferable to long-word (phrase).  Simple bigram representation can give very good  retrieval. A stopword list is not necessary; and  presence of non-content terms does not hurt evaluation  results much. One only needs screening out statistical  stopwords of high frequency. Character indexing by  itself is not competitive, but is useful for augmenting  short-words or bigrams. Best results were obtained by  combining retrievals of bigram and short-word with  character epresentation. Chinese IR retums better  precision than English, and it is not clear if this is a  language-related, or collection-related phenomenon.
302	Explains Technical Concepts	Much research effort has been done to ad- dress the transliteration issue in the research  community (Knight and Graehl, 1998; Wan  and Verspoor, 1998; Kang and Choi, 2000;  Meng et al, 2001; Al-Onaizan and Knight,  2002; Gao et al, 2004; Klementiev and Roth,  2006; Sproat, 2006; Zelenko and Aone, 2006;  Li et al, 2004, 2009a, 2009b; Sherif and Kon- drak, 2007; Bertoldi et al, 2008; Goldwasser  and Roth, 2008). These previous work can be  categorized into three classes, i.e., grapheme- based, phoneme-based and hybrid methods.  Grapheme-based method (Li et al, 2004)  treats transliteration as a direct orthographic  mapping process and only uses orthography- related features while phoneme-based method  (Knight and Graehl, 1998) treats transliteration  as a phonetic mapping issue, converting source  grapheme to source phoneme followed by a  mapping from source phoneme to target pho- neme/grapheme. Hybrid method in machine  transliteration refers to the combination of sev- eral different models or decoders via re- ranking their outputs. The report of the first  machine transliteration shared task (Li et al,  2009a, 2009b) provides benchmarking data in  diverse language pairs and systemically sum- marizes and compares different transliteration  methods and systems using the benchmarking  data.
837	Explains Non-Technical Concepts	The largest remaining category of operators is the answer handlers. These operators handle a variety of answers for each of the five questions that the system can ask. The answers we recognize include categories such as "don't know" as well as specific answers (e.g. a direction perpendicular to the correct answer) which we recognize because the tutor has specific replies for them. In order to reduce the number of S: (draws acceleration vector in same direction as velocity) T: What is the definition of acceleration? S: Don't know. T: OK, let's try this. If a car was driving along east, which way would you have to push on it to make it stop? S: West. T: Exactly. The opposite direction. So the net force goes the opposite direction, and so does the acceleration. Try to draw the acceleration vector again now. S: (draws acceleration vector correctly)
273	Explains Technical Concepts	The data set statistics are shown in Tables 3 and 4.  Note that non-relevant documents (Table 4) still have  a high percentage of relevant sentences. Ten  documents in the 110 set were non-relevant and had  no relevant sentences. We also see that the summary  length or number of relevant sentences chosen per  document varies significantly.
83	Explains Non-Technical Concepts	It is quite clear that the localization from one source language to several target languages, which are typologically similar, but different from the source language, is a waste of money and effort. It is of course much easier to translate texts from Czech to Polish or from Russian to Bulgarian than from English or German to any of these languages. There are several reasons, why localization and translation is not being performed through some pivot language, representing a certain group of closely related languages. Apart from political reasons the translation through a pivot language has several drawbacks. The most important one is the problem of the loss of translation quality. Each translation may to a certain extent shift the meaning of the translated text and thus each subsequent translation provides results more and more different from the original. The second most important reason is the lack of translators from the pivot to the target language, while this is usually no problem for the translation from the source directly to the target language.
579	Assumes Prior Knowledge	The Word-Sister Convention:  When one daughter is specified for STEM, the  category of the other daughter must be an extension  of the value of STEM.  The purpose of this third convention is to allow the  subcategorization f affixes with respect to the type of  stem they may attach to. The behavlour of affixes that  attach to more than one category can be handled natur-  ally by giving them a suitable specification for STEM.  If it is desired to have anti- attached to both nouns and  adjectives, for example, the specification (STEM ((N +)))  will have that effect, since both adjectives and nouns are  extensions of the category ((N +)1.
229	Explains Non-Technical Concepts	1 Introduction  Affect sensing from open-ended text-based  natural language input is a rising research area.  Zhang et al (2008a) reported an affect detection  component on detecting simple and complex  emotions, meta-emotions, value judgments etc  from literal expressions. Recently, metaphorical  language has drawn researchers? attention since  it has been widely used to provide effective  vivid description. Fainsilber and Ortony (1987)  commented that ?an important function of  metaphorical language is to permit the  expression of that which is difficult to express  using literal language alone?. In Wallington et  al?s work (2008), several metaphorical affective  expressions (such as animal metaphor (?X is a  rat?) and affects as external entities metaphor  (?joy ran through me?)) have been intensively  studied and affect has been derived from some  simple animal metaphorical expressions.
814	Explains Technical Concepts	For example, we have collected the following  transcript for testing. Normally the director in- tervened to suggest a topic change (e.g. ?find  out why Mayid is a bully?). Thus for a testing  situation for a particular character, we use the  emotion context attached with his/her user input  starting right after the most recent director?s  intervention and ending at his/her last second  input, since such a context may belong to one  particular topic.  DIRECTOR: U R IN THE PLAYGROUND  (indicating bullying starts)  1. Lisa Murdoch: leave me alone! [angry]  2. Mayid Rahim: WAT U GONNA DU?  [neu] -> [angry]  3. Mayid Rahim: SHUT UR FAT MOUTH  [angry]  4. Elise Brown: grrrrr [angry]  5. Elise Brown: im telin da dinna lady!  [threatening]  6. Mayid Rahim: go on den [neutral] -> [an- gry]  7. Elise Brown: misssssssssssssssss [neu]  8. Elise Brown: lol [happy]  1485 9. Lisa Murdoch: mayid u gna gt banned  [threatening]  10. Mayid Rahim: BY HU [neu] -> [angry]  The affect detection component detected that  Lisa was ?angry? by saying ?leave me alone!?. It  also sensed that Mayid was ?neutral? by saying  ?WAT U GONNA DU (what are you going to  do)?? without consideration of context. From  Rasp, we obtained that the input is a simplified  question sentence (a linguistic contextual indi- cator). Thus, it implies that it could be an emo- tional situation caused by the previous context  (e.g. previous input from Lisa) and the further  processing for emotion prediction is activated.  Since we don?t have an emotional context yet at  this stage for Mayid (the very first input from  Mayid after the director?s intervention), we  cannot resort to the Markov chain and the dy- namic algorithm currently to predict the affect.  However, we could use the emotional context of  other characters to predict the affect for Mayid?s  current input since we believe that an emotional  input from a character, especially from an op- ponent character, has great potential to affect  the emotions expressed by the current speaking  character.
584	Explains Technical Concepts	2 Text  Genre  Detect ion  In order to detect automatically tile text genre  we used discriminant analy.vis, a well-known  classification technique of lnultivariate statistics  that has been used in previous work in text genre  detection (Biber, 1993; Karlgrcn and Cutting,  1994). This methodology takes some  multivariate vectors precategorized into  naturally occurring groups (i.e., training data)  and extracts a set of discriminant./imctions that  distinguish the groups. The mathenlatical  objective of discHminant analysis is to weight  and linearly combine tile discriminating  variables (i.e., style markers) in some way so  that the groups are tbrced to be as statistically  distinct as possible (Eisenbeis & Avery, 1972).  Then, discrinfinant analysis can be used lbr  predicting tile group membership of previously  unseen cases (i.e., test data).
714	Explains Technical Concepts	Pronunciation Optimization. Here we use the forward-  backward algorithm to iteratively optimize asenone sequence  appropriate for modeling multiple utterances of a word. To  explore the idea, given the multiple xamples, we train a word  HMM whose number of states is proportional to the average  duration. When the Baum-Welch reestimation reaches its  optimum, each estimated state is quantized with the senone  codebook. The closest one is used to label the states of the  word HMM. This sequence of senones becomes the senonic  baseform of the word. Here arbitrary sequences ofsenones are  allowed to provide the flexibility for the automatically learned  pronunciation. When the senone sequence of every word is  determined, the parameters ( enones) may be re-trained. Al-  though each word model generally has more states than the  traditional phoneme-concatenated word model, the number  of parameters emains the same since the size of the senone  codebook isunchanged. When senones were used for pronun-  ciation optimization i a preliminary experiment, we achieved  10-15% error reduction in a speaker-independent continuous  spelling task \[ 19\].
695	Explains Non-Technical Concepts	The present implementation represents a a major effort in bringing natural anguage into practical use. A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web ( http : //www. idi. ntnu. no/bustuc/).
911	Explains Technical Concepts	3.2. Phonetic Dictionary  Two distinct phonetic dictionaries were supplied for training and  testing purposes, We found the dictionaries for training and test-  ing were not consistent. That is, there were many words that  appeared in both dictionaries, but had different spellings. We  also modified the speRings of several words that we judged to be  wrong. However, after correcting all of these mistakes, including  the inconsistency between the training and testing dictionary, the  improvement was only 0.2%, which is statistically insignificant.  One inadequacy of the supplied dictionary was that it did not  contain any schwa phonemes to represent redue~ vowels. It  did, on the other hand, distinguish three levels of stress. But  we traditionally remove the stress distinction before using the  dictionary. So we translated all of the lowest stress level of the  UH and IH phonemes into AX and IX (We will use Random  House symbols here). This resulted in another 0.2% reduction in  word error.
117	Assumes Prior Knowledge	Referring to the defuzzification, TIMEX2  Standard [Ferro et al, 2005] takes the X place- holder to express fuzzy times? value, so the re- lated works [Jang et al, 2004; Lin et al, 2008;  Vicente-Diez and Martinez, 2009] almost follow  this vague expressing way. However, this me- thod can not address the actual situation that the  fuzzy time is referred to by other times. Based  on the human cognitive psychology, Anderson et  al. [1983] presented a classical scenario-time  shifting model that discussed the time includes  the fuzzy time is the clue to scenario shifting  when people reading. Inspired by this issue and  based on our experiments, we find all times in a  same scenario own strong dependences in tem- poral granularity, which can effectively help us  determine granularity in defuzzification. And  more details are discussed in section 4.2.  Aiming at solving these challenges above, we  establish a temporal expression normalization  system for real texts, which improves the accu- racy of temporal reference normalization re- markably by the dynamic-choosing mechanism.
349	Assumes Prior Knowledge	Figure 7: Lexico-Structural Transfer of English Lexerne  MOVE to French  More general exico-structural rules for transfer  can also be implemented using our grammar rule  formalism. Figure 8 gives an English-French  transfer ule applied to a weather domain for the  transfer of a verb modified by the adverb  ALMOST:  It almost rained.  --o II a fail l i  pleuvoir.  TRANSFER-RULE:   SX  \[ c lass :verb  \]  ( ATTR ALMOST )  <- ->  FA ILL IR  \[ c lass :verb  \]  ( I I  SX  \[ mood: in f  \] )  Figure 8: English to French Lexico-Structural  Transfer Rule with Verb Modifier ALMOST  More details on how the structural divergences  described in (Dorr, 1994) can be accounted for  using our formalism can be found in (Nasr et  al., 1998).
770	Explains Technical Concepts	   Ranking comparison between the two me- thods is shown in Tables 5, 6, and 7, which give  the precisions of top 50, 100 and 200 results  respectively. Note that the experiments reported  in these tables were run on the whole data sets.  There were no more results for the ?LCD? data  beyond top 200 as there were only a limited  number of features discussed in the data. So the  column for ?LCD? in Table 7 is empty. We rank  the extracted feature candidates based on fre- quency for the double propagation method (DP).  Using occurrence frequency is the natural way  to rank features. The more frequent a feature  occurs in a corpus, the more important it is.  However, frequency-based ranking assumes the  extracted candidates are correct features. The  tables show that our proposed method (Ours)  outperforms double propagation considerably.  The reason is that some highly-frequent feature  candidates extracted by double propagation are  not correct features. Our method considers the  feature relevance as an important factor. So it  produces much better rankings.
150	Assumes Prior Knowledge	We have additionally prepared a much larger initial  lexicon list L1 (-27K) based on the association list in  the Cxterm software. Together with the derived  lexicon L l l  (43K), we have studied the effects of  using these four lexicons for segmentation and  retrieval. The results are shown in Table 5. We  observe that larger lexicon list can lead to  incrementally better AvPre values (.463 vs .455 for  long queries and .409 vs .398 for short), but the rate of  increase is very slow. The initial 2K lexicon gives  surprisingly good results.
389	Assumes Prior Knowledge	In conclusion, it appears that simple on-line translation of queries can support effective cross-language information retrieval, for certain applications. We showed how an image retrieval application eliminates ome of the problems of cross-language r trieval, and how carefully tuned WordNet expansion simplifies word choice issues. We used a variety of machine translation systems, none of them high-end and all of them free, and nonetheless achieved commercially viable results.
74	Explains Non-Technical Concepts	While we are aware of the work being done by speech recognition companies like Nuance (www.nuance.com) and Speechworks (www.speechworks.com) in the area of providing more natural anguage dialogue-based customer service, we are not aware of any conference or journal publications from them. Some magazine articles which mention their work are (Rosen 1999; Rossheim 1999; Greenemeier 1999 ; Meisel 1999). In addition, when we tried out a demo of Nuance's ystems, we found that their systems had a very IVRish feel to them. For example, if one wanted to transfer $50 from one account o another, the system would first ask the account that the money was coming from, then the account hat the money was going to, and finally, the amount to be transferred. Therefore, a user could not say "I want to transfer $50 from my savings account o my checking account" and have the system conduct that transaction.
416	Explains Non-Technical Concepts	6 Experiments 6.1 Data Our data comes from Wikipedia2, the world?s largest free online encyclopedia. This dataset con- sists of 1127 paragraphs from 441 pages from the online encyclopedia Wikipedia. We labeled 7740 entities into 8 categories, yielding 1243 person, 1085 location, 875 organization, 641 date, 1495 year, 38 time, 59 number, and 2304miscellaneous names. This dataset alo contains 4701 relation instances and 53 labeled relation types. The 10 most frequent relation types are job title, visited, birth place, associate, birth year, member of, birth day, opus, death year, and death day. Note that this compound IE task involving entity iden- tification and relation extraction is very challeng- ing, and modeling tight interactions between enti- ties and their relations is highly attractive.
40	Explains Non-Technical Concepts	In addition to the works mentioned above, there have been several classic projects in the area of natural language dialogue like TRAINS/TRIPS project at Rochester (Allen et al., 1989, 1995, 1996), Duke's Circuit-Fixit- Shoppe and Pascal Tutoring System (Biermann et al, 1997; 1995), etc. While the Circuit-Fixit- Shoppe system helps users fix a circuit through a dialogue with the system, the TRIPS and the TRAINS projects allow users to plan their itineraries through dialogue. Duke's Pascal tutoring system helps students in an introductory programming class debug their programs by allowing them to analyze their syntax errors, get additional information on the error, and learn the correct syntax. Although these systems have been quite successful, they use detailed models of the domain and therefore cannot be used for diverse applications uch as the ones required for customer service centers. Other related work on dialogue include (Carberry, 1990; Grosz and Sidner, 1986; Reichman, 1981).
725	Explains Non-Technical Concepts	? The Web-collected corpus is noisy and it is dif- ficult to align English-Chinese t xts. The align- ment method we employed has performed more poorly than on English-French alignment. This in turn leads to poorer performance of the trans- lation model. In general, we observe a higher variability in Chinese-English translations than in English-French translations.
685	Explains Technical Concepts	5. CHINESE AD-HOC RETRIEVAL   Our research continues the work of other  investigators on Chinese IR during Tipster l&2 (e.g.  \[8\]). We have augmented our PIRCS system to handle  the 2-byte encoding of Chinese characters according to  the GB2312 convention. During processing, our  system can handle both English and Chinese present  simultaneously in documents and queries.
801	Explains Non-Technical Concepts	Introduction Although the field of machine translation has a very long history, the number of really successful systems is not very impressive. Most of the funds invested into the development of various MT systems have been wasted and have not stimulated a development of techniques which would allow to translate at least technical texts from a certain limited domain. There were, of course, exceptions, which demonstrated that under certain conditions it is possible to develop a system which will save money and efforts invested into human translation. The main reason why the field of MT has not met the expectations of sci-fi literature, but also the expectations of scientific community, is the complexity of the task itself. A successful automatic translation system requires an application of techniques from several areas of computational inguistics (morphology, syntax, semantics, discourse analysis etc.) as a necessary, but not a sufficient condition. The general opinion is that it is easier to create an MT system for a pair of related languages. In our contribution we would like to demonstrate hat this assumption holds only for really very closely related languages.
593	Explains Technical Concepts	The discriminant ranking model is applied dur- ing the manual annotation sessions. When a parse forest is loaded and the discriminants are con- structed, each discriminant is assigned an (un- normalized) score ?ki=1 ?ifi(d, Y ), and the list of discriminants is sorted by descending order of the score accordingly. The scoring and sort- ing adds negligible additional computation on the treebanking software, and is not noticeable to the human annotators. By putting those discriminants that are potentially to be manually judged near the top of the list, the model saves manual labor on scanning through the lengthy list by filtering out ambivalent discriminants.
669	Explains Technical Concepts	 3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed foreach new application. The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictio- nary. In general, the logic is generated automatically from the semantic knowledge base. The nouns play a key role in the understanding part as they constitute the class or type hierarchy. Nouns are defined in an a-kind-of hierarchy. The hierarchy is tree-structured with single inheritance. The top level also constitute the top level ontology of TUC's world.
500	Explains Technical Concepts	2 System Architecture 2.1 Pre-processing The available corpora were pre-processed using an in-house script that normalizes quotes, dashes, spaces and ligatures. We also reaccentuated French words starting with a capital letter. We significantly cleaned up the parallel Giga word corpus (noted as gw hereafter), keeping 18.1 M of the original 22.5 M sentence pairs. For exam- ple, sentence pairs with numerous numbers, non- alphanumeric characters or words starting with capital letters were removed.
618	Explains Technical Concepts	1 Introduction The purpose of the Atlas project is to enlarge the scope of student interaction in an intelligent tutoring system (ITS) to include coherent conversational sequences, including both written text and GUI actions. A key component of Atlas is APE, the Atlas Planning Engine, a "just-in- time" planner specialized for easy construction and quick generation of hierarchically organized dialogues. APE is a domain- and task-independent system. Although to date we have used APE as a dialogue manager for intelligent tutoring systems, APE could also be used to manage other types of human-computer conversation, such as an advice- giving system or an interactive help system. Planning is an essential component of a dialogue-based ITS. Although there are many reasons for using natural anguage in an ITS, as soon as the student gives an unexpected response to a tutor question, the tutor needs to be able to This research was supported by NSF grant number 9720359 to CIRCLE, the Center for Interdisciplinary Research on Constructive Learning Environments atthe University of Pittsburgh and Carnegie-Mellon University.
932	Assumes Prior Knowledge	4.2.2 Transperfect Kwok (1999) investigated the CLIR performance of an English-Chinese MT software called Transper- fect, using the same TREC Chinese collection as we used in this study. Using the MT software alone, Kwok achieved 56% of monolingual precision. The precision is improved to 62% by refining the trans- lation with a dictionary. Kwok also adopted pre- translation query expansion, which further improved the precison to 70% of the monolingual results. In our case, the best E-C CLIR precison using the translation model (and dictionary) is 56.1%. It is lower than what Kwok achieved using Transperfect, however, the difference is not large.
298	Explains Technical Concepts	The term o~(,?i,wiwi_l...)logPr(wilwi_l...) could be  merged as one item in Equation 2. Thus we can have lan-  guage probabilities directly estimated from the acoustic train-  ing data. The proposed approach is fundamentally different  from traditional stochastic language modeling. Firstly, con-  ventional language modeling uses a text corpus only. Any  acoustical confusable words will not be reflected in language  probabilities. Secondly, maximum likelihood estimation is  usually used, which is only loosely related to minimum sen-  tence error. The reason for us to keep a 0 separate from the  language probability is that we may not have sufficient acous-  tic data to estimate the language parameters atpresent. Thus,  we are forced to have a0  shared across different words so we  may have n-gram-dependent, word-dependent or even word-  count-dependent language weights. We can use the gradient  decent method to optimize all of the parameters in the USE.  When we jointly optimize L(A), we not only obtain our uni-  fied acoustic models but also the unified language models. A  preliminary experiment reduced error rate by 5% on the WSJ  task \[14\]. We will extend the USE paradigm for joint acoustic  and language model optimization. We believe that t_he USE  can further educe the error rate with an increased amount of  training data.
766	Explains Technical Concepts	This paper focuses on the Web-based Eng- lish-Chinese OOV term translation pattern, and  emphasizes particularly on the translation se- lection strategy based on the fusion of multiple  features and the translation ranking mechanism  based on Ranking Support Vector Machine  (Ranking SVM). By utilizing the CoNLL2003  corpus for the English Named Entity Recogni- tion (NER) task and manually selected new  terms in various fields, the established OOV  term translation model can ?filter? the most  possible translation candidates with better abil- ity. This paper also attempts to apply the OOV  term translation mechanism above in English- Chinese CLIR. It can be observed from the  experimental results on the data sets of Text  Retrieval Evaluation Conference (TREC) that  the obvious performance improvement for  query translation can be obtained, which is  very beneficial to CLIR and can improve the  whole retrieval performance.
348	Explains Technical Concepts	2.3.1 Declarative Template Generation  REES outputs the extracted information in the  form of either MUC-style templates, as  illustrated in Figure 1 and 2, or XML. A  crucial part of a portable, scalable system is to  be able to output different ypes of relations  and events without changing the template  generation code. REES maps XML-tagged  output of the co-reference module to templates  using declarative template definitions, which  specifies the template label (e.g.,  ATTACK_TARGET), XML attribute names  (e.g., ARGUMENT l), corresponding template  slot names (e.g., ATTACKER), and the type  restrictions on slot values (e.g., string).
346	Explains Technical Concepts	The parser scans a user utterance and returns, as output, a list of semantic tuples associated with each keyword/phrase contained in the utterance. It is mainly interested in "key words" (words that are contained in product and part descriptions, user action words, etc.) and it ignores all the other words in the user utterance. The parser also returns a special tuple containing the entire input string which may be used later by the context-based parser for sub-string matching specially in cases when the DM has asked a specific question to the user and is expecting a particular kind of response.
376	Assumes Prior Knowledge	See Table 2 for numbers on perplexity, corpus sizes, and interpolation weights. Note, for in- stance, the relatively high weight for the News Commentary corpus (0.204) compared to the Eu- roparl corpus (0.105) in the English language model for the French-English system, despite the latter being about 25 times bigger.
939	Explains Technical Concepts	6. CONCLUSION  A 2-stage retrieval strategy with pseudo-feedback  often returns better ad-hoc results than 1-stage alone.  We have further investigated term, phrasal and topical  concept level evidence methods for improving retrieval  accuracy in this situation. We showed that five term  level methods together are effective for enhancing ad-  hoc short query results some 20 to 40% for TREC5 &  6 experiments. A particularly useful technique is  collection enrichment, which simply adds domain-  related external collections to a target collection to help  improve 2 ?d stage retrieval downstream. It brings  substantial improvements in many cases and does not  hurt much in others. It works for long and short  queries in both English and Chinese IR.
928	Explains Non-Technical Concepts	2.1.2 Morpheme chain model  The lack of restrictions on the relationship of  consecutive Bunsetsu increases the ambiguity of  segmentation i the morphological analysis. This is  especailly true if the formation of compound words is not  restricted in some way. Under these circumstances the  result is often meaningless because compound words are  generated mechanically.
810	Explains Non-Technical Concepts	1. INTRODUCTION  The need for speech recognition systems and spoken lan-  guage systems to be robust with respect to their acoustical  environment has become more widely appreciated in recent  years (e.g. \[1\]). Results of many studies have demonstrated  that even automatic speech recognition systems that are  designed to be speaker independent can perform very  poorly when they are tested using a different type of micro-  phone or acoustical environment from the one with which  they were trained (e.g. \[2,3\]), even in a relatively quiet  office environment. Applications uch as speech recogni-  tion over telephones, in automobiles, on a factory floor, or  outdoors demand an even greater degree of environmental  robusmess.
965	Assumes Prior Knowledge	Abstract We propose an event-enriched model to  alleviate the semantic deficiency  problem in the IR-style text processing  and apply it to sentence ordering for  multi-document news summarization. The ordering algorithm is built on event  and entity coherence, both locally and  globally. To accommodate the event- enriched model, a novel LSA-integrated  two-layered clustering approach is  adopted. The experimental result shows  clear advantage of our model over  event-agonistic models.
274	Explains Technical Concepts	2. TRA IN ING .  2.1. Acoust ic  Mode l ing   DECIPHER TM uses a hierarchy of phonetic context-  dependent models, including word-specific, triphone, general-  ized-triphone, biphone, generalized-biphone, and context inde-  pendent models. Six spectral features are used to model the  speech signal: the eepstral vector (C1-CN) and its first and sec-  ond derivatives, and cepstral energy (CO) and its first and second  derivatives. These features are computed from an FFT filterbank  and subsequent high-pass RASTA filtering of the filterbank log  energies, and are modeled either with VQ and scalar codebooks  or with tied-mixture Gaussian models. The acoustic models used  for the Switchboard task use no cross word acoustic onstraints.
666	Explains Non-Technical Concepts	Figure 9: Transfer dictionary example with a speaker's role Even though we do not have rules and en- tries for pattern conditions and word condi- tions according to another participant's infor- mation, such as ":s-role customer'(which means the speaker's role is a customer) and ":s-gender male" (which means the speaker's gender is male), TDMT can translate xpressions corre- sponding to this information too. For example, "Very good, please let me confirm them" will be translated into "shouchiitashimasita kakunin sasete itadakimasu" when the speaker is a clerk or "soredekekkoudesu kakunin sasete kudasai" when the speaker is a customer, as shown in Example (2).
817	Assumes Prior Knowledge	Figure 6: Deep-Syntactic Rule for Determiner Insertion  The lexicon formalism has also been extended to  implement lexeme-specific lexico-structural  transfer rules. Figure 7 shows the lexico-  structural transfer of the English verb lexeme  MOVE to French implemented for a military  and weather domain (Nasr et al, 1998):  Cloud will move into the western regions.  Des nuages envahiront les rdgions ouest.  They moved the assets forward.  -.9 lls ont amen~ les ressources vers l 'avant.  The 79 dcg moves forward.  ---~ La 79 dcg avance  vers l'avant.  A disturbance will move north of Lake Superior.  --~ Une perturbation se diplacera au nord du lac  supdrieur.  LEXEME : MO~'E  CATEGORY : verb  FEATORES : \[ \]  TRANSFER: \[  TRANSFER-RULE:  MOVE  I ATTR INTO \ [ c lass :prepos i t ion \ ]   ( II SXl ) )  .-.>  E2~VAH IR \[class:verb\]  ( II SX1 )  TRANSFER-RULE :  MOVE  ( II $X2 )  AMENER \[class:verb\]  \[ II $X2 )  TRANSFER-RULE:  MOVE  ( ATTR SX \[Iexe~e:FORWARD class:adverb\] )  AVANCER  ( ATTR SX )  TRANSFER-RULE :  MOVE  <-->  DEPLACER \[class:verb refl:?\]  \]
170	Explains Non-Technical Concepts	8 Conclusions We have presented Exodus, a joint pilot project of the European Commission?s Directorate-General for Translation (DGT) and the European Parliament?s Directorate-General for Translation (DG TRAD) with the aim of exploring the potential of deploying new approaches to machine translation in European insti- tutions.
173	Explains Non-Technical Concepts	2 Participation in WMT 2010 Shared Task After the English-Portuguese experiments, the first lan- guage pair for which we developed a system with a sizeable amount of training data was English-to- French. This system has been developed for testing at the European Parliament. As English-to-French is also one of the eight translation directions evaluated in this year?s shared translation task, we decided to partic- ipate. The reasons behind this decision are manifold: We would like to ? know where we stand in comparison to other sys- tems, ? learn about what system adaptations are the most beneficial, ? make our project known to potential collaborators, ? compare the WMT10 evaluation results to the out- come of our in-house evaluation.
136	Explains Technical Concepts	4. Progressive Search Lattices  We have experimented with generating word lattices  where the early-pass recognition technique is a simple version of  the DECIPHER TM speech recognition system, a 4-feature,  discrete density HMM trained to recognize a 5,000 vocabulary  taken from DARPA's WSJ speech corpus. The test set is a  difficult 20-sentence subset of one of the development sets.  We define the number of errors in a single path p in a  lattice, Errors(p), to be the number of insertions, deletions, and  substitutions found when comparing the words inp to a reference  string. We define the number of errors in a word lattice to be the  minimum of Errors(p) for all paths p in the word lattice.  The following tables how the effect adjusting the beam  width and LatticeThresh as on the lattice error rate and on the  lattice size (the number of nodes and arcs in the word lattice).  The grammar used by the has approximately 10,000 nodes and  1,000,000 arcs. The the simple recognition system had a 1-best  word error-rate ranging from 27% (beam width le-52) to 30%  (beam width le-30).
86	Explains Non-Technical Concepts	The syntax field consists of a syntactic category, as  defined by Gazdar et al (1985), i.e. a set of feature-  value pairs. Some of these are relevant only to the  workings of the word grammar, and may thus be  Ignored by other components In an integrated natural  language processing system. Their purpose is to control  the distribution of morphemes in complex words, as  described in the following section.
41	Explains Technical Concepts	These properties can be used to distinguish very  likely candidates for compound words from unlikely ones.  Morpheme M can be represented by the property set ( P, C,  L ), where P, C and L mean the part of speech, the word  category and the length in Kana, respectively. A  compound word can then be modeled as a morpheme  chain, and is represented by pairs of the property set. The  pairs can be classified into three levels according to the  probability of occurrance. To generalize the  representation a ull property set ( - , - , - )  is introduced  for the edge of a compound word. Table 1 is a part of the  model representation.
66	Explains Non-Technical Concepts	4.2 Linguistic Contextual Indicators    In our study, we noticed some linguistic indica- tors for contextual communication in the rec- orded transcripts. One useful indicator is (i) im- peratives, which are often used to imply nega- tive or positive responses to the previous speak- ing characters, such as ?shut up?, ?go on then?,  ?let?s do it? and ?bring it on?. Other useful con- textual indicators are (ii) prepositional phrases  (e.g. ?by who??), semi-coordinating conjunc- tions (e.g. ?so we are good then?), subordinating  conjunctions (?because Lisa is a dog?) and  coordinating conjunctions (?and?, ?or? and  ?but?). These indicators are normally used by  the current ?speaker? to express further opinions  or gain further confirmation.
240	Explains Non-Technical Concepts	 ABSTRACT  Over the last decade technological dvances have been made which en-  able us to envision real-world applications of speech technologies. It is  possible to foresee applications where the spoken query is to be recognized  without even prior knowledge of the language being spoken, for exana-  pie, information centers in public places such as train stations and airports.  Other applications may require accurate identification of the speaker for se-  curity reasons, including control of access to confidential information or for  telephone-based transactions. Ideally, the speaker's identity can be verified  continually during the transaction, in a manner completely transparent to the user. With these views in mind, this paper presents a unified approach  to identifying non-linguistic speech features from the recorded signal using  phone-based acoustic likelihoods.
545	Explains Non-Technical Concepts	However, the traditional fashion of CRFs can only deal with single task, they lack the capabil- ity to represent more complex interaction between multiple subtasks. In the following we will de- scribe our joint model in detail for this problem.
822	Explains Non-Technical Concepts	6. SUMMARY  This paper provided an overview of work using tied-  mixture models for speech recognition. We described the  use of tied mixtures in the SSM as well as several innova-  tions in the training algorithm. Experiments comparing  performance for different parameter allocation choices  using tied-mixtures were presented. The performance  of the best tied-mixture SSM is comparable to HMM  systems that use similar input features. Finally, we pre-  sented a general method we are investigating for model-  ing segmental dependence with the SSM.
547	Explains Technical Concepts	Moreover, training material was tokenized with the tool provided for the workshop and truecased, meaning that the words occuring after a strong punctuation mark were lowercased when they be- longed to a dictionary of common all-lowercased forms; the others were left unchanged. In order to reduce the number of words unknown to the translation models, all numbers were serialized, i.e. mapped to a special unique token. The origi- nal numbers are then placed back in the translation in the same order as they appear in the source sen- tence. Since translations are mostly monotonic be- tween French and English, this simple algorithm works well most of the time.
784	Explains Technical Concepts	Figure 1: Design of the Tree Transduction Module  3 The Framework's Representations  The representations used by all instantiations of the tree transduction module in the framework  are dependency tree structures. The main  characteristics of all the dependency tree  structures are:  ? A dependency tree is unordered (in contrast  with phrase structure trees, there is no  ordering between the branches of the tree).  ? All the nodes in the tree correspond to  lexemes (i.e., lexical heads) or concepts  depending on the level of representation. I   contrast with a phrase structure  representation, there are no phrase-structure  nodes labeled with nonterminal symbols.  Labelled arcs indicate the dependency  relationships between the lexemes.  The first of these characteristics makes a  dependency tree structure a very useful  representation for MT and multilingual NLG,  since it gives linguists a representation that  allows them to abstract over numerous cross-  linguistic divergences due to language specific  ordering (Polgu~re, 1991).  We have implemented 4 different types of  dependency tree structures that can be used for  NLG, MT or both:  ? Deep-syntactic structures (DSyntSs);  ? Surface syntactic structures (SSyntSs);  ? Conceptual structures (ConcSs);  ? Parsed syntactic structures (PSyntSs).
560	Assumes Prior Knowledge	Finally, as discussed above, we did not utilise the full potential of multi-modality when distilling the dialogues. For instance, some dialogues could be further distilled if we had assumed that the system had presented a time-table. One reason for this is that we wanted to capture as many interesting as- pects intact as possible. The advantage is, thus, that we have a better corpus for understanding human- computer interaction and can from that corpus do a second distillation where we focus more on multi- modal interaction.
988	Explains Non-Technical Concepts	Parallel text alignment is still an experimental area. Measuring the confidence values of an align- ment is even more complicated. For example, the alignment algorithm we used in the training of the statistical translation model produces acceptable alignment results but it does not provide a confi- dence value that we can "confidently" use as an eval- uation criterion. So, for the moment his criterion is not used in candidate pair evaluation.
98	Assumes Prior Knowledge	1.1.2 The presupposition i dicators  The indicators for presuppositions were tested  against questions rated as "unproblematic" to  eliminate items that failed to discriminate  questions with versus without presuppositions.  We constructed a second list of indicators that  detect questions containing no presuppositions.  All indicators are listed in Table 1. These lists  are certainly far from complete, but they present a good basis for evaluating of  how well  presuppositions can be detected by an NLP  system. These rules were integrated into a  decision tree structure, as illustrated in Figure 1.
972	Explains Non-Technical Concepts	8.1 Data Sets  We created two data sets for our pilot experiments.  For the first { 110 Set} we took 50 documents from  the TIPSTER evaluation provided set of 200 news  articles spanning 1988-1991. All these documents  were on the same topic (see Figure 3). Three  evaluators ranked each of the sentences in the  document as relevant, somewhat relevant and not  relevant. For the purpose of this experiment,  somewhat relevant was treated as relevant and the  final score for the sentence was determined by a  majority vote. The sentences that received this  majority vote were tabulated as a relevant sentence  (to the topic). The document was ranked as relevant  or not relevant. All three assessors had 68%  agreement in their relevance judgments. The query  was extracted from the topic (see Figure 3).  The second data set {Model Sutures} was provided  as a training set for the Question and Answer portion  of the TIPSTER evaluation. It consisted of "model  summaries" which contained sentences of an article  that answered a list of questions. These model  sentences were used to score the summarizer. The  query was extracted from the questions.
218	Explains Technical Concepts	Generat ing  the  summary   Once sentences have been selected, they are pre-  sented in the order they occurred in the document.  Pronouns which do not have a referent in the pre-  vious sentence of the summary are filled with a  more descriptive string whenever a referent can be  determined. If space is of concern, prepositional  phrases attached to nouns (which are not nominal-  izations), appositives, conjoined noun phrases and  relative clauses are removed, provided they contain  no tokens associated with the query or the head-  line. Since determining pronoun referents and the  selection of clauses for removal are subject to er-  rors, filled pronouns are placed in square brackets  and removed clauses are replaced with an ellipsis  to indicate to the reader that the original text has  been modified.
439	Explains Non-Technical Concepts	Not surprisingly, we have noticed that the amo-  unt of improvement in recall and precision which  we could attribute to NLP, appeared to be related  to the quality of the initial search request, which  in turn seemed unmistakably related to its length  (cf. Table 1). Long and descriptive queries re-  sponded well to NLP, while terse one-sentence s arch  directives showed hardly any improvement. This  was not particularly surprising or even new, con-  sidering that the shorter queries tended to contain  highly discriminating words in them, and that was  just enough to achieve the optimal performance. On  the other hand, comparing various evaluation cat-  egories at TREC, it was also quite clear that the  longer queries just did better than the short ones,  no matter what their level of processing. Further-  more, while the short queries needed no better in-  dexing than with simple words, their performance  remained inadequate, and one definitely could use  better queries. Therefore, we started looking into  ways to build full-bodied search queries, either auto-  matically or interactively, out of users' initial search  statements.
347	Explains Technical Concepts	2 Structure of the IE System  An outline of our IE system \[5, 11, 12\] is shown in  figure 1. The system is a pipeline of modules: each  module draws on attendant KBs to process its input,  and passes its output to the next module.  The lexical analysis module is responsible for  breaking up the document into sentences, and sen-  tences into tokens. This module draws on a set of  on-line dictionaries. Lexical analysis attaches to each  token a reading, or a list of alternative readings, in  case the token is syntactically ambiguous. A read-  ing contains a list of features and their values (e.g.,  "syntactic ategory = Noun"). The lexical analyzer  incorporates a statistical part-of-speech tagger, which  eliminates unlikely readings for each token, a The name recognition module identifies proper  names in the text by using local textual cues, such  SWe wish to thank BBN for providing us with their tagger.  as capitalization, personal titles ("Mr.", "Esq."), and  company suffixes ("Inc.", "C0.").4 The next module  identifies small syntactic units, such as basic noun  groups (nouns with their left modifiers) and verb  groups. When it identifies a phrase, the system marks  the text segment with a semantic description, which  includes the semantic lass of the head of the phrase. 5  The next module (incrementally building on infor-  mation gathered by the earlier modules) finds larger  noun phrases, involving (for example) conjunction,  apposition, and prepositional phrase modifiers, us-  ing local semantic information. The following mod-  ule identifies cenario-specific clauses and nominaliza-  tions.
138	Explains Non-Technical Concepts	Although further work is needed, the new de- velopments on metaphorical and contextual af- fect sensing have improved EMMA?s perfor- mance of affect detection in the test transcripts  comparing with the previous version.  The evaluation results indicated that most of  the improvements (approximately 80%) are ob- tained for negative affect detection based on the  inference of context information. But there are  still some cases: when the two human judges  both believed that user inputs carried negative  affective states (such as angry, threatening, dis- approval etc), EMMA regarded them as neutral.  One most obvious reason is that some of the  previous pipeline processing (such as dealing  with mis-spelling, acronyms etc, and syntactic  processing from Rasp etc) failed to recover the  standard user input or recognize the complex  structure of the input which led to less interest- ing and less emotional context and may affect  the performance of contextual affect sensing.  (The work of Sproat et al (2001) can point out  helpful directions on this aspect.) Currently we  achieved 69% average accuracy rate for the  contextual affect sensing for the emotion inter- pretation of all the human controlled characters  in school bullying scenario. We also aim to ex- tend the evaluation of the context-based affect  detection using transcripts from other scenarios.  Moreover, some of the improvements (nearly  20%) in the updated affect sensing component  are made by the metaphorical processing. How- ever, since the test transcripts contained a very  small number of metaphorical language pheno- mena comparatively, we intend to use other re- sources (e.g. The Wall Street Journal and other  metaphorical databases (such as ATT-Meta,  2008)) to further evaluate the new development  on metaphorical affect sensing.
576	Explains Technical Concepts	 Double propagation assumes that features are  nouns/noun phrases and opinion words are ad- jectives. It is shown that opinion words are  usually associated with features in some ways.  Thus, opinion words can be recognized by iden- tified features, and features can be identified by  known opinion words. The extracted opinion  words and features are utilized to identify new  opinion words and new features, which are used  again to extract more opinion words and fea- tures. This propagation or bootstrapping process  ends when no more opinion words or features  can be found. The biggest advantage of the me- thod is that it requires no additional resources  except an initial seed opinion lexicon, which is  readily available (Wilson et al, 2005, Ding et  al., 2008). Thus it is domain independent and  unsupervised, avoiding laborious and time- consuming work of labeling data for supervised  learning methods. It works well for medium? size corpora. But for large corpora, this method  may extract many nouns/noun phrases which  are not features. The precision of the method  thus drops. The reason is that during propaga- tion, adjectives which are not opinionated will  be extracted as opinion words, e.g., ?entire? and  ?current?. These adjectives are not opinion  words but they can modify many kinds of  nouns/noun phrases, thus leading to extracting  wrong features. Iteratively, more and more  noises may be introduced during the process.  The other problem is that for certain domains,  some important features do not have opinion  words modifying them. For example, in reviews  of mattresses, a reviewer may say ?There is a  valley on my mattress?, which implies a nega- tive opinion because ?valley? is undesirable for  a mattress. Obviously, ?valley? is a feature, but  ?valley? may not be described by any opinion  adjective, especially for a small corpus. Double  propagation is not applicable in this situation.
335	Explains Non-Technical Concepts	1 How to detect presuppositions  We conducted a content analysis of questions  with presupposition problems to construct a list  of indicators for presuppositions. 22 questions  containing problematic presuppositions were  selected from a corpus of 550 questions, taken  from questionnaires provided by the U.S. Census  Bureau. The 22 questions were identified based  on ratings by three human expert raters. It may  seem that this problem is infrequent, but then,  these questions are part of commonly used  questionnaires that have been designed and  revised very thoughtfully.
740	Explains Technical Concepts	Stopwords are function words that do not carry  much content by themselves, and are usually removed  based on a compiled stopword list to improve precision  and efficiency. In addition, high frequency terms in a  collection, which we call statistical stopwords, are also  removed because they are too widespread. On the  other hand, stopword removal always carry the risk  that one might delete some words that might be crucial  for particular queries or documents but in general not  very useful. Examples (in English) are words like  'hope' in 'Hope Project' \[9\], or 'begin' in 'Prime  Minister Begin'. They can normally be regarded as not  content-bearing, but in the examples given they  become crucial. Removing them will adversely affect  results. Experiments with and without stopword  removal (from a list) however shows that retrieval  results are minimally affected. Chinese IR seems to  tolerate noisy indexing well. The lesson is not to use  any stopword list at all else one might run into perils as  discussed. Statistical stopwords are still removed.
160	Explains Technical Concepts	In this study we used the unlemmatized word  frequency list of the written language  component of the BNC I which comprises  roughly 89.7M tokens. Since the homographic  forms are separated, we added the frequencies of  the words with more than one forms (e.g. "to")  in order to attain a representative ordered list of  the most frequent words of the entire written  language. The resulted ordered word list of the  50 most frequent words is given in table 1.  The comparison of the most frequent word list  of the genre-corpus with the one acquired by the  BNC is given in figure 1. The common words  (i.e., those included in both lists) constitute  I Available at: http://www.itri.brighton.ac.uk/  -Adam.Kilgarriff/bnc-rcadmc.html  apt)l'oximately 75% of tile most frequent words  of BNC.
749	Explains Technical Concepts	A comparison of the pruning effect of binary and level tags for the sentence ?Playing card games is fun? is shown in Figure 4. With a level begin tag, more cells can be pruned from the col- umn for ?card?. Therefore, level tags are poten- tially more powerful for pruning. We now need a method for assigning level tags to words in a sentence. However, we cannot achieve this with a straighforward classifier since level tags are related; for example, a level tag (be- gin or end) with value 2 implies level tags with values 3 and above. We develop a novel method for calculating the probability of a level tag for a particular word. Our mechanism for calculat- ing these probabilities uses what we call maxspan tags, which can be assigned using a maximum en- tropy tagger.
288	Assumes Prior Knowledge	Abstract We describe our system for the translation task of WMT 2010. This system, devel- oped for the English-French and French- English directions, is based on Moses and was trained using only the resources sup- plied for the workshop. We report exper- iments to enhance it with out-of-domain parallel corpora sub-sampling, N-best list post-processing and a French grammatical checker.
411	Explains Technical Concepts	3 Our Solutions  To support more precise English-Chinese  OOV term translation, we establish a multiple- feature-based translation pattern based on Web  mining and Ranking SVM. On the one hand, a  Chinese key term extraction strategy is built on  the simplified extraction computation for PAT- Tree, in which the optimization processing for  the confidence of word building is improved to  a certain extent. On the other hand, translation  candidates are chosen by the fusion of multiple  features. The representation forms of local,  global and Boolean feature are constructed  under the consideration of the complex charac- teristics of English/Chinese OOV term and  Web information. Moreover, for the relevance  measurement between an OOV term to be  translated and its translation candidates, the  supervised learning based on Ranking SVM is  introduced to rank candidates precisely.  At first, given an OOV term to be translated  as a query, it is input into the Google search  engine to acquire the returned webpage snippet  set. Next, Chinese key terms are extracted  from the PAT-Tree built on the snippet set to  determine the translation candidates. Subse- quently, local, global and Boolean features are  extracted from the candidates based on the fu- sion of multiple features. Finally, the candi- dates are filtered and ranked through the su- pervised learning based on Ranking SVM.
626	Explains Technical Concepts	Upon encountering a query like "Reporting  on possibility of and search for extra-terrestrial  life/intelligence.", we assume that the user has de-  fined a class of actions, ideas, and/or entities that  he or she is interested in. The job of an informa-  tion retrieval engine is to find instantiations ofthose  classes in text documents in some database. We  view summarization as an additional step in this  process where we attempt o present he user with  the smallest collection of sentences in the document  that instantiate the user specified classes and do  not mislead the user about the overall content of  the document. By doing so, we can greatly shorten  the amount of the document hat the user must  read in order to determine whether the document  is relevant for the user's needs.
333	Assumes Prior Knowledge	The subcategorlzation class of a word remains constant  under Inflection, but is likely to be changed by the  attachment of a derlvatlonal suffix. Moreover, the sub-  categorization of a prefixed word is the same as that of  its stem. The WDaughter convention is designed to  reflect these facts by enforcing a feature correspondence  between one of the daughters and the mother. When  the feature set WDaughter is defined as including the  subcategorlzation feature SUBCAT, the convention results  in configuratkms such as:  ((SUBCAT NP)) ((SUBCAT NP))  ((V +)(N +\]) ((SUBCAT NP))  ((SUBCAT NP)) ((VFORM ING))  which show the relevant feature specifications in local  trees arising from suffixatton of an adjective with +ize to  produce a transitive verb and suffixatlon of a transitive  verb with +ing to produce a present participle.
795	Mathematically-Oriented Paragraph	The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines: ? a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start. ? a heuristics was followed that proved almost ir- reproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation. ? due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places. As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking. These problems also made it imperative to intro- duce a timeout on the parsing process of embarass- ing 10 seconds. Although most sentences, would be parsed within a second, some legal sentences ofmod- erate size actually need this time.
909	Explains Non-Technical Concepts	Although promising results have been re- ported, one of major issues is that the state-of- the-art machine transliteration approaches rely  heavily on significant source-target parallel  name pair corpus to learn transliteration model.  However, such corpora are not always availa- ble and the amounts of the current available  corpora, even for language pairs with English  involved, are far from enough for training, let- ting alone many low-density language pairs.  Indeed, transliteration corpora for most lan- guage pairs without English involved are un- available and usually rather expensive to ma- nually construct. However, to our knowledge,  almost no previous work touches this issue.
824	Explains Technical Concepts	Last but not least, a stylometric approach  proposed by Burrows (1987; 1992) uses as style  markers the li'equeneies of occurrence of the  most frequent words (typically the 50 most  fi'equent words) as regards a training corpus.  1"his method requires mininml computational  cost and has achieved remarkable results for a  wide variety of authors. Moreover, it is domain  and language independent since it does not  require the mauual selection of the words that  best distinguish the categories (i.e., fnnction  words), ltowever, in order to achieve better  results Burrows took into account some  additional restrictions, namely:  ? Expansion of the contracted forms. For  exalnple, "l'na" counts as "1" and "am".  ? Separation of common homographic tbrms.  For exalnple, the word "to" has the infinitive  and the prepositional form.  ? Exception of proper names fi'om the list of  the most frequent words.  ? Text-sampling so that only the narrative  parts of the text contribute to the  compilation of the list of the most frequent  words. Note that a 'narrative' part is simply  defined as 'non-dialogue'.  From a computational point of view, these  restrictions (except he first one) complicate the  procedure of extracting the 1nest frequent words  of the training corpus. Thus, the second  restriction requires a part-of-speech tagger, the  third has to be performed via a named-entity  recognizer, and the last requires the development  of a robust text sampling tool able to detect he  narrative parts of any text.
926	Assumes Prior Knowledge	We also tried to combine the translations given by the translation model and the dictionary. In both C-E and E-C CLIR, significant improvements were achieved (as shown in Tab. 1). The improvements show that the translations given by the translation model and the dictionary complement each other well for IR purposes. The translation model may give either exact ranslations orincorrect but related words. Even though these words are not correct in the sense of translation, they are very possibly re- lated to the subject of the query and thus helpful for IR purposes. The dictionary-based approach ex- pands a query along another dimension. It gives all the possible translations for each word including those that are missed by the translation model.
980	Explains Non-Technical Concepts	5 Evaluation and Conclusion  We carried out user testing with 220 secondary  school students from Birmingham and Darling- ton schools for the improvisation of school bul- lying and Crohn?s disease scenarios. Generally,  our previous statistical results based on the col- lected questionnaires indicate that the involve- ment of the AI character has not made any sta- tistically significant difference to users? en- gagement and enjoyment with the emphasis of  users? notice of the AI character?s contribution  throughout. Briefly, the methodology of the  testing is that we had each testing subject have  an experience of both scenarios, one including  the AI minor character only and the other in- cluding the human-controlled minor character  only. After the testing sessions, we obtained  users? feedback via questionnaires and group  debriefings. Improvisational transcripts were  automatically recorded during the testing so that  it allows further evaluation of the performance  of the affect detection component.
681	Assumes Prior Knowledge	On the other hand, any of the additional  restrictions mentioned in the introduction, and  especially the separation of the common  homographic forlns, can still be considered. The  combination of this approach with style markers  dealing with syntactic annotation seems to be a  better solution for a general-purpose automated  text genre detector. Another useful direction is  the development of a text-sampling tool able to  detect different genres within the same  document.
106	Explains Technical Concepts	9.3 Compression  An important evaluation criteria for summarization  is what is the ideal summary output length  (compression of the document) and how does it  affects the user's task. To begin looking at this issue,  we evaluated the performance of our system at  different summary lengths as a percentage of the  document length.
504	Explains Technical Concepts	We also found that it is not necessary to remove the stopwords of the source language. In fact, as il- lustrated on the right side of Fig. 5, the existence of the English stopwords has two effects on the proba- bility of the translation E -~ C: 1 They may often be found together with the Chi- nese word C. Owing to the Expectation Maxi- mization algorithm, the probability of E -~ C may therefore be reduced. 2 On the other hand, there is a greater likelihood that English stopwords will be found together with the most frequent Chinese words. Here, we use the term "Chinese frequent words" in- stead of "Chinese stopwords" because ven if a stop-list is applied, there may still remain some common words that have the same effect as the stopwords. The coexistence ofEnglish and Chi- nese frequent words reduces the probability that the Chinese frequent words are the translations of E, and thus raise the probability of E -+ C. The second effect was found to be more signifi- cant than the first, since the model trained without the English stopwords has better precision than the model trained with the English stopwords. For the correct ranslations given by both models, the model
198	Explains Technical Concepts	In order to test the decision tree, another  plain I-MB corpus (the test corpus), which  consists of 72 articles fi'om various fields, is  employed. All strings in the test corpus are  extracted and filtered out by the same process as  used in the training set. After the filtering process,  we get about 30,000 strings to be tested. These  30,000 strings are manually tagged in order that  the precision and recall of the decision tree can be  evaluated. The experimental results will be  discussed in the next section.
968	Assumes Prior Knowledge	The method is essentially multilingual because  it is based on GDA tags gild the GDA tagset  is designed to address nmltilingual coverage.  Since this tagset can en,bed various linguistic  intbrination into documents, it could be a stan-  dard tbrmat for the study of the transformation  and/or generdtion stage of doculnent summa-  rization, among other natural language process-  ing tasks.
979	Assumes Prior Knowledge	We classified the investigated ata into dif- ferent ypes of English expressions for Japanese politeness, i.e., into honorific titles, parts of speech such as verbs, and canned phrases, as shown in Table 1; however, not all types appeared in the data. For example, when the clerk said "How will you be paying, Mr. Suzuki," the Japanese translation was made polite as "donoyouni oshiharaininarimasu-ka suzuki-sama" in place of the standard expres- sion "donoyouni shiharaimasu-ka suzuki-san." Table 1 shows that there is a difference in how expressions should be made more polite ac- cording to the type, and that many polite ex- pressions can be translated by using only local information, i.e., transfer rules and dictionary entries. In the next section, we describe how to incorporate the information on dialogue partic- ipants, such as roles and genders, into transfer rules and dictionary entries in a dialogue trans- lation system.
587	Explains Technical Concepts	When we made these substitutions, we found that the word  error rate decreased by 0.2% again. While this change is not  significant, the size of the system was subtanfially decreased ue  to the smaller number of triphone models.  Finally, we reinstated the last tluee phonemes in the list, since  we were uncomfortable with removing too many distinctions.  Again, the word error rate was reduced by another 0.2%.  While each of the above improvements was miniscul?, the total  improvement from changes to the phonetic dictionary was 0.8%,  which is about a 7% reduction in word error. At the same time,  we now only have a single phonetic dictionary to keep track of,  and the system is substantially smaller.
832	Explains Non-Technical Concepts	The approach described makes several novel contribu-  tions, including: (1) a method for dramatic improvement in the  FOM (figure of merit) for word-spotting results compared to  more traditional approaches; (2) a demonstration f the benefit of  language modeling in keyword spotting systems; and (3) a  method that provides rapid porting of to new keyword vocabular-  ies.
101	Explains Non-Technical Concepts	1 Introduction In this paper we describe the systems that we built for our participation in the Shared Trans- lation Task of the ACL 2010 Joint Fifth Work- shop on Statistical Machine Translation and Met- ricsMATR. Our translations are generated using a state-of-the-art phrase-based translation system and applying different extensions and modifica- tions including Discriminative Word Alignment, a POS-based reordering model and bilingual lan- guage models using POS and stem information. Depending on the source and target languages, the proposed models differ in their benefit for the translation task and also expose different correl- ative effects. The Sections 2 to 4 introduce the characteristics of the baseline system and the sup- plementary models. In Section 5 we present the performance of the system variants applying the different models and chose the systems used for creating the submissions for the English-German and German-English translation task. Section 6 draws conclusions and suggests directions for fu- ture work.
355	Explains Non-Technical Concepts	Conclusions In this paper we have described a robust system that provides customer service for a medical parts application. The preliminary results are extremely encouraging with the system being able to successfully process approximately 80% of the requests from users with diverse accents.
205	Explains Technical Concepts	We constrained our fusion functions to be  weighted linear combinations of the five retrieval  scores lk)r a query-document pair. We considered the  possibility of more complex non-linear fusion models  through exploration of K-Ncarest Neighbor (K-NN)  classifiers. (The use of K-NN tbr selecting a single  retrieval system has been documented in \[5\]. By  contrast, we sought to use K-NN to fuse relevance  scores.) In this approach, training documents and  their rclevance judgments populatcd a space whose  (the proportion of the five top-ranked ocuments that  are relevantl. To date, we have lk~und the optimal  vector using an exhaustive search over the set of  vectors whose elements are non-negative, evenly  divisible by 0.1. and whose elements um to 1.0.  (We had tried using logistic regression to find the  coefficients, but the coefficients we found in this  manner yielded considerably lower precision than  those we found using the exhaustive search method.)
387	Explains Technical Concepts	Approach   We conducted a simple experiment with summaries  produced in the T IPSTER summarization dry run  \[8\]. For 5 queries with 200 documents each, we  took the set of summaries produced by the 6 dry-  run participants and retained only those summaries  that were true-positives, i.e., the summary was  judged 'relevant' and the full document was judged  'relevant'. Over all the queries, at least one of  the six systems produced a true-positive summary  for 96.6% of the documents, although no individ-  ual system performed nearly at that level. This  meant that some existing technology produced a  correct summary for almost every relevant docu-  ment. Hence we viewed the problem as one of bal-  ancing the capabilities of our system to behave like  the amalgamated system implicit in joined output.  Based on this result we are confident hat this class  of summarization is tractable with current tech-  nologies and this has strongly motivated our design  decisions.
704	Explains Non-Technical Concepts	5 MDS Sentence Ordering with Event and Entity Coherence In this section, we discuss how event can  facilitate MDS sentence ordering with layered  clustering on the event and sentence levels and  then how event and entity information can be  integrated in a coherence-based algorithm to  order sentences based on sentence clusters.
521	Explains Technical Concepts	PHONE-BASED ACOUSTIC  L IKEL IHOODS  The basic idea is to train a set of large phone-based rgodic  hidden Markov models (HMMs) for each non-linguistic fea-  ture to be identified (language, gender, speaker, ...). Feature  identification on the incoming signal x is then performed by  computing the acoustic likelihoods f(xlAi) for all the mod-  els Ai of a given set. The feature value corresponding to the  model with the highest likelihood is then hypothesized. This  decoding procedure can efficiently be implemented by pro-  cessing all the models in parallel using a time-synchronous  beam search strategy.
637	Explains Non-Technical Concepts	In this paper we describe a multimedia application for which cross-language information retrieval works particularly well. eMotion, Inc. has developed a natural language information retrieval application that retrieves images, such as photographs, based on short textual descriptions or captions. The captions are typically one to three sentences, although they may also Recent Web utilization data for PictureQuest indicate that of the 10% of users from outside the United States, a significant portion come from Spanish-speaking, French-speaking, and German-speaking countries. It is expected that adding appropriate language interfaces and listing PictureQuest in foreign-language search engines will dramatically increase non- English usage.
175	Explains Non-Technical Concepts	4.1 Results The English test corpus (for C-E CLIR) was the AP corpus used in TREC6 and TREC7. The short English queries were translated manually into Chi- nese and then translated back to English by the translation model. The Chinese test corpus was the one used in the TREC5 and TREC6 Chinese track. It contains both Chinese queries and their English translations.
471	Explains Non-Technical Concepts	When there is a long distance between amodifier  and its modifiee, the modifier is defined as a bun-  setsu having a deep dependency. For example,  the usual word order of modifiers in Japanese  is tlm following: a bunsetsu which contains an  interjection, a bunsetsu which contains an ad-  verb of time, a bunsetsu which contains a sub-  ject, and a bunsetsu which contains an object.  Here, the bunsetsu containing an adverb of time  is defined as a bunsetsu having deeper depen-  dency than the one containing a subject. We  call the concept representing the distance be-  tween a modifier and its modifiee the depth of  dependency.
827	Explains Technical Concepts	Thus, we constructed a genre-corpus of four  categories, namely: Editorials, Letters to the  Editor, Reportage, and Spot news taking  documents from the WSJ corpus of the year  1989. The documents containing the string  "REVIEW & OUTLOOK (Ed i to r ia l )  :"in  their headline tag were classified as editorials  while the documents containing the string  "Let te rs  to the  Ed i to r :  '" were  considered as letters to the editor. The  documents containing either the string  "What ' s  News  -" or "Who 's  News: "   were considered as spot news. Finally, all the  documents containing one of the following  strings in their headline:  " In ternat iona l  : ", "Market ing  &  Med ia : " ,  "Po l i t i cs  & Po l i cy : " ,  or  "Wor ld  Markets  : " without inchlding a line  starting with the string "@ By " were  considered as reportage. The latter assures that  no signed article is considered as reportage. For  example, the document of the above example  was not included ill any of the tour genre  categories.
222	Assumes Prior Knowledge	2 Support for Syntax-based Translation The initial release of Joshua supported only Hiero-style SCFGs, which use a single nontermi- nal symbol X. This release includes support for ar- bitrary SCFGs, including ones that use a rich set of linguistic nonterminal symbols. In particular we have added support for Zollmann and Venu- gopal (2006)?s syntax-augmented machine trans- lation. SAMT grammar extraction is identical to Hiero grammar extraction, except that one side of the parallel corpus is parsed, and syntactic labels replace the X nonterminals in Hiero-style rules. Instead of extracting this Hiero rule from the bi- text [X]? [X,1] sans [X,2] | [X,1] without [X,2] the nonterminals can be labeled according to which constituents cover the nonterminal span on the parsed side of the bitext. This constrains what types of phrases the decoder can use when produc- ing a translation. [VP]? [VBN] sans [NP] | [VBN] without [NP] [NP]? [NP] sans [NP] | [NP] without [NP] Unlike GHKM (Galley et al, 2004), SAMT has the same coverage as Hiero, because it allows non-constituent phrases to get syntactic labels us- ing CCG-style slash notation. Experimentally, we have found that the derivations created using syn- tactically motivated grammars exhibit more coher- ent syntactic structure than Hiero and typically re- sult in better reordering, especially for languages with word orders that diverge from English, like Urdu (Baker et al, 2009).
604	Explains Technical Concepts	3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sentence f . It is today common practice to use phrases as translation units (Koehn et al, 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the 122 translation process: e? = argmax e p(e|f) = argmax e {exp( ? i ?ihi(e, f))} (1) The feature functions hi are the system mod- els and the ?i weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM).
38	Explains Technical Concepts	? Main Grammar: This consists of the lexico-  structural mapping rules that apply at this  level and which are not lexeme- or concept-  specific (e.g. DSynt-rules for the DSynt-  module, Transfer-rules for the Transfer  module, etc.)  ? Preprocessing rammar: This consists of  the lexico-structural mapping rules for  transforming the input structures in order to  make them compliant with the main  grammar, if this is necessary. Such rules are  used to integrate new modules together  when discrepancies in the formalism need to  be fixed. This grammar can also be used  for adding default features (e.g. setting the  default number of nouns to singular) or for  applying default transformations (e.g.  replacing non meaning-bearing lexemes  with features).
833	Explains Technical Concepts	7. EVALUATION OF  S INGLE  DOCUMENT SUMMARIZAT ION  An ideal text summary contains the relevant  information for which the user is looking, excludes  extraneous information, provides background to suit  the user's profile, eliminates redundant information  and filters out relevant information that the user  knows or has seen. The first step in building such  summaries i extracting the relevant pieces of articles  to a user query. We performed a pilot evaluation in  which we used a database of assessor marked relevant  sentences to examine how well a summarization  system could extract the relevant sections of  documents.
8	Explains Non-Technical Concepts	For male speakers, we again tried systems of 200 and  300 diagonal covariance density systems, obtaining error  rates of 10.9% and 9.1% for each, respectively. Unlike  the females, however, this was only slightly better than  the result for the baseline SSM, which achieves 9.5%.  We tried a system of 500 diagonal covariance densities,  which gave only a small improvement in performance to  8.8% error. Finally, we tried using full-covariance Gaus-  sians for the 300 component system and obtained an  8.0% error rate. The context-dependent performance for  males using this configuration showed similar improve-  ment over the non-mixture SSM, with an error rate of  3.8% for the mixture system compared with 4.7% for the  baseline. Returning to the females, we found that us-  ing full-covariance densities gave the same performance  as diagonal. We have adopted the use of full-covariance  models for both genders for uniformity, obtaining a com-  bined word error rate of 3.3% on the Feb89 test set.  In the RM SI-109 training corpus, the training data for  males is roughly 2.5 times that for females, so it is not  unexpected that the optimal parameter allocation for  each may differ slightly.
133	Assumes Prior Knowledge	I n t roduct ion   CAMP software has been used in a variety of areas,  but at the end of T IPSTER it finishes as it started-  as a coreference annotation system. The corefer-  ence output has been used to participate in MUC-  6 and MUC-7, served as the foundation for three  types of summarization engines and been input to  a cross-document coreference system for names and  events. This document focuses on the most success-  ful of these application, a query sensitive summa-  rization system and a cross-document coreference  system.
258	Explains Technical Concepts	4.2 Lattice Phrase Extraction In translations from German to English, we often have the case that the English verb is aligned to both parts of the German verb. Since this phrase pair is not continuous on the German side, it can- not be extracted. The phrase could be extracted, if we also reorder the training corpus. For the test sentences the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the train- ing sentences, we would be able to extract the phrase pairs for originally discontinuous phrases and could apply them during translation of the re- ordered test sentences.
578	Explains Technical Concepts	Therefore, to determine possible sites for English- Chinese parallel texts, we can request an English document containing the following anchor: anchor : "engl ish version H \["in english", ...\]. Similar requests are sent for Chinese documents. From the two sets of pages obtained by the above queries we extract wo sets of Web sites. The union of these two sets constitutes then the candidate sites. That  is to say, a site is a candidate site when it is found to have either an English page linking to its Chinese version or a Chinese page linking to its English version.
935	Explains Technical Concepts	A query is processed into variable length noun  phrases using a POS-tagger from Mitre and simple  bracketing. (We have also experimented with the BBN  tagger before). Given a retrieved ocument, each noun  phrase concept of the query is then matched within up  to a 3-sentence context anywhere in the document.  When there are matches of two or more terms,  appropriate weights are noted for this phrase and the  sentence counted. In addition, the amount of coverage  of all the query phrases by the document is also a  factor by which the original RSV of a document is  boosted. However, not all documents have their RSV  modified. They need to pass a threshold for coverage.  After many experiments for the TREC 5 and 6 long  query environments, the attempt was moderately  successful as shown in Table 2. For TREC5, an  improvement in AvPre of 4% (.273 vs. .262) was  obtained, but in TREC6 only about 1% (.308 vs..305).
162	Explains Non-Technical Concepts	Furthermore, we tried to improve the translation quality by introducing additional features to the translation model. On the one hand we included bilingual language models based on different word factors into the log-linear model. This led to very slight improvements which differed also with re- spect to language and data set. We will investigate in the future whether further improvements are achievable with this approach. On the other hand we included the unaligned word feature which has been applied successfully for other language pairs. The improvements we could gain with this method are not as big as the ones reported for other lan- guages, but still the performance of our systems could be improved using this feature.
870	Explains Non-Technical Concepts	6 Discuss ion We have been presenting a method for distilling hu- man dialogues to make them resemble human com- puter interaction, in order to utilise such dialogues as a knowledge source when developing dialogue sys- tems. Our own main purpose has been to use them for developing multimodal systems, however, as dis- cussed above, we have in this paper rather assumed a speech-only system. But we believe that the basic approach can be used also for multi-modal systems and other kinds of natural language dialogue sys- tems.
317	Explains Technical Concepts	2.4 Document Structuring Stage  After the text to be summarized is available in  UNICODE encoding, its structure needs to be  determined. This is the job of the Document Struc-  turing Stage. In this stage, three tokenization  stages are performed. The first one pose of identi-  fies the paragraphs in the document. The second  tokenization stage identifies entences within each  paragraph. To identify sentence boundaries for  many languages requires a list of abbreviations for  the language. Languages such as Chinese and Japa-  nese have an unambiguous " top" character and  thus do not present his problem. Finally, word  tokenization is carried out to identify individual  words in each sentence. Here Chinese and Japa-  nese which do not use spaces between words  require some segmentation method to be applied.  The current system actually uses two character  pairs, bi-grams, for all its calculations for Japanese.  These bi-grams are produced starting at every char-  acter position in the document.  All the structuring information is stored in a  "Document Object", which is the main data struc-  ture of the system, holding all the information gen-  erated during the processing. After the  tokenization stage is complete and depending on  the lexical resources available for each language,  other stages are performed, such as Morphology,  Proper Name Recognition and Tagging.
78	Explains Non-Technical Concepts	As a novel research problem, cross document  coreference provides an different perspective from  related phenomenon like named entity recognition  and within document coreference. Our system  takes summaries about an entity of interest and  uses various information retrieval metrics to rank  the similarity of the summaries. We found it quite  challenging to arrive at a scoring metric that sat-  isfied our intuitions about what was good system  output v.s. bad, but we have developed a scoring  algorithm that is an improvement for this class of  data over other within document coreference scor-  ing algorithms. Our results are quite encouraging  with potential performance being as good as 84.6%  (F-Measure).
882	Explains Non-Technical Concepts	3. The Lexicon  The lexicon itself consists of a sequence of entries, each  in the form of a Lisp s-expression. An entry has five  elements: (1) and (ii) the head word, in its written form  and in a phonological transcription, (ill) a 'syntactic  field', (iv) a 'semantic field', and (v) a 'user field'. The  semantic field has been provided as a facility for users,  and any Lisp s-expression can be inserted here. No  significant semantic information is present in our entries,  beyond the fact that e.g. better and best are related in  meaning to good.
313	Assumes Prior Knowledge	It is important o be aware of the limitations of the method, and how 'realistic' the produced result will be, compared to a dialogue with the final sys- tem. Since we are changing the dialogue moves, by for instance providing all required information in one move, or never asking to be reminded of what the us- er has previously requested, it is obvious that what follows after the changed sequence would probably be affected one way or another. A consequence of this is that the resulting dialogue is less accurate as a model of the entire dialogue. It is therefore not an ideal candidate for trying out the systems over-all performance during system development. But for the smaller sub-segments or sub-dialogues, we be- lieve that it creates a good approximation of what will take place once the system is up and running. Furthermore, we believe distilled dialogues in some respects to be more realistic than Wizard of Oz- dialogues collected with a wizard acting as a com- puter.
48	Explains Technical Concepts	3.2. Tied-Mixture Context Modeling  We have investigated two methods for training context-  dependent models. In the first, weights are used to com-  bine the probability of different ypes of context. These  weights can be chosen by hand \[18\] or derived automat-  ically using a deleted-interpolation algorithm \[3\]. Paul  evaluated both types of weighting for tied-mixture con-  text modeling and reported no significant performance  difference between the two \[4\]. In our experiments, we  evaluated just the use of hand-picked weights.  In the second method, only models of the most de-  tailed context (in our case triphones) are estimated i-  rectly from the data and simpler context models (left,  right, and context-independent models) are computed  as marginals of the triphone distributions. The com-  putation of marginals is negligible since it involves just  the summing and normalization of mixture weights at  the end of training. This method reduces the number of  model updates in training in proportion to the number  of context ypes used, although the computation of ob-  servation probabilities conditioned on the mixture com-  ponent densities, remains the same. In recognition with  marginal models, it is still necessary to combine the dif-  ferent context ypes, and we use the same hand-picked  weights as before for this purpose. We compared the  two training methods and found that performance on an  independent test set was essentially the same for both  methods (marginal training produced 2 fewer errors on  the Feb89 test set) and the marginal trainer required  20 to 35% less time, depending on the model size and  machine memory.
217	Explains Non-Technical Concepts	We are concerned with the following question:  what response should we give a user when his in-  put cannot be analyzed for the reasons just de-  scribed? The response "please rephrase" gives the  user no clue as to how to rephrase. This leads  to the well-known "stonewalling" phenomenon,  where a user tries repeatedly, without success, to  rephrase his request in a form the system will un-  derstand. This may seem amusing to the outside  observer, but it can be terribly frustrating to the  user, and to the system designer watching his sys-  tem being used.
128	Explains Non-Technical Concepts	It is well known that he use of sex-dependent models gives  improved performance over one set of speaker-independent  models. However, this approach can be costly in terms of  computation for medium-to-large-size tasks, since recogni-  tion of the unknown sentence is typically carried out twice,  once for each sex. A logical alternative is to first determine  the speaker's sex, and then to perform word recognition us-  ing the models of selected sex. This is the approach used in  our Nov-92 WSJ system\[6\]. In these experiments he stan-  dard SI-84 training material, containing 7240 sentences from  84 speakers (42m/42f) is used to build speaker-independent  phone models. Sex-dependent models are then obtained us-  ing MAP estimation\[11\] with the SI seed models. The phone  likelihoods using context-dependent male and female mod-  els were computed, and the sex of the speaker was selected  as the sex associated with the models that gave the highest  likelihood. Since these CD male and female models are the  same as are used for word recognition, there is no need for ad-  ditional training material or effort. No errors were observed  in sex identification for WSJ on the Feb92 or Nov92 5k test  data containing 851 sentences, from 18 speakers (10m/8f).  For BREF, sex-dependent models were also obtained from  SI seeds by MAP estimation. The training data consisted of  2770 sentences from 57 speakers (28m/29f). No errors in  sex-identification were observed on 109 test sentences from  21 test speakers (10m/1 If).
305	Explains Non-Technical Concepts	TREC-5 (1996), therefore, marks a shift in our  approach away from text representation issues and  towards query development problems. While our  TREC-5 system still performs extensive text pro-  cessing in order to extract phrasal and other in-  dexing terms, our main focus moved on to query  construction using words, sentences, and entire pas-  sages to expand initial search specifications in an at-  tempt to cover their various angles, aspects and con-  texts. Based on the observations that NLP is more  effective with highly descriptive queries, we designed  an expansion method in which entire passages from  related, though not necessarily relevant documents  were quite liberally imported into the user queries.  This method appeared to have produced a dramatic  improvement in the performance of several differ-  ent statistical search engines that we tested boost-  ing the average precision by anywhere from 40% to  as much as 130%. Therefore, topic expansion ap-  pears to lead to a genuine, sustainable advance in  IR effectiveness. Moreover, we show in TREC-6 and  TREC-7 that this process can be automated while  maintaining the performance gains.
60	Explains Technical Concepts	4 Tree-Based Models A major extension of the capabilities of the Moses system is the accommodation of tree-based mod- els (Hoang et al, 2009). While we have not yet carried out sufficient experimentation and opti- mization of the implementation, we took the occa- sion of the shared translation task as a opportunity to build large-scale systems using such models. We build two translation systems: One using tree-based models without additional linguistic an- notation, which are known as hierarchical phrase- based models (Chiang, 2005), and another sys- tem that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al, 2006).
739	Explains Technical Concepts	2.5  Poss ib le  Correspondences   The rules act as filters to weed out seqnenees of character pairs,  but before a particular mapping can bc weeded out, somcthlng  needs to propose it ~s being possible. There is a list called a  list of l)ossible correspondences, or sometimes, a list of feasible  pairs - that tells which characters may correspond to which  others. Using this list, the ri:cognizer generates l)ossible Icxica\]  forms to correspond to tile input surface form. These can then bc  checked against he rules and against he lexicon. If tim rules (1o  not weed it out, and it is also in the lexicon, we have successfully  recognized a morpheme.
646	Assumes Prior Knowledge	6 Conclusion Using our novel method of level tagging for prun- ing complete cells in a CKY chart, the CCG parser was able to process almost 100 Wikipedia sen- tences per second, using both CCGbank and the output of the parser to train the taggers, with little or no loss in accuracy. This was a 103% increase over the baseline with no pruning.
846	Assumes Prior Knowledge	Conclusion  DP can detect presuppositions, and can thereby  reliably help a survey methodologist to eliminate  incorrect presuppositions. The results for DP  with respect o Pco~p are comparable to, and in  some cases even better than, the results for the  other five categories. This is a very good result,  since most of the five problems allow for "easy"  and "elegant" solutions, whereas DP needs to be  adjusted to a variety of problems.
328	Explains Technical Concepts	5.5 B igram Representat ion   We have further experimented with using simpler  representation methods uch as single characters and  bigrams (consecutive overlapping two character) for  retrieval. Bigram representation does not need any  segmentation or linguistic rules, but often over-  generates a large number of indexing terms that are not  meaningful to humans. Character indexing is even  simpler, but they are highly ambiguous ince there are  only 6763 distinct characters in the GB2312 scheme.  Surprisingly results with single characters are good,  though not competitive; but bigram results can rival  those of short-words when the queries are long. This  has important ramifications ince it means .that for  effective Chinese IR, one need not worry about which  segmentation method to use. (More intensive linguistic  processing of course still requires accurate  segmentation.) For large-scale collections, bigrram  segmentation is also more efficient ime-wise, although  it is more expensive space-wise. Table 6 shows  examples of retrieval measures using character and  bigram representation.
589	Assumes Prior Knowledge	Figure 3: Sample plan operator to allow the planner to remove goals from the agenda based on a change in circumstances. It removes goals sequentially from the top of the agenda, one at a time, until the supplied argument becomes false. Then it replaces the removed goals with an optional ist of new goals. Prune-replace allows a type of decision-making frequently used in dialogue generation. When a conversation partner does not give the expected response, one would often like to remove the next goal from the agenda and replace it with one or more replacement goals. Prune-replace implements a generalized version of this concept. APE is domain-independent a d communicates with a host system via an API. As a partner in a dialogue, it needs to obtain information from the world as well as produce output turns. Preconditions on plan operators can be used to access information from external knowledge sources. APE contains a recipe item type that can be used to execute an external program such as a call to a GUI interface. APE also has recipe items allowing the user to assert and retract facts in a knowledge base. Further details about the APE planner can be found in (Freedman, 2000).
691	Explains Non-Technical Concepts	Maxspan tag values i from 1 to x represent dis- joint events in which the largest constituent that the corresponding word begins or ends has size i. Summing the probabilities of these disjoint events gives the probability that the largest constituent the word begins or ends has a size between 1 and x, inclusive. That is also the probability that all the constituents the word begins or ends are in the range of cells from rows 1 to row x in the corre- sponding column or diagonal. And therefore that is also the probability that the chart cells above row x in the corresponding column or diagonal do not contain any constituents, which means that the column and diagonal can be pruned from row x upward. Therefore, it is also the probability of a level tag with value x.
404	Explains Technical Concepts	The noun thus formed behaves just the same as other nouns.  In particular, a pluralizing Is\] may be added, or a possessive \['s\],  or any other affix that can be appended to a noun.  There are other rules in the grammar for handling adjective  endings, more verb endings, etc. Irregular forms are handled in  a fairly reasonable way. The irregular nouns are listed in the  lexicon with form: irregular. Other rules than the ones shown  here refer to that feature; they prevent tile addition of plural  morphemes to words that are already plural. Irregular verbs  are listed in the lexicon with an appropriate value for tense (not  unifiable with inf) so that the test for infinitivcness will fail when  it should. Irregular adjectives, e.g. good, better, best are dealt  with in an analogous manner.
963	Explains Technical Concepts	The stages in the parsing pipeline are as fol- lows. First, a POS tagger assigns a single POS tag to each word in a sentence. Second, a CCG su- pertagger assigns lexical categories to the words in the sentence. Third, the parsing stage combines the categories, using CCG?s combinatory rules, and builds a packed chart representation contain- ing all the derivations which can be built from the lexical categories. Finally, the Viterbi algo- rithm finds the highest scoring derivation from the packed chart, using the normal-form log-linear model described in Clark and Curran (2007). Sometimes the parser is unable to build an anal- ysis which spans the whole sentence. When this happens the parser and supertagger interact us- ing the adaptive supertagging strategy described in Clark and Curran (2004): the parser effectively asks the supertagger to provide more lexical cate- gories for each word. This potentially continues for a number of iterations until the parser does create a spanning analysis, or else it gives up and moves to the next sentence.
664	Explains Technical Concepts	The parser uses the CKY algorithm (Kasami, 1965; Younger, 1967) described in Steedman (2000) to create a packed chart. The CKY al- gorithm applies naturally to CCG since the gram- mar is binary. It builds the chart bottom-up, start- ing with two-word constituents (assuming the su- pertagging phase has been completed), incremen- tally increasing the span until the whole sentence is covered. The chart is packed in the standard sense that any two equivalent constituents created during the parsing process are placed in the same equivalence class, with pointers to the children used in the creation. Equivalence is defined in terms of the category and head of the constituent, to enable the Viterbi algorithm to efficiently find the highest scoring derivation.1 A textbook treat- ment of CKY applied to statistical parsing is given in Jurafsky and Martin (2000).
536	Explains Non-Technical Concepts	3.2 Corpus sub-sampling Whereas using all corpora improves translation quality, it requires a huge amount of memory and disk space. We investigate in this section ways to select sentence pairs among large out-of-domain corpora.
228	Explains Technical Concepts	Resu l ts   Figure 6 shows the precision, recall, and F-Measure  (with equal weights for both precision and recall)  using the B-CUBED scoring algorithm. The Vec-  tor Space Model in this case constructed the space  of terms only from the summaries extracted by  SentenceExtractor. In comparison, Figure 7 shows  the results (using the B-CUBED scoring algorithm)  when the vector space model constructed the space  of terms from the articles input to the system (it  still used the summaries when computing the simi-  larity). The importance of using CAMP to extract  summaries i  verified by comparing the highest F-  Measures achieved by the system for the two cases.  The highest F-Measure for the former case is 84.6%  while the highest F-Measure for the latter case is  78.0%. In comparison, for this task, named-entity  tools like NetOwl and Textract would mark all the  John Smiths the same. Their performance using  our scoring algorithm is 23% precision, and 100%  recall.
461	Assumes Prior Knowledge	The PictureQuest application avoids several of the major stumbling blocks that stand in the way of high-accuracy cross-language retrieval. Ballesteros and Croft (1997) note several pitfalls common to cross-language information retrieval: (1) The dictionary may not contain specialized vocabulary (particularly bilingual dictionaries). (2) Dictionary translations are inherently ambiguous and add extraneous terms to the query. (3) Failure to translate multi-term concepts as phrases reduces effectiveness.
528	Assumes Prior Knowledge	The heuristic criterion comes from the following observation: We observe that parallel text pairs usu- ally have similar name patterns. The difference be- tween the names of two parailel pages usually lies in a segment which indicates the language. For ex- ample, "file-ch.html" (in Chinese) vs. "file-en.html" (in English). The difference may also appear in the path, such as ".../chinese/.../fi le.html" vs. ".../en- glish/.../f i le.html'. The name patterns described above are commonly used by webmasters to help or- ganize their sites. Hence, we can suppose that a pair of pages with this kind of pattern are probably parallel texts.
711	Explains Technical Concepts	Document Features  There is currently one document feature,  instantiated separately for each query, for each  retrieval system S:  Length of Top-Ranked Documents Retrieved by  System (DLEN\[S\]). This is the average of the  number of tokens in the top 5 documents cored by  system S.
645	Assumes Prior Knowledge	1 Introduction  In this paper we present a linguistically  motivated framework for uniform lexico-  structural processing. It has been used for  transformations of conceptual and syntactic  structures during generation i monolingual nd  multilingual natural language generation (NLG)  and for transfer in machine translation (MT).  Our work extends directions taken in systems  such as Ariane (Vauquois and Boitet, 1985),  FoG (Kittredge and Polgu6re, 1991), JOYCE  (Rainbow and Korelsky, 1992), and LFS  (Iordanskaja et al, 1992). Although it adopts  the general principles found in the above-  mentioned systems, the approach presented in  this paper is more practical, and we believe,  would eventually integrate better with emerging  statistics-based approaches toMT.  * The work performed on the framework by this co-  author was done while at CoGenTex, Inc.  The framework consists of a portable Java  environment for building NLG or MT  applications by defining modules using a core  tree transduction engine and single declarative  ASCII specification language for conceptual or  syntactic dependency tree structures 1 and their  transformations. Developers can define new  modules, add or remove modules, or modify  their connections. Because the processing of the  transformation engine is restricted to  transduction of trees, it is computationally  efficient.
225	Explains Non-Technical Concepts	1 In t roduct ion   This paper describes a natural language understanding sys-  tem for the domain of naive thermodynamics. The system  transforms exercises formulated in (a subset of) Danish to a  somewhat "adhoc" chosen meaning representation language.  Given the representation f an exercise, a problem solver shall  deduce its solution in a subsequent computation.  The weakest demand on tim system is that it transforms  texts into representations wlfich are "equivalent" to the texts.  The ultimate demand on the system and the problem solver  is of course that exercises are solved correctly.  The system consists of three parts dealing with respec-  tively morphology, syntax and semantics. The morphological  and syntactical analyses are domain independent and only  related to the natural language. The semantical analysis is  dependent on both the natural anguage and the specific do-  main. During the semantical nalysis of an exercise, syntactic  structures are transformed into a set of logical propositions  arranged as (implicitly) in the exercise. After having com-  pleted the semantical analysis, a language independent rep-  resentation exists. The semantic omponent does not include  an inferential mechanism for deducing the progress in ther-  modynamic experiments. Therefore, it may regard a text as  being ambiguous. For instance, it may not be possible to de-  termine the referent of an anaphora unambiguously without  considering common sense reasoning. However, such ambigu-  ities will be solved by the problem solver, which uses domain-  dependent knowledge as well as commonsense knowledge (see  e.g. (Hobbs, Moore 1985)), and operates w i than  interval-  based representation f time (Allen 1984).
557	Assumes Prior Knowledge	2.2.3 The Filler and Template Modules The filler takes as input the set of tuples generated by the parser and attempts to check off templates contained in the templates module using these tuples, The set of templates in the templates module contains most of remaining domain-specific knowledge required by the system. Each template is an internal representation of a part in the database. It contains for each part, its ID, its description, and the product which contains it. In addition, there are several additional templates corresponding to pre-specified user actions like "operator," and "quit." A sample template follows: tl__I = ( 'product' = > 'SFD', 'product__ids' = > 2229005" 'product_descriptions' => 'IR RECEIVER PC BOARD CI104 BISTABLE MEMORY') For each tuple input from the parser, the filler checks off the fields which correspond to the tuple. For example, if the filler gets as input (description_word, collimator), it checks off the description fields of those templates containing collimator as a word in the field. A template is checked off iff one or more of its fields is checked off. In addition, the filler also maintains a list of all description and product words passed through the tuples (i.e. these words have been uttered by the user). These two lists are subsequently passed to the dialogue manager.
831	Explains Technical Concepts	The epenthests rule states that e must be inserted at a  morpheme boundary if an(:\[ only if the boundary has to  its left sh, s, x, z or eh and to Its right s. The  Interpretation of the rule Is simple; the character pair  ('lexical character:surface haracter') to the left of the  arrow specifies the change that takes place between the  contexts (again stated in character pairs) given to the  right of the arrow. Braces ('{','}') Indicate disjunction  and angled brackets Indicate a sequence, Alternative  contexts may be specified using the word 'or'. IJexlcal  and surface strings of unequal length can be matched by  using the null character '0', and special characters may  be defined and used in rules, for example to cover the  set of alphabetic haracters representing vowels.  The spelling rules are able to match any pair of char-  acter strings. It would for example be possible to  analyse the suppletlve went as a surface form  corresponding to the lexlcal form go+ed. In this case,  four rules would be needed to effect the change, and a  better solution is to list went separately In the lexicon.  in practice, the choice between treating this type of  alternation dynamically, with morphological and spelling  rules, and statically, by exploiting the lexicon directly,  depends on the user's Idea of which is the more elegant  solution. While elegance may be in the eye of the  beholder, computational efficiency is mffortunately not.  I\[ will generally be more efficient to list a word In the  lexicon titan to add spelling or morphological rules  specific to small number of cases.
694	Explains Non-Technical Concepts	3.1 Transfer  Dr iven  Mach ine Trans la t ion TDMT uses bottom-up left-to-right chart pars- ing with transfer rules as shown in Figure 1. The parsing determines the best structure and best transferred result locally by performing structural disambiguation using semantic dis- tance calculations, in parallel with the deriva- tion of possible structures. The semantic dis- tance is defined by a thesaurus. (source pattern) ==~ J ((target pattern 1) ((source xample 1) (source xample 2) ? "- ) (target pattern 2) ?o* )
518	Assumes Prior Knowledge	For this reason we have adopted the ideas of the philosopher Michael Bratman (1987, 1990). Bratman uses the term "practical reason" to describe his analysis since he is concerned with how to reason about practical matters. For human beings, planning is required in order to accomplish one's goals. Bratman's key insight is that human beings tend to follow a plan once they have one, although they are capable of dropping an intention or changing a partial plan when necessary. In other words, human beings do not decide what to do from scratch at each turn. Bratman and others who have adopted his approach use a tripartite mental model that includes beliefs, desires and intentions (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al 1998), hence the name "BDI model." Beliefs, which are uninstantiated plans in the speaker's head, are reified by the plan library. Desires are expressed as the agent's goals. Intentions, or plan steps that the agent has committed to but not yet acted on, are stored in an agenda. Thus the agent's partial plan for achieving a goal is a network of intentions. A plan can be left in a partially expanded state until it is necessary to refine it further.
721	Explains Technical Concepts	4 Structure of the Atlas Planning Engine Figure3 shows a sample plan operator. For legibility, the key elements have been rendered in English instead of in Lisp. The hiercx slot provides a way for the planner to be aware of the context in which a decomposition is proposed. Items in the hiercx slot are instantiated and added to the transient database only so long as the operator which spawned them is in the agenda. To initiate a planning session, the user invokes the planner with an initial goal. The system searches the operator library to find all operators whose goal field matches the next goal on the agenda and whose filter conditions and precon- An elevator slows to a stop from an initial downward velocity of 10.0 m\]s in 2.00 seconds. A passenger in the elevator is holding a 3.00 kilogram package by a vertical string. What is the tension in the string during the process?
973	Explains Non-Technical Concepts	4. EXPERIMENTS  4.1. ATIS Task  The ATIS task \[13\] was chosen for keyword-spotting  experiments because (1) the template-based system that inter-  prets the queries of the airline database focuses on certain key-  words that convey the meaning of the query, and ignores many of  the other filler words (e.g. "I would like...", "Can you please ...'),  (2) the task uses spontaneous speech, and (3) we have worked  extensively on this recognition task over the last two years.  Sixty-six keywords and their variants were selected as keywords  based on the importance of each of the words to the SRI tem-  plate-mateher which interprets he queries.
164	Assumes Prior Knowledge	Second, there are subtle presuppositions that may  go undetected even by a skilled survey designer.  These are presuppositions about things that are  likely (but not necessarily) true. For example, a question may inquire about a person's close  friends (presupposing close friends) or someone's  standard place for preventive care (presupposing  the habit of making use of preventive care). DP  does not know which presuppositions are likely to  be valid or invalid, and is therefore more likely to  detect such subtle incorrect presuppositions than a  human expert.
736	Explains Technical Concepts	Compared with Gennan NP-rcpairs, Chinese  speakers produce rather simple repair sequences  in NPs. Only 62.7% (84 out of 134) of Chinese  repairs found in the corpus are single NP phrases.  The rest of repair sequences in which NPs are  involvcd, contain other phrasal categories uch  as verb phrases or adverbials. Since these  dialogues arc concerned with normal and  everyday conversations, no complicated noun  phrases were used. These NP-rcpairs have the  following structures:  NP => N  NP => DET  NP => DET + N  NP => ADI + N  NP => QUAN + CLASS  NP => OUAN + CLASS + N  where QUAN denotes numbers and CLASS  means classifiers in Chinese.
36	Explains Non-Technical Concepts	2.4 The TABOR Project It so happened that a Universtity Project was start- eded in 1996, called TABOR ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone. At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
149	Assumes Prior Knowledge	The system is based on the Moses SMT toolkit (Koehn et al, 2007) and constructed as follows. First, word alignments in both directions are cal- culated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words.
406	Explains Technical Concepts	The information for a group of ambiguous  morphemes i represented by the data structure: VTX  (Vertex), EDG (Edge) and ABL (AmBiguity List). The  VTX represents he position of morpheme in a sentence.  The EDG represents the common attributes of the  ambiguous morphemes. The common attributes are a  part of speech, the type of inflection and Kana string. The  ABL represents individual attributes of the morphemes.  The individual attributes are the Kanji representation,  the meaning code and the word frequency. An ABL list is  referenced by EDG. VTX and EDG refer to each other. A  VTX is considered to be shared if the grammatical  relationship between the preceeding EDG and its  succeeding EDG is the same. A double circled VTX can  usually be shared.
18	Assumes Prior Knowledge	Our suggestion is therefore to supplement he above mentioned methods, and bridge the gap be- tween them, by post-processing human dialogues to give them a computer-like quality. The advantage, compared to having people do the simulation on the fly, is both that it can be done with more consis- tency, and also that it can be done by researchers that actually know what human-computer natural language dialogues can look like. A possible dis- advantage with using both Wizard of Oz-and real computer dialogues, is that users will quickly adapt to what the system can provide them with, and will therefore not try to use it for tasks they know it cannot perform. Consequently, we will not get a full picture of the different services they would like the system to provide.
230	Explains Technical Concepts	The syntactic discriminants record the differ- ences between derivation trees by memorizing di- rect rule applications and lexical choices. Beside the rule or lexical entry name, the discriminant also records the information concerning the corre- sponding constituent, e.g., the category and span- ning of the constituent, the parent and daughters of the constituent, etc. Furthermore, given the dis- criminant d and the parse forest Y , we can calcu- late the distribution of parses over the value of the discriminant function ?, which can be character- ized by ?y?Y ?(y)/|Y |. This numeric feature in- dicates how many parses can be ruled out with the given discriminant.
21	Explains Technical Concepts	4.3 Parser precision  An initial concern in implementing the present  model was that parsing ambiguous input might  proliferate syntactic analyses. In theory, the  number of analyses might grow exponentially as  the input sentence length increased, making the  reliable ranking of parse results unmanageable. In  practice, however, pathological proliferation of  syntactic analyses is not a problem s. Figure 4  tallies the average number of parses obtained in  relation to sentence l ngth for all successful parses  in the 5,000-sentence t st corpus (corpus A in  Table 1). There were 4,121 successful parses in the  corpus, corresponding to 82.42% coverage. From  Figure 4, we can see that the number of parses  does increase as the sentence grows longer, but the  increment is linear and the slope is very moderate.  Even in the highest-scoring range, the mean  number of parses is only 2.17. Averaged over all  sentence lengths, about 68% of the successfully  parsed sentences receive only one parse, and 22%  receive two parses. Only about 10% of sentences  receive more than 2 analyses. From these results  we conclude that the overgeneration f parse trees  is not a practical concern within our approach.
442	Assumes Prior Knowledge	This technique is shown to be effective for text-independent language,  sex, and speaker identification and can enable better and more friendly  human-machine teraction. With 2s of speech, the language can be identi-  fied with better than 99% accuracy. Error in sexddentification is about 1%  on a per-sentence basis, and speaker identification accuracies of 98.5% on  'lIMIT (168 speakers) and 99.2% on BREF (65 speakers), were obtained  with one utterance per speaker, and 100% with 2 utterances for both corpora.  An experiment using unsupervised a aptation for speaker identification on  the 168 TIMIT speakers had the same identification accuracies obtained  with supervised adaptation.
620	Explains Non-Technical Concepts	There are several obvious advantages to software-based rec-  ognizers: greater flexibility, lower cost, and the opportunity  for large gains in speed due to clever search algorithms.  1. Since the algorithms are in a constant state of flux,  any special-purpose hardware is obsolete before it is  finished.  2. Software-only systems are key to making the technol-  ogy broadly usable.  - Many people will simply not purchase xtra hardware.  - Integration is much easier.  91  - 'Iqae systems are more flexible.  3. For those people who already have workstations, oft-  ware is obviously less expensive.  4. Most importantly, it is possible to obtain much larger  gains in speed ue to clever search algorithms than from  faster hardware.  We have previously demonstrated real-time software-only  recognition for the ATIS task with over 1,000 words. More  recently, we have developed new search algorithms that per-  form recognition of 20,000 words with fully-connnected bi- gram and trigram statistical grammars in strict real-time with  little loss in recognition accuracy relative to research levels.  First, we will very briefly review some of the search algo-  rithms that we have developed. Then we will explain how  the Forward-Backward Search can be used to achieve real-  time 20,000-word continuous peech recognition.
986	Explains Technical Concepts	The multiple-word terms are sequences of lemmas (not word forms). This structure has several advantages, among others it allows to minimize the size of the dictionary and also, due to the simplicity of the structure, it allows modifications of the glossaries by the linguistically naive user. The necessary morphological information is introduced into the domain-related glossary in an off-line preprocessing stage, which does not require user intervention. This makes a big difference when compared to the RUSLAN Czech-to-Russian MT system, when each multiword dictionary entry cost about 30 minutes of linguistic expert's time on average.
522	Assumes Prior Knowledge	Most approaches (Brown et al, 1992; Li & Abe,  1997) inherently extract semantic knowledge in  the abstracted form of semantic clusters. Our  method produces emantic similarity relations as  an intermediate (and information-richer)  semantics representation formalism, from which  cluster hierarchies can be generated. Of great  importance is that soft clustering methods can also  be applied to this set of relations and cluster  polysemous words to more than one classes.  Stock lnarket-financial news and Modem Greek,  were used as domain and language test case  respectively, ltowever demonstrative xamples  taken from the WSJ corpus have been used  throughout the paper as well.
747	Explains Non-Technical Concepts	With long queries we showed that using linguistic  phrases to match within document windows as further  evidence to re-rank retrieval output can lead to some  small improvements. We also studied re-ranking of  output documents based on topical concept level  evidence using document clustering, but the effort has  so far not been successful.
259	Assumes Prior Knowledge	Another major problem is related to the  maintenance of both the grammar and the  lexicon. On several occasions during the  development of these resources, the developer in  charge of adding lexical and grammatical data  must make some decisions that are domain  specific. For example, in MT, writing transfer  rules for terms that can have several meanings or  uses, they may simplify the problem by  choosing a solution based on the context found  in the current corpus, which is a perfectly natural  strategy. However, later, when porting the  transfer esources to other domains, the chosen  strategy may need to be revised because the  context has changed, and other meanings or uses  are found in the new corpora. Because the  current approach is based on handcrafted rules,  maintenance problems of this sort cannot be  avoided when porting the resources to new  domains.
111	Assumes Prior Knowledge	For the future, it will be interesting to see if phrasal  evidence can be employed for Chinese IR, and to study  how to improve its usefulness. Topical clustering for  enhancing retrieval, display and for data reduction in  general are also important issues for large scale IR.
264	Explains Non-Technical Concepts	At the training stage, first of all, the school  bullying transcripts collected from previous user  testing have been divided into several topic sec- tions with each of them belonging to one of the  story sub-themes. The classification of the sub- themes is mainly based on the human director?s  intervention which was recorded in the tran- scripts. Then we used two human annotators to  mark up the affect of every turn-taking input in  the transcripts using context inference. Thus, for  each character, we have summarized a series of  emotions expressed throughout the improvisa- tion of a particular story sub-theme. Since the  improvisation is creative under the loose scena- rio, some of the sub-themes (e.g. ?why Mayid is  so nasty?) have been suggested for improvisa- tion for one than once in some transcripts and  some of the topics (e.g. ?why Lisa is crying?)  are only shown in a few of the collected tran- scripts. We made attempts to gather as many  emotional contexts as possible for each charac- ter for the improvisation of each sub-theme in  order to enrich the training data.
192	Mathematically-Oriented Paragraph	2.2 Problem Formulation Let x be an observation sequence of tokens in encyclopedic text and x = {x1, ? ? ? , xN}. Let sp be the principal entity (we assume that it is known or can be easily recognized), and let s = {s1, ? ? ? , sL} be a segmentation assignment of ob- servation sequence x. Each segment si is a triple si = {?i, ?i, yi}, where ?i is a start position, ?i is an end position, and yi is the label assigned to all tokens of this segment. The segment si satis- fies 0 ? ?i < ?i ? |x| and ?i+1 = ?i + 1. Let rpn be the relation assignment between principal entity sp and secondary entity candidate sn from the segmentation s, and r be the set of relation as- signments for sequence x.
431	Explains Technical Concepts	5 Multiple Feature Representation  Local Feature (LF) is constructed based on  neighboring tokens and the token itself. There  are two types of contextual information to be  considered when extracting LFs, namely inter- nal lexical and external contextual information.  (1) Term length (Len) ? Aims to consider the  length of the translation candidate.  (2) Phonetic Value (PV) ? Aims to investigate  the phonetic similarity between an OOV term  and its translation candidates. Because the as- sociated syllabification representations can  often be found between Chinese and English  syllables with fewer ambiguities, the syllabifi- cation has become an effective channel in pho- netic feature expression. PV means that for  measuring the edit distance similarity between  the syllabification sequences of an OOV term  and its candidates, the processing is executed  according to the specific linguistic rules.
351	Explains Technical Concepts	2.3 Template Generation Module  A typical template generation module is a  hard-coded post-processing module which has  to be written for each type of template. By  contrast, our Template Generation module is  unique as it uses declarative rules to generate  and merge templates automatically so as to  achieve portability.
395	Explains Technical Concepts	4 Dis t i l l a t ion  gu ide l ines Distilling dialogues requires guidelines for how to handle various types of utterances. In this section we will present our guidelines for distilling a corpus of telephone conversations between a human infor- mation provider on local buses 1to be used for devel- oping a multimodal dialogue system (Qvarfordt and JSnsson, 1998; Flycht-Eriksson and JSnsson, 1998; Dahlb~ick et al, 1999; Qvarfordt, 1998). Similar guidelines are used within another project on devel- oping Swedish Dialogue Systems where the domain is travel bureau information. We can distinguish three types of contributors: 'System' (i.e. a future systems) utterances, User ut- terances, and other types, such as moves by other speakers, and noise.
319	Explains Technical Concepts	This paper considers only the semantical interpretation of  sentences. The semantical analysis is based on a composi-  tional principle similar to the one used by Katz and Fodor  (1963). It claims that the semantical interpretation of a sen-  tence is obtained by replacing its words or phrases with their  semantic representations and combining these according to  the syntactic structure of the sentence as well as the con-  text. The interpretation is controlled by a case grammar,  which consists of case frames. The case frames relate syntac-  tic structures to a case system and place semantic onstraints  on their constituents. In examining if constraints are fulfilled  during the analysis, a static worldknowledge is used. The  most important component of the worldknowledge is an is-a  hierarchy which organizes all concepts in the dogmin of dis-  course. The worldknowledge is called "static", since it does  not contain "dynamic" information such as implications or  preconditions of actions.
24	Explains Non-Technical Concepts	Abstract This paper proposes a way to improve the trans- lation quality by using information on dialogue participants that is easily obtained from out- side the translation component. We incorpo- rated information on participants' ocial roles and genders into transfer ules and dictionary entries. An experiment with 23 unseen dia- logues demonstrated a recall of 65% and a preci- sion of 86%. These results howed that our sim- ple and easy-to-implement method is effective, and is a key technology enabling smooth con- versation with a dialogue translation system.
116	Explains Non-Technical Concepts	6 Conclusion  In this paper, we present an approach to auto- matically normalizing temporal expressions un- der the reference time dynamic-choosing me- chanism. The referential feature in temporal  nouns is applied to classify Implicit Times.  Based on this, different classes of times can be  normalized according to their respective classes.  Meanwhile, we introduce the scenario-time  shifting model to deal with the defuzzification  problem. The experiment shows that our ap- proach achieves more promising evaluation re- sults, and makes the automatic normalization  more adaptable to real texts than the prior works.  However, the neglect on the event-anchored ex- pression certainly restricts the whole system in  applications, so the event-anchored expression  will be our research focus in future.
485	Explains Non-Technical Concepts	2. EXTRACTION OF AMBIGUITY FROM INPUT  KANA SENTENCES  This section first describes the method for  extracting highly possible ambiguities by morphological  analysis, and then describes an efficient data structure for  storing those ambiguities in memory.
508	Assumes Prior Knowledge	To meet this challenge we incrementally apply all available  acoustic and linguistic information in three search phases.  Phase one is a left to right Viterbi Beam search which produces  word end times and scores using right context between-word  models with a bigram language model. Phase two, guided  by the results from phase one, is a right to left Viterbi Beam  search which produces word beginning times and scores based  on left context between-word models. Phase three is an A*  search which combines the results of phases one and two with  a long distance language model.
329	Explains Non-Technical Concepts	In 1990 we realized that we could make faster advances  in the algorithms using off-the-shelf hardware than by us-  ing special hardware. Since then we have gained orders of  magnitude in speed in a short time by changing the search  algorithms in some fundamental ways, without the need for  additional or special hardware other than a workstation. This  has resulted in a major paradigm shift. We no longer think  in terms of special-purpose hardware - we take it for granted  that recognition of any size problem will be possible with a  software-only solution.
892	Explains Technical Concepts	5. The Spell ing Rules  The rules are based on the work of Koskennlemt (1983a,  1983b, Karttunen 1983), though their application here is  solely to the question of 'morphographemlcs'; the more  general morphological effects of Koskenniemi's rules are  produced dlffenmtly. The current version of the system  contains a compiler allowing the rules to be written in a  high level notation based on KoskennIemi (1985). Any  number of spelling rules can be employed, though our  system has fifleen. They are compiled during the gen-  eral dictionary pre-processlng stage into deterministic  finite state transducers, of which one tape represents the  lexlcal form and the other the surface form.  The following rule describes the process by which an  additional e is Inserted when some nouns are suffixed  with the plural morpheme +s:  Epenthesls  +:e <=~> { < s:s h:h > s:s x:x z:z } ---  s:s  or < c:c h:h2> .... s:s
465	Explains Technical Concepts	3.3. Parallel Training  To reduce computation, our system prunes low probabil-  ity observations, as in \[4\], and uses the marginal training  algorithm described above. However, even with these  savings, tied-mixture training involves a large computa-  tion, making experimentation potentially cumbersome.  When the available computing resources consist of a net-  work of moderately powerful workstations, as is the case  at BU, we would like to make use of many machines  at once to speed training. At the highest level, tied  mixture training is inherently a sequential process, since  each pass requires the parameter estimates from the pre-  vious pass. However, the bulk of the training compu-  tation involves estimating counts over a database, and  these counts are all independent of each other. We can  therefore speed training by letting machines estimate the  counts for different parts of the database in parallel and  combine and normalize their results at the end of each  pass.
301	Assumes Prior Knowledge	When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus. As a last preprocessing step we remove sen- tences that are too long and empty lines to obtain the final training corpus.
920	Assumes Prior Knowledge	As stated above, incorrect presuppositions are  infrequent in well-designed questionnaires. For  example, questions about details of somebody's  marriage are usually preceded by a question  establishing the person's marital status.  In spite of this, providing feedback about  presuppositions to the survey methodologist is useful. Importantly, QUAID is designed to aid in  the design process. Consider a survey on health-  related issues. In the context of this topic, a  survey methodologist may be interested in how  many days of work a person missed because of  illness, but not think about whether the person  actually has a job. Upon entering the question  "how many days of work did you miss last year  because of illness" into the QUAID tool, DP  would report that the question presupposes  employment. The survey methodologist could  then insert a question about employment.
663	Assumes Prior Knowledge	Having declarative rules facilitates their reuse  when migrating from one programming  environment toanother; if the rules are based on  functions pecific to a programming language,  the implementation f these functions might no  longer be available in a different environment.  In addition, having all lexical information and  all rules represented eclaratively makes it  relatively easy to integrate into the framework  techniques for generating some of the rules  automatically, for example using corpus-based  methods. The declarative form of  transformations makes it easier to process them,  compare them, and cluster them to achieve  proper classification and ordering.  1 In this paper, we use the term syntactic dependency  (tree) structure as defined in the Meaning-Text  Theory (MTT; Mel'cuk, 1988). However, we  extrapolate from this theory when we use the term  conceptual dependency (tree) structure, which has no  equivalent in MTT (and is unrelated to Shank's CD  structures proposed inthe 1970s).
279	Assumes Prior Knowledge	The reported experinaents have been carried out on  a 220.000 words corpus, comprised o1' financial  news of 1998, which was constructed in the  framework of a currently carried out R&D project  for Information Extraction from raw text ~.  The methods and their variations described in  sections 4 and 5 for obtaining lexical senmntic  relations were tested and their accuracy per  nurnber of best hits was measured by hunmn  inspection. The VWCSE method was tested using  only the previous and next word as context  parameters (N&P method), to sketch a method  baseline lbr the particular corpus. Using a  Morphological Analyzer & Part-of Speech tagger  to restrict semantic relations only between words  of the same Part-of-Speech (PoS) we obtain  apparently higher accuracy, though we loose some  interesting verb - noun pairs referring to the same  action or condition, e.g. (mOiOqKc (=increased)  and dvogo.q (=increment). The results indicate that  the norlnalization factors indeed iinprove the  accuracy of the methods and that context  similarity detection based on dynamic pattern-  matching yields significantly more reliable results  than the word-based method. This demonstrates  the importance of the cross-correlation algorithm,  which is the only suitable tbr pattern-based  context similarity detection.
672	Explains Technical Concepts	 3 General  and Specific Patterns  Before we describe our example-based strategy for  building patterns, we examine the organization of the  pattern base in more detail. We can group the pat-  terns into "layers" according to their range of appli-  cability:  1. Domain-independent: this layer contains the  most generally applicable patterns. Included in  this layer are many of the patterns for name  recognition (for people, organizations, and loca-  tions, as well as temporal and numeric expres-  sions, currencies, etc.), and the purely syntactic  patterns for noun groups and verb groups. These  patterns are useful in a wide range of tasks.  2. Domain-specific: the next layer contains domain-  specific patterns, which are useful across a nar-  rower range of scenarios, but still have consid-  erable generality. These include domain-specific  name patterns, such as those for certain types  of artifacts, as well as patterns for noun phrases  which express relationships among entities, such  as those between persons and organizations.  3. Scenario-specific: the last layer contains  scenario-specific patterns, having the narrowest  applicability, such as the clausal patterns that  capture relevant events.
862	Explains Technical Concepts	The first- and second-order expectation semi- rings can also be used to compute many interesting quantities over hypergraphs. These quantities in- clude expected translation length, feature expec- tation, entropy, cross-entropy, Kullback-Leibler divergence, Bayes risk, variance of hypothesis length, gradient of entropy and Bayes risk, covari- ance and Hessian matrix, and so on.
950	Assumes Prior Knowledge	It can be seen from the Tables that the accuracy loss with self-trained binary or level taggers was not large (in the worst case, the accuracy dropped from 84.23% to 83.39%), while the speed was significantly improved. Using binary taggers, the largest speed improvement was from 47.6 sen- tences per second to 80.8 sentences per second (a 69.7% relative increase). Using level taggers, the largest speed improvement was from 47.6 sen- tences per second to 96.6 sentences per second (a 103% relative increase).
800	Assumes Prior Knowledge	 Figure 2: Andes hint sequence formatted as dialogue ditions are satisfied. Goals are represented in first-order logic without quantifiers and matched via unification. Since APE is intended especially for generation of hierarchically organized task- oriented iscourse, each operator has a multi-step recipe in the style of Wilkins (1988). When a match is found, the matching goal is removed from the agenda and is replaced by the steps in the recipe. APE has two kinds of primitive actions; one ends a turn and the other doesn't. From the point of view of discourse generation, the most important APE recipe items are those allowing the planner to change the agenda when necessary. These three types of recipe items make APE more powerful than a classical planner. ? Fact: Evaluate a condition. If false, skip the rest of the recipe. Fact is used to allow run-time decision making by bypassing the rest of an operator when circumstances change during its execution. Fact can be used with retry-at to implement a loop just as in Prolog. ? Retry-at. The purpose of retry-at is to allow the planner to back up to a choice point and make a new decision. It removes goals sequentially from the top of the agenda, a full operator at a time, until the supplied argument is false. Then it restores the parent goal of the last operator removed, so that further planning can choose a new way to achieve it. Retry-at implements a Prolog-like choice of alternatives, but it differs from backtracking in that the new operator is chosen based on conditions that apply when the retry operation is executed, rather than on a list of possible operators formed when the original operator was chosen. For retry-at o be useful, the author must provide multiple operators for the same goal. Each operator must have a set of preconditions enabling it to be chosen at the appropriate time.
163	Assumes Prior Knowledge	Most added utterances, both from the user and the 'system', provide explicit requests for informa- tion that is later provided in the dialogue, e.g. ut- terance $3 in figure 6. We have added ten utterances in all 39 dialogues, five 'system' utterances and five user utterances. Note, however, that we utilised the transcribed ialogues, without information on into- nation. We would probably not have needed to add this many utterances if we had utilised the tapes. Our reason for not using information on intonation is that we do not assume that our system's peech recogniser can recognise intonation.
171	Explains Technical Concepts	1 Introduction We investigate a compound information extrac- tion (IE) problem from encyclopedia articles, which consists of two subtasks ? recognizing structured information about entities and extract- ing the relationships between entities. The most common approach to this problem is a pipeline architecture: attempting to perform different sub- tasks, namely, named entity recognition and rela- tion extraction between recognized entities in sev- eral separate, and independent stages. Such kind of design is widely adopted in NLP.
71	Explains Non-Technical Concepts	3. IMPROVEMENTS IN ACCURACY  In this section, we describe several modifications that each re-  salted in an improvement in accuracy on the WSJ corpus. In  all cases, we used the same training set (SI-12) and the standard  bigram grammars. The initial word error rate when testing on  a SK-word closed-vocabulasy VP language model was 12.0%~  Each of these methods is described below.
297	Assumes Prior Knowledge	Abstract  Natural language processing techniques may hold  a tremendous potential for overcoming the inade-  quacies of purely quantitative methods of text in-  formation retrieval. Under the Tipster contracts in  phases I through III, GE group has set out to ex-  plore this potential through development and evalu-  ation of new text processing techniques. This work  resulted in some significant advances and in a better  understanding on how NLP may benefit IR. Tipster  research as laid a critical groundwork for future  work.  In this paper we summarize GE work on document  detection in Tipster Phase III. Our summarization  research is described in a separate paper appearing  in this volume.
851	Explains Non-Technical Concepts	Unlike other reported systems which treat cepstral pa-  rameters and their derivatives as independent observa-  tion streams, the BU system models them jointly using  a single output stream, which gives better performance  than independent streams with a single Gaussian dis-  tribution (non-mixture system). Presumably, the result  would also hold for mixtures.
189	Assumes Prior Knowledge	The domain-related bilingual glossaries contain pairs of individual words and pairs of multiple-word terms. The glossaries are organized into a hierarchy specified by the user; typically, the glossaries for the most specific domain are applied first. There is one general matching rule for all levels of glossaries - the longest match wins.
639	Explains Technical Concepts	For a document set, event vectorization  begins with aggregating all the event terms and  entities in a set of event units (eu). Given m distinct event terms, n distinct named entities,  and p distinct high-frequency common entities,  the m + n + p eu?s are a concatenation of the  event terms and entities such that eui is an event  term for 1 ? i ? m, a named entity for m + 1 ? i ? m + n, and a high-frequency entity for m + n + 1 ? i ? m + n + p. The eu?s define the m + n + p dimensions of an event vector in an eu-by-event  matrix E = [eij], as shown in Figure 2.
112	Explains Technical Concepts	Model BLEU Baseline 19.51 + compound splitting 20.09 (+0.58) + pre-reordering 20.03 (+0.52) + both 20.85 (+1.34) Table 8: Special handling of German?English Language Pair Baseline Weighted TM Spanish-English 26.20 26.15 (?0.05) French-English 26.11 26.30 (+0.19) German-English 21.09 20.81 (?0.28) Czech-English 21.33 21.21 (?0.12) English-German 15.28 15.01 (?0.27) avg. ?0.11 Table 9: Interpolating the translation model with language model weights
847	Explains Non-Technical Concepts	3.5 109 Corpus Last year, due to time constraints, we were not able to use the billion word 109 corpus for the French?English language pairs. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008).
255	Assumes Prior Knowledge	Our objective is to maximize the recognition accuracy with  a minimal increase in computational complexity. With  our decomposed, incremental, semi-between-word-triphones  search, we observed that early use of detailed acoustic mod-  els can significantly reduce the recognition error rate with  a negligible increase computational complexity as shown in  Figure 2.    5. UNIFIED STOCHASTIC ENGINE  Acoustic and language models are usually constructed sepa-  rately, where language models are derived from a large text  corpus without consideration for acoustic data, and acoustic  models are constructed from the acoustic data without ex-  ploiting the existing text corpus used for language training.  We recently have developed a unified stochastic engine (USE)  that jointly optimizes both acoustic and language models. As  the true probability distribution of both the acoustic and lan-  guage models can not be accurately estimated, they can not be  considered as real probabilities but scores from two different  sources. Since they are scores instead of probabilities, the  straightforward implementation of the Bayes equation will  generally not lead to a satisfactory recognition performance.  To integrate language and acoustic probabilities for decoding,  we are forced to weight acoustic and language probabilities  with a so called language weight \[6\]. The constant language  weight is usually tuned to balance the acoustic probabilities  and the language probabilities such that the recognition error  rate can be minimized. Most HMM-based speech recognition  systems have one single constant language weight hat is in-  dependent of any specific acoustic or language information,  and that is determined using a hill-climbing procedure on de-  velopment data. It is often necessary to make many runs with  different language weights on the development data in order  to determine the best value.
287	Explains Technical Concepts	One minor disadvantage of the Viterbi search is that it finds  the state sequence with the highest probability rather than  the word sequence with the highest probability. This is only  a minor disadvantage b cause the most likely state sequence  has been empirically shown to be highly correlated to the  most likely word sequence. (We have shown in \[6\] that a  slight modification to the Viterbi computation removes this  problem, albeit with a slight approximation. When two paths  come to the same state at the same time, we add the prob-  abilities instead of taking the maximum.) A much more  serious problem with the time-synchronous search is that it  must follow a very large number of theories in parallel even  though only one of them will end up scoring best. This can  be viewed as wasted computation.  We get little benefit from using a fast match algorithm with  the time-synchronous search because we consider starting all  possible words at each frame. Thus, it would be necessary  to run the fast match algorithm at each frame, which would  be too expensive for all but the least expensive of fast match  algorithms.
773	Explains Non-Technical Concepts	The context of our work is the development a multi-modal dialogue system. However, in our cur- rent work with distilling dialogues, the abilities of a multi-modal system were not fully accounted for. The reason for this is that the dialogues would be significantly affected, e.g. a telephone conversation where the user always likes to have the next con- nection, please will result in a table if multi-modal output is possible and hence a fair amount of the di- alogne is removed. We have therefore in this paper analysed the corpus assuming a speech-only system, since this is closer to the original telephone conversa- tions, and hence needs fewer assumptions on system performance when distilling the dialogues.
772	Explains Non-Technical Concepts	Our individual sentence scoring algorithm shares  some properties with \[14\]. Their approach includes  scores for anaphoric density, string equivalence with  the title or headline of a document, and position  of the sentence in the document. However, we do  not take advantage of overt cues for summary sen-  tences, such as 'in summary'  or 'in conclusion', nor  do we use temporal information in generating a summary.
14	Explains Technical Concepts	Abstract  "Word" is difficult to define in the languages that  do not exhibit explicit word boundary, such as  Thai. Traditional methods on defining words for  this kind of languages have to depend on human  judgement which bases on unclear criteria o1"  procedures, and have several limitations. This  paper proposes an algorithm for word extraction  from Thai texts without borrowing a hand from  word segmentation. We employ the c4.5 learning  algorithm for this task. Several attributes uch as  string length, frequency, nmtual information and  entropy are chosen for word/non-word  determination. Our experiment yields high  precision results about 85% in both training and  test corpus.
463	Assumes Prior Knowledge	The dictionary was changed from English to Nor- wegian together with new rules for morphological analysis. The change of grammar from English to Norwegian proved to be amazingly easy. It showed that the langauges were more similar than one would believe, given that the languages are incomprehen- sible to each other's communities. After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds. Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
17	Explains Technical Concepts	2.2.2 Parser and the Lexicon The parser is domain-driven i the sense that it uses domain-dependent information produced by the lexicon to look for information, in a user utterance, that is useful in the current domain. However, it does not attempt to understand fully each user utterance. It is robust enough to handle ungrammatical sentences, hort phrases, and sentences that contain mis-recognized text. The lexicon, in addition to providing domain-dependent keywords and phrases to the parser, also provides the semantic knowledge associated with each keyword and phrase. Therefore, for each content word in the inverted hash tables, the lexicon contains entries which help the system determine whether the word was used in a part description, or a product name. In addition, the lexicon also provides the semantic knowledge associated with the pre-specified actions which can be taken by the user like "operator" which allows the user to transfer to an operator, and "stop," or "quit" which allow the user to quit the system. Some sample ntries are: collimator => (description_word, collimator) camera => (product_word, camera) operator => (user action, operator) etc.
85	Explains Technical Concepts	3.3 Features  and Agreement  Rate   This section describes how much each feature set con-  tributes to improving the agreement rate.  The values listed in the rightmost columns in Ta-  bles 3 and 4 shows the performance of the word or-  der estimation without each feature set. The values  in parentheses are the percentage of improvement or degradation to the formal experiment. In the exper-  iments, when a basic feature was deleted, the com-  bined features that included the basic feature were  also deleted. The most useful feature is the type of  3When wl and w2 were the same word, we used the head  words in Bt  and 132 as Wl and w2. When one offreq(wt2) and  freq(w21) was zero and the other was five or more, we used  the f lequencies when they appeared in the order "Wl ws" and  "w2 wt,"  respectively~ instead of frcq(wi2) al,d freq(wsl).  When both freq(wl.2) and freq(w27) were zero, we instead  used random figures between 0 and t.  bunsetsu, which basically signifies the case marker or  inflection type. This result is close to our expecta-  tions.
72	Explains Non-Technical Concepts	Many approaches have been considered in the development  of robust speech recognition systems including techniques  based on autoregressive analysis, the use of special distor-  tion measures, the use of auditory models, and the use of  microphone arrays, among many other approaches (as  reviewed in \[1,4\]).
689	Assumes Prior Knowledge	Apart from the propositional content &the text,  stylistic aspects can also be used as  classificatory means. Biber studied the stylistic  differences between written and spoken  language (Biber, 1988) as well as the variation  of registers in a cross-linguistic omparison  (Biber, 1995) and presented a model for  interpreting the functions of various linguistic  features. Unfortunately, his model can not be  easily realized using existing natural anguage  processing tools. On the other hand, some  computational models for detecting  automatically the text genre have recently been  available (Karlgren and Cutting, 1994; Kessler  et al, 1997). Kessler gives an excellent  summarization f the potential applications of a  text genre detector. In particular, part-of-speech  tagging, parsing accuracy and word-sense  disambiguation could be considerably enhanced  by taking genre into account since certain  grammatical constructions or word senses are  closely related to specific genres. Moreover, in  information retrieval the search results could be  sorted according to the genre as well.
33	Explains Technical Concepts	4.1. Modified A* Stack Search  Each theory, th, on the stack consists of five entries. A partial  theory, th.pt, a one word extension th.w, a time th.t which  denotes the boundary between th.pt and th.w, and two scores  th.g, which is the score for th.pt up to time th.t and th.h which  is the best score for the remaining portion of the input starting  with ti~.w at time th.t+l through to the end. Unique theories  are detlermined by th.pt and th.w. The algorithm proceeds as  follows: l. Add initial states to the stack.  2. According to the evaluation function th.g+ th.h, remove  the best theory, th, from the stack.  3. Ifth accounts for the entire input then output he sentence  corresponding to th. Halt if this is the Nth utterance  output.  4. For the word th.w consider all possible nd times, t as  provided by the left/right lattice.  (a) For all words, w, beginning at time t+ 1 as provided  by the right/left lattice  i. Extend theory th with w. Designate this  theory as th'. Set th'.pt = th.pt + th.w,  th'.w ::= w and th'.t = t.  ii. Compute scores  th'.g = th.g + w_score(w, th.t + 1,t), and  th'.h. See following for definition of w_score  and thqh computation.  iii. If th' is already on the stack then choose the  best instance of th' otherwise push th' onto  the stack.  5. Goto step 2.
995	Explains Technical Concepts	3.4 Bipartite Graph and HITS Algorithm  Hyperlink-induced topic search (HITS) is a link  analysis algorithm that rates Web pages. As  discussed in the introduction section, we can  apply the HITS algorithm to compute feature  relevance for ranking.
